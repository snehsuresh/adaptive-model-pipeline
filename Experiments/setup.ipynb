{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snehsuresh/anaconda3/envs/mynewenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/snehsuresh/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "teacher_model_name = \"distilbert-base-uncased\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_model_name, num_labels=2\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4286.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return teacher_tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu_available': False, 'mps_available': True, 'gpu_type': 'MPS', 'gpu_count': 1, 'gpu_memory': None, 'total_ram': 8.0, 'available_ram': 1.30828857421875, 'cpu_cores': 8, 'logical_cores': 8}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "\n",
    "def get_hardware_info():\n",
    "    # Check if a CUDA GPU is available\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "\n",
    "    # Check if MPS is available (for Apple Silicon devices like M1/M2)\n",
    "    mps_available = torch.backends.mps.is_available()\n",
    "\n",
    "    # Get available system RAM\n",
    "    total_ram = psutil.virtual_memory().total / (1024**3)  # Convert to GB\n",
    "    available_ram = psutil.virtual_memory().available / (1024**3)  # Convert to GB\n",
    "\n",
    "    # Get CPU core count\n",
    "    cpu_cores = psutil.cpu_count(logical=False)  # Physical cores\n",
    "    logical_cores = psutil.cpu_count(logical=True)  # Logical cores\n",
    "\n",
    "    # Get GPU details if a CUDA GPU is available\n",
    "    if gpu_available:\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (\n",
    "            1024**3\n",
    "        )  # Convert to GB\n",
    "        gpu_type = \"CUDA\"\n",
    "\n",
    "    # Get MPS details if an MPS GPU is available\n",
    "    elif mps_available:\n",
    "        gpu_count = 1  # MPS typically means 1 Apple GPU\n",
    "        gpu_mem = None  # No easy way to fetch memory for MPS currently\n",
    "        gpu_type = \"MPS\"\n",
    "\n",
    "    # If neither CUDA nor MPS is available\n",
    "    else:\n",
    "        gpu_count = 0\n",
    "        gpu_mem = 0\n",
    "        gpu_type = \"None\"\n",
    "\n",
    "    # Combine the hardware info into a dictionary\n",
    "    hardware_info = {\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"mps_available\": mps_available,\n",
    "        \"gpu_type\": gpu_type,\n",
    "        \"gpu_count\": gpu_count,\n",
    "        \"gpu_memory\": gpu_mem,\n",
    "        \"total_ram\": total_ram,\n",
    "        \"available_ram\": available_ram,\n",
    "        \"cpu_cores\": cpu_cores,\n",
    "        \"logical_cores\": logical_cores,\n",
    "    }\n",
    "\n",
    "    return hardware_info\n",
    "\n",
    "\n",
    "# Check hardware info\n",
    "hardware_info = get_hardware_info()\n",
    "print(hardware_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveStudentModel(nn.Module):\n",
    "    def __init__(self, teacher_model_name, hardware_info):\n",
    "        super(AdaptiveStudentModel, self).__init__()\n",
    "\n",
    "        # Load teacher model to get its hidden size\n",
    "        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            teacher_model_name, num_labels=2\n",
    "        )\n",
    "        hidden_size = self.teacher_model.config.hidden_size\n",
    "\n",
    "        # Adjust the student model architecture based on hardware\n",
    "        if hardware_info[\"gpu_available\"]:\n",
    "            if hardware_info[\"gpu_memory\"] < 4:  # For low-end GPUs\n",
    "                self.student_model = nn.Linear(hidden_size, 128)  # Fewer units\n",
    "            else:\n",
    "                self.student_model = nn.Linear(\n",
    "                    hidden_size, 256\n",
    "                )  # Higher-end GPUs get a larger model\n",
    "        elif hardware_info[\"mps_available\"]:\n",
    "            self.student_model = nn.Linear(\n",
    "                hidden_size, 128\n",
    "            )  # Adapt for MPS (Apple Silicon)\n",
    "        else:\n",
    "            self.student_model = nn.Linear(\n",
    "                hidden_size, 64\n",
    "            )  # Smaller model for CPU-only devices\n",
    "\n",
    "        self.output_layer = nn.Linear(\n",
    "            self.student_model.out_features, 2\n",
    "        )  # Output layer size stays the same\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        teacher_outputs = self.teacher_model(input_ids, attention_mask=attention_mask)\n",
    "        student_outputs = self.student_model(teacher_outputs[1])  # Use pooled output\n",
    "        return self.output_layer(student_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveStudentModel(nn.Module):\n",
    "    def __init__(self, teacher_model_name, hardware_info):\n",
    "        super(AdaptiveStudentModel, self).__init__()\n",
    "        \n",
    "        # Load teacher model to get its hidden size\n",
    "        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_name, num_labels=2)\n",
    "        hidden_size = self.teacher_model.config.hidden_size\n",
    "        \n",
    "        # Adjust the student model architecture based on hardware\n",
    "        if hardware_info[\"gpu_available\"]:\n",
    "            if hardware_info[\"gpu_memory\"] < 4:  # For low-end GPUs\n",
    "                self.student_model = nn.Linear(hidden_size, 128)  # Fewer units\n",
    "            else:\n",
    "                self.student_model = nn.Linear(hidden_size, 256)  # Higher-end GPUs get a larger model\n",
    "        elif hardware_info[\"mps_available\"]:\n",
    "            self.student_model = nn.Linear(hidden_size, 128)  # Adapt for MPS (Apple Silicon)\n",
    "        else:\n",
    "            self.student_model = nn.Linear(hidden_size, 64)  # Smaller model for CPU-only devices\n",
    "\n",
    "        self.output_layer = nn.Linear(self.student_model.out_features, 2)  # Output layer size stays the same\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        teacher_outputs = self.teacher_model(input_ids, attention_mask=attention_mask)\n",
    "        student_outputs = self.student_model(teacher_outputs[1])  # Use pooled output\n",
    "        return self.output_layer(student_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation loss function\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_probs = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "    return nn.KLDivLoss()(student_probs, teacher_probs) * (temperature**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
