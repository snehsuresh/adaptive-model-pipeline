{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snehsuresh/anaconda3/envs/mynewenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/snehsuresh/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "teacher_model_name = \"distilbert-base-uncased\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_model_name, num_labels=2\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:08<00:00, 2893.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "# Updated tokenization function with padding and truncation\n",
    "def tokenize_function(examples):\n",
    "    # Ensure the input is a list of strings\n",
    "    texts = [str(text) for text in examples[\"text\"]]\n",
    "\n",
    "    # Tokenize each example and ensure padding, truncation, and correct tensor formatting\n",
    "    tokenized = teacher_tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",  # Use max_length padding\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Remove the batch dimension added by the tokenizer\n",
    "    tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a data collator that pads sequences dynamically\n",
    "data_collator = DataCollatorWithPadding(teacher_tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu_available': False, 'mps_available': True, 'gpu_type': 'MPS', 'gpu_count': 1, 'gpu_memory': None, 'total_ram': 8.0, 'available_ram': 2.5740509033203125, 'cpu_cores': 8, 'logical_cores': 8}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "\n",
    "def get_hardware_info():\n",
    "    # Check if a CUDA GPU is available\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "\n",
    "    # Check if MPS is available (for Apple Silicon devices like M1/M2)\n",
    "    mps_available = torch.backends.mps.is_available()\n",
    "\n",
    "    # Get available system RAM\n",
    "    total_ram = psutil.virtual_memory().total / (1024**3)  # Convert to GB\n",
    "    available_ram = psutil.virtual_memory().available / (1024**3)  # Convert to GB\n",
    "\n",
    "    # Get CPU core count\n",
    "    cpu_cores = psutil.cpu_count(logical=False)  # Physical cores\n",
    "    logical_cores = psutil.cpu_count(logical=True)  # Logical cores\n",
    "\n",
    "    # Get GPU details if a CUDA GPU is available\n",
    "    if gpu_available:\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (\n",
    "            1024**3\n",
    "        )  # Convert to GB\n",
    "        gpu_type = \"CUDA\"\n",
    "\n",
    "    # Get MPS details if an MPS GPU is available\n",
    "    elif mps_available:\n",
    "        gpu_count = 1  # MPS typically means 1 Apple GPU\n",
    "        gpu_mem = None  # No easy way to fetch memory for MPS currently\n",
    "        gpu_type = \"MPS\"\n",
    "\n",
    "    # If neither CUDA nor MPS is available\n",
    "    else:\n",
    "        gpu_count = 0\n",
    "        gpu_mem = 0\n",
    "        gpu_type = \"None\"\n",
    "\n",
    "    # Combine the hardware info into a dictionary\n",
    "    hardware_info = {\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"mps_available\": mps_available,\n",
    "        \"gpu_type\": gpu_type,\n",
    "        \"gpu_count\": gpu_count,\n",
    "        \"gpu_memory\": gpu_mem,\n",
    "        \"total_ram\": total_ram,\n",
    "        \"available_ram\": available_ram,\n",
    "        \"cpu_cores\": cpu_cores,\n",
    "        \"logical_cores\": logical_cores,\n",
    "    }\n",
    "\n",
    "    return hardware_info\n",
    "\n",
    "\n",
    "# Check hardware info\n",
    "hardware_info = get_hardware_info()\n",
    "print(hardware_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveStudentModel(nn.Module):\n",
    "    def __init__(self, teacher_model_name, hardware_info):\n",
    "        super(AdaptiveStudentModel, self).__init__()\n",
    "\n",
    "        # Load teacher model to get its hidden size\n",
    "        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            teacher_model_name, num_labels=2\n",
    "        )\n",
    "        hidden_size = self.teacher_model.config.hidden_size\n",
    "\n",
    "        # Adjust the student model architecture based on hardware\n",
    "        if hardware_info[\"gpu_available\"]:\n",
    "            if hardware_info[\"gpu_memory\"] < 4:  # For low-end GPUs\n",
    "                self.student_model = nn.Linear(hidden_size, 128)  # Fewer units\n",
    "            else:\n",
    "                self.student_model = nn.Linear(\n",
    "                    hidden_size, 256\n",
    "                )  # Higher-end GPUs get a larger model\n",
    "        elif hardware_info[\"mps_available\"]:\n",
    "            self.student_model = nn.Linear(\n",
    "                hidden_size, 128\n",
    "            )  # Adapt for MPS (Apple Silicon)\n",
    "        else:\n",
    "            self.student_model = nn.Linear(\n",
    "                hidden_size, 64\n",
    "            )  # Smaller model for CPU-only devices\n",
    "\n",
    "        self.output_layer = nn.Linear(\n",
    "            self.student_model.out_features, 2\n",
    "        )  # Output layer size stays the same\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        teacher_outputs = self.teacher_model(input_ids, attention_mask=attention_mask)\n",
    "        student_outputs = self.student_model(teacher_outputs[1])  # Use pooled output\n",
    "        return self.output_layer(student_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveStudentModel(nn.Module):\n",
    "    def __init__(self, teacher_model_name, hardware_info):\n",
    "        super(AdaptiveStudentModel, self).__init__()\n",
    "\n",
    "        # Load teacher model to get its hidden size\n",
    "        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_name, num_labels=2)\n",
    "        hidden_size = self.teacher_model.config.hidden_size\n",
    "\n",
    "        # Adjust the student model architecture based on hardware\n",
    "        if hardware_info[\"gpu_available\"]:\n",
    "            if hardware_info[\"gpu_memory\"] < 4:  # For low-end GPUs\n",
    "                self.student_model = nn.Linear(hidden_size, 128)  # Fewer units\n",
    "            else:\n",
    "                self.student_model = nn.Linear(hidden_size, 256)  # Higher-end GPUs get a larger model\n",
    "        elif hardware_info[\"mps_available\"]:\n",
    "            self.student_model = nn.Linear(hidden_size, 128)  # Adapt for MPS (Apple Silicon)\n",
    "        else:\n",
    "            self.student_model = nn.Linear(hidden_size, 64)  # Smaller model for CPU-only devices\n",
    "\n",
    "        self.output_layer = nn.Linear(self.student_model.out_features, 2)  # Output layer size stays the same\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        teacher_outputs = self.teacher_model(input_ids, attention_mask=attention_mask)\n",
    "        student_outputs = self.student_model(\n",
    "            teacher_outputs.pooler_output\n",
    "        )  # Use pooled output\n",
    "        return self.output_layer(student_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation loss function\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_probs = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "    # Kullback-Leibler (KL) Divergence\n",
    "    return nn.KLDivLoss()(student_probs, teacher_probs) * (temperature**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptive_batch_size(hardware_info):\n",
    "    if hardware_info[\"gpu_available\"]:\n",
    "        if hardware_info[\"gpu_memory\"] < 4: \n",
    "            return 16\n",
    "        else:  \n",
    "            return 32\n",
    "    elif hardware_info[\"mps_available\"]:\n",
    "        return 16  \n",
    "    else:\n",
    "        return 8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function (example implementation)\n",
    "def adaptive_train(\n",
    "    student_model, teacher_model, tokenized_datasets, hardware_info, epochs=3\n",
    "):\n",
    "    student_model.train()\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Define the DataLoader with dynamic padding and batching\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        batch_size=get_adaptive_batch_size(hardware_info),\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # input_ids and attention_mask are now PyTorch tensors\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Get teacher outputs\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(\n",
    "                    input_ids, attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "            # Get student outputs\n",
    "            student_outputs = student_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = distillation_loss(student_outputs, teacher_outputs.logits)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_evaluate(student_model, tokenized_datasets):\n",
    "    student_model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Use DataLoader for evaluation as well\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"test\"],\n",
    "        batch_size=get_adaptive_batch_size(hardware_info),\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = student_model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "        correct_predictions += (predictions == batch[\"label\"]).sum().item()\n",
    "        total_predictions += len(predictions)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardware_info = get_hardware_info()\n",
    "\n",
    "student_model = AdaptiveStudentModel(teacher_model_name, hardware_info).to(device)\n",
    "\n",
    "teacher_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 775\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:737\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43madaptive_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhardware_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m, in \u001b[0;36madaptive_train\u001b[0;34m(student_model, teacher_model, tokenized_datasets, hardware_info, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     10\u001b[0m     tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mget_adaptive_batch_size(hardware_info),\n\u001b[1;32m     12\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     13\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# input_ids and attention_mask are now PyTorch tensors\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3544\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3541\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3542\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mynewenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:791\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    790\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "adaptive_train(student_model, teacher_model, tokenized_datasets, hardware_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snehsuresh/anaconda3/envs/mynewenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'teacher_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define a data collator that pads sequences dynamically\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(\u001b[43mteacher_tokenizer\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Updated tokenization function with padding and truncation\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Tokenize each example and ensure padding, truncation, and correct tensor formatting\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'teacher_tokenizer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu_available': False, 'mps_available': True, 'gpu_type': 'MPS', 'gpu_count': 1, 'gpu_memory': None, 'total_ram': 8.0, 'available_ram': 1.310760498046875, 'cpu_cores': 8, 'logical_cores': 8}\n",
      "Epoch: 0, Loss: 0.6953972578048706\n",
      "Epoch: 0, Loss: 0.6927683353424072\n",
      "Epoch: 0, Loss: 0.6878712177276611\n",
      "Epoch: 0, Loss: 0.6898900866508484\n",
      "Epoch: 0, Loss: 0.698832631111145\n",
      "Epoch: 0, Loss: 0.6825283765792847\n",
      "Epoch: 0, Loss: 0.7095991969108582\n",
      "Epoch: 0, Loss: 0.6893239617347717\n",
      "Epoch: 0, Loss: 0.6985973715782166\n",
      "Epoch: 0, Loss: 0.6958765387535095\n",
      "Epoch: 0, Loss: 0.6848708391189575\n",
      "Epoch: 0, Loss: 0.6960991024971008\n",
      "Epoch: 0, Loss: 0.6945747137069702\n",
      "Epoch: 0, Loss: 0.6977376341819763\n",
      "Epoch: 0, Loss: 0.6892644166946411\n",
      "Epoch: 0, Loss: 0.6899850964546204\n",
      "Epoch: 0, Loss: 0.6892284154891968\n",
      "Epoch: 0, Loss: 0.6915101408958435\n",
      "Epoch: 0, Loss: 0.6922131776809692\n",
      "Epoch: 0, Loss: 0.7037166953086853\n",
      "Epoch: 0, Loss: 0.691405177116394\n",
      "Epoch: 0, Loss: 0.6984713673591614\n",
      "Epoch: 0, Loss: 0.701328456401825\n",
      "Epoch: 0, Loss: 0.6961019039154053\n",
      "Epoch: 0, Loss: 0.6969629526138306\n",
      "Epoch: 0, Loss: 0.6854199171066284\n",
      "Epoch: 0, Loss: 0.6898694038391113\n",
      "Epoch: 0, Loss: 0.691126823425293\n",
      "Epoch: 0, Loss: 0.6876258850097656\n",
      "Epoch: 0, Loss: 0.6879090666770935\n",
      "Epoch: 0, Loss: 0.687816858291626\n",
      "Epoch: 0, Loss: 0.689347505569458\n",
      "Epoch: 0, Loss: 0.6886767745018005\n",
      "Epoch: 0, Loss: 0.6863811612129211\n",
      "Epoch: 0, Loss: 0.6922398209571838\n",
      "Epoch: 0, Loss: 0.6918110847473145\n",
      "Epoch: 0, Loss: 0.691685140132904\n",
      "Epoch: 0, Loss: 0.6893341541290283\n",
      "Epoch: 0, Loss: 0.6917911171913147\n",
      "Epoch: 0, Loss: 0.6914187669754028\n",
      "Epoch: 0, Loss: 0.6879003047943115\n",
      "Epoch: 0, Loss: 0.6904407739639282\n",
      "Epoch: 0, Loss: 0.6857435703277588\n",
      "Epoch: 0, Loss: 0.6883798837661743\n",
      "Epoch: 0, Loss: 0.6880757808685303\n",
      "Epoch: 0, Loss: 0.6880276799201965\n",
      "Epoch: 0, Loss: 0.6928478479385376\n",
      "Epoch: 0, Loss: 0.6895613670349121\n",
      "Epoch: 0, Loss: 0.689986526966095\n",
      "Epoch: 0, Loss: 0.6929135322570801\n",
      "Epoch: 0, Loss: 0.6888238787651062\n",
      "Epoch: 0, Loss: 0.689525306224823\n",
      "Epoch: 0, Loss: 0.6944811344146729\n",
      "Epoch: 0, Loss: 0.6866500377655029\n",
      "Epoch: 0, Loss: 0.6867170333862305\n",
      "Epoch: 0, Loss: 0.6878862977027893\n",
      "Epoch: 0, Loss: 0.691074013710022\n",
      "Epoch: 0, Loss: 0.6895487308502197\n",
      "Epoch: 0, Loss: 0.6894165277481079\n",
      "Epoch: 0, Loss: 0.6879966855049133\n",
      "Epoch: 0, Loss: 0.6922942996025085\n",
      "Epoch: 0, Loss: 0.6869552135467529\n",
      "Epoch: 0, Loss: 0.6834701299667358\n",
      "Epoch: 0, Loss: 0.6861389875411987\n",
      "Epoch: 0, Loss: 0.6808534860610962\n",
      "Epoch: 0, Loss: 0.6893936991691589\n",
      "Epoch: 0, Loss: 0.6867964267730713\n",
      "Epoch: 0, Loss: 0.6867066025733948\n",
      "Epoch: 0, Loss: 0.6869922876358032\n",
      "Epoch: 0, Loss: 0.6826076507568359\n",
      "Epoch: 0, Loss: 0.6811283230781555\n",
      "Epoch: 0, Loss: 0.6932124495506287\n",
      "Epoch: 0, Loss: 0.6831855773925781\n",
      "Epoch: 0, Loss: 0.6828775405883789\n",
      "Epoch: 0, Loss: 0.6872639656066895\n",
      "Epoch: 0, Loss: 0.691534698009491\n",
      "Epoch: 0, Loss: 0.6830686926841736\n",
      "Epoch: 0, Loss: 0.6802416443824768\n",
      "Epoch: 0, Loss: 0.6917974948883057\n",
      "Epoch: 0, Loss: 0.6856886744499207\n",
      "Epoch: 0, Loss: 0.68666672706604\n",
      "Epoch: 0, Loss: 0.6896021962165833\n",
      "Epoch: 0, Loss: 0.6886394619941711\n",
      "Epoch: 0, Loss: 0.6854066848754883\n",
      "Epoch: 0, Loss: 0.676511287689209\n",
      "Epoch: 0, Loss: 0.680531919002533\n",
      "Epoch: 0, Loss: 0.6853700876235962\n",
      "Epoch: 0, Loss: 0.6868695616722107\n",
      "Epoch: 0, Loss: 0.6848645210266113\n",
      "Epoch: 0, Loss: 0.685132622718811\n",
      "Epoch: 0, Loss: 0.6811923384666443\n",
      "Epoch: 0, Loss: 0.6828285455703735\n",
      "Epoch: 0, Loss: 0.6787391901016235\n",
      "Epoch: 0, Loss: 0.6807733774185181\n",
      "Epoch: 0, Loss: 0.6743181943893433\n",
      "Epoch: 0, Loss: 0.6809967160224915\n",
      "Epoch: 0, Loss: 0.6937801837921143\n",
      "Epoch: 0, Loss: 0.6820310950279236\n",
      "Epoch: 0, Loss: 0.688940167427063\n",
      "Epoch: 0, Loss: 0.6821368336677551\n",
      "Epoch: 0, Loss: 0.6814898252487183\n",
      "Epoch: 0, Loss: 0.6845813393592834\n",
      "Epoch: 0, Loss: 0.6779358983039856\n",
      "Epoch: 0, Loss: 0.6852869391441345\n",
      "Epoch: 0, Loss: 0.6806337237358093\n",
      "Epoch: 0, Loss: 0.6755253672599792\n",
      "Epoch: 0, Loss: 0.6805406212806702\n",
      "Epoch: 0, Loss: 0.6905567049980164\n",
      "Epoch: 0, Loss: 0.6741589307785034\n",
      "Epoch: 0, Loss: 0.6838456988334656\n",
      "Epoch: 0, Loss: 0.6863867044448853\n",
      "Epoch: 0, Loss: 0.6750430464744568\n",
      "Epoch: 0, Loss: 0.6772562265396118\n",
      "Epoch: 0, Loss: 0.6750065088272095\n",
      "Epoch: 0, Loss: 0.6610472202301025\n",
      "Epoch: 0, Loss: 0.6855908036231995\n",
      "Epoch: 0, Loss: 0.6711735725402832\n",
      "Epoch: 0, Loss: 0.677112340927124\n",
      "Epoch: 0, Loss: 0.6834573745727539\n",
      "Epoch: 0, Loss: 0.6738265752792358\n",
      "Epoch: 0, Loss: 0.6749722957611084\n",
      "Epoch: 0, Loss: 0.6658331751823425\n",
      "Epoch: 0, Loss: 0.6844654679298401\n",
      "Epoch: 0, Loss: 0.6828091144561768\n",
      "Epoch: 0, Loss: 0.6830142736434937\n",
      "Epoch: 0, Loss: 0.6800827383995056\n",
      "Epoch: 0, Loss: 0.6865149140357971\n",
      "Epoch: 0, Loss: 0.6706692576408386\n",
      "Epoch: 0, Loss: 0.6713101267814636\n",
      "Epoch: 0, Loss: 0.6652321219444275\n",
      "Epoch: 0, Loss: 0.6749048233032227\n",
      "Epoch: 0, Loss: 0.6789353489875793\n",
      "Epoch: 0, Loss: 0.674172043800354\n",
      "Epoch: 0, Loss: 0.6817551255226135\n",
      "Epoch: 0, Loss: 0.6774007081985474\n",
      "Epoch: 0, Loss: 0.6740614175796509\n",
      "Epoch: 0, Loss: 0.6726504564285278\n",
      "Epoch: 0, Loss: 0.6774826049804688\n",
      "Epoch: 0, Loss: 0.6757397651672363\n",
      "Epoch: 0, Loss: 0.666214644908905\n",
      "Epoch: 0, Loss: 0.6780160665512085\n",
      "Epoch: 0, Loss: 0.6733214855194092\n",
      "Epoch: 0, Loss: 0.674456000328064\n",
      "Epoch: 0, Loss: 0.6737798452377319\n",
      "Epoch: 0, Loss: 0.6675495505332947\n",
      "Epoch: 0, Loss: 0.680001437664032\n",
      "Epoch: 0, Loss: 0.6734290719032288\n",
      "Epoch: 0, Loss: 0.667641282081604\n",
      "Epoch: 0, Loss: 0.6689667701721191\n",
      "Epoch: 0, Loss: 0.6707728505134583\n",
      "Epoch: 0, Loss: 0.6658573150634766\n",
      "Epoch: 0, Loss: 0.6521921753883362\n",
      "Epoch: 0, Loss: 0.6784688234329224\n",
      "Epoch: 0, Loss: 0.6632089018821716\n",
      "Epoch: 0, Loss: 0.6770111322402954\n",
      "Epoch: 0, Loss: 0.6631768941879272\n",
      "Epoch: 0, Loss: 0.6721532344818115\n",
      "Epoch: 0, Loss: 0.6712083220481873\n",
      "Epoch: 0, Loss: 0.676392674446106\n",
      "Epoch: 0, Loss: 0.6773865818977356\n",
      "Epoch: 0, Loss: 0.6488120555877686\n",
      "Epoch: 0, Loss: 0.6622620224952698\n",
      "Epoch: 0, Loss: 0.6685410141944885\n",
      "Epoch: 0, Loss: 0.6660401225090027\n",
      "Epoch: 0, Loss: 0.6719883680343628\n",
      "Epoch: 0, Loss: 0.6587125658988953\n",
      "Epoch: 0, Loss: 0.6592596769332886\n",
      "Epoch: 0, Loss: 0.6670984029769897\n",
      "Epoch: 0, Loss: 0.6534752249717712\n",
      "Epoch: 0, Loss: 0.6696133613586426\n",
      "Epoch: 0, Loss: 0.6534529328346252\n",
      "Epoch: 0, Loss: 0.6586508750915527\n",
      "Epoch: 0, Loss: 0.6503796577453613\n",
      "Epoch: 0, Loss: 0.6579142808914185\n",
      "Epoch: 0, Loss: 0.681652843952179\n",
      "Epoch: 0, Loss: 0.6731364727020264\n",
      "Epoch: 0, Loss: 0.6654821038246155\n",
      "Epoch: 0, Loss: 0.6549510359764099\n",
      "Epoch: 0, Loss: 0.6639975309371948\n",
      "Epoch: 0, Loss: 0.6596336364746094\n",
      "Epoch: 0, Loss: 0.6492545008659363\n",
      "Epoch: 0, Loss: 0.6622283458709717\n",
      "Epoch: 0, Loss: 0.6543861627578735\n",
      "Epoch: 0, Loss: 0.6745374202728271\n",
      "Epoch: 0, Loss: 0.6675843596458435\n",
      "Epoch: 0, Loss: 0.6499801278114319\n",
      "Epoch: 0, Loss: 0.6716322302818298\n",
      "Epoch: 0, Loss: 0.6746900081634521\n",
      "Epoch: 0, Loss: 0.6603463888168335\n",
      "Epoch: 0, Loss: 0.656910240650177\n",
      "Epoch: 0, Loss: 0.634824275970459\n",
      "Epoch: 0, Loss: 0.6558564901351929\n",
      "Epoch: 0, Loss: 0.6451365947723389\n",
      "Epoch: 0, Loss: 0.6567830443382263\n",
      "Epoch: 0, Loss: 0.6443585753440857\n",
      "Epoch: 0, Loss: 0.6676989197731018\n",
      "Epoch: 0, Loss: 0.6765520572662354\n",
      "Epoch: 0, Loss: 0.6501724720001221\n",
      "Epoch: 0, Loss: 0.659448504447937\n",
      "Epoch: 0, Loss: 0.6340330243110657\n",
      "Epoch: 0, Loss: 0.6357918977737427\n",
      "Epoch: 0, Loss: 0.6566965579986572\n",
      "Epoch: 0, Loss: 0.6379483342170715\n",
      "Epoch: 0, Loss: 0.6556244492530823\n",
      "Epoch: 0, Loss: 0.637484610080719\n",
      "Epoch: 0, Loss: 0.6746727228164673\n",
      "Epoch: 0, Loss: 0.6422867774963379\n",
      "Epoch: 0, Loss: 0.6662697792053223\n",
      "Epoch: 0, Loss: 0.6395711302757263\n",
      "Epoch: 0, Loss: 0.6441338658332825\n",
      "Epoch: 0, Loss: 0.6340243220329285\n",
      "Epoch: 0, Loss: 0.6414484977722168\n",
      "Epoch: 0, Loss: 0.6598319411277771\n",
      "Epoch: 0, Loss: 0.635708749294281\n",
      "Epoch: 0, Loss: 0.6432970762252808\n",
      "Epoch: 0, Loss: 0.6270362138748169\n",
      "Epoch: 0, Loss: 0.6298603415489197\n",
      "Epoch: 0, Loss: 0.665964663028717\n",
      "Epoch: 0, Loss: 0.639506459236145\n",
      "Epoch: 0, Loss: 0.6307393908500671\n",
      "Epoch: 0, Loss: 0.64585942029953\n",
      "Epoch: 0, Loss: 0.6366307139396667\n",
      "Epoch: 0, Loss: 0.6265152096748352\n",
      "Epoch: 0, Loss: 0.6251382827758789\n",
      "Epoch: 0, Loss: 0.6296051144599915\n",
      "Epoch: 0, Loss: 0.6397256851196289\n",
      "Epoch: 0, Loss: 0.6490808129310608\n",
      "Epoch: 0, Loss: 0.6446053981781006\n",
      "Epoch: 0, Loss: 0.6439850926399231\n",
      "Epoch: 0, Loss: 0.6321269869804382\n",
      "Epoch: 0, Loss: 0.6348706483840942\n",
      "Epoch: 0, Loss: 0.5960769653320312\n",
      "Epoch: 0, Loss: 0.645589292049408\n",
      "Epoch: 0, Loss: 0.6595419049263\n",
      "Epoch: 0, Loss: 0.6754179000854492\n",
      "Epoch: 0, Loss: 0.6919394731521606\n",
      "Epoch: 0, Loss: 0.6373244524002075\n",
      "Epoch: 0, Loss: 0.6086366772651672\n",
      "Epoch: 0, Loss: 0.6398801803588867\n",
      "Epoch: 0, Loss: 0.6532332301139832\n",
      "Epoch: 0, Loss: 0.6511192321777344\n",
      "Epoch: 0, Loss: 0.6133647561073303\n",
      "Epoch: 0, Loss: 0.6106792092323303\n",
      "Epoch: 0, Loss: 0.6048769950866699\n",
      "Epoch: 0, Loss: 0.6516802310943604\n",
      "Epoch: 0, Loss: 0.6105919480323792\n",
      "Epoch: 0, Loss: 0.6028250455856323\n",
      "Epoch: 0, Loss: 0.6330875754356384\n",
      "Epoch: 0, Loss: 0.630042552947998\n",
      "Epoch: 0, Loss: 0.608588457107544\n",
      "Epoch: 0, Loss: 0.6777198910713196\n",
      "Epoch: 0, Loss: 0.6413301229476929\n",
      "Epoch: 0, Loss: 0.6267839074134827\n",
      "Epoch: 0, Loss: 0.6535041928291321\n",
      "Epoch: 0, Loss: 0.5934047698974609\n",
      "Epoch: 0, Loss: 0.6300958395004272\n",
      "Epoch: 0, Loss: 0.6300742626190186\n",
      "Epoch: 0, Loss: 0.5591577291488647\n",
      "Epoch: 0, Loss: 0.6298770904541016\n",
      "Epoch: 0, Loss: 0.6366325616836548\n",
      "Epoch: 0, Loss: 0.6304073929786682\n",
      "Epoch: 0, Loss: 0.6549077033996582\n",
      "Epoch: 0, Loss: 0.6141921877861023\n",
      "Epoch: 0, Loss: 0.5976516008377075\n",
      "Epoch: 0, Loss: 0.6172318458557129\n",
      "Epoch: 0, Loss: 0.6014626026153564\n",
      "Epoch: 0, Loss: 0.6199555397033691\n",
      "Epoch: 0, Loss: 0.6195312738418579\n",
      "Epoch: 0, Loss: 0.6506845951080322\n",
      "Epoch: 0, Loss: 0.6630499362945557\n",
      "Epoch: 0, Loss: 0.6083444952964783\n",
      "Epoch: 0, Loss: 0.6427361369132996\n",
      "Epoch: 0, Loss: 0.6337538957595825\n",
      "Epoch: 0, Loss: 0.6226000189781189\n",
      "Epoch: 0, Loss: 0.6094152331352234\n",
      "Epoch: 0, Loss: 0.6003174185752869\n",
      "Epoch: 0, Loss: 0.6464866399765015\n",
      "Epoch: 0, Loss: 0.6643263101577759\n",
      "Epoch: 0, Loss: 0.6241114139556885\n",
      "Epoch: 0, Loss: 0.6193282604217529\n",
      "Epoch: 0, Loss: 0.5987553000450134\n",
      "Epoch: 0, Loss: 0.5990341901779175\n",
      "Epoch: 0, Loss: 0.651512622833252\n",
      "Epoch: 0, Loss: 0.6316088438034058\n",
      "Epoch: 0, Loss: 0.6322129964828491\n",
      "Epoch: 0, Loss: 0.6069590449333191\n",
      "Epoch: 0, Loss: 0.5744457840919495\n",
      "Epoch: 0, Loss: 0.5847464799880981\n",
      "Epoch: 0, Loss: 0.6003209352493286\n",
      "Epoch: 0, Loss: 0.6299652457237244\n",
      "Epoch: 0, Loss: 0.6150820851325989\n",
      "Epoch: 0, Loss: 0.5981894731521606\n",
      "Epoch: 0, Loss: 0.5861380100250244\n",
      "Epoch: 0, Loss: 0.6222941875457764\n",
      "Epoch: 0, Loss: 0.6116873025894165\n",
      "Epoch: 0, Loss: 0.6053367257118225\n",
      "Epoch: 0, Loss: 0.6165176630020142\n",
      "Epoch: 0, Loss: 0.6335054636001587\n",
      "Epoch: 0, Loss: 0.6168549060821533\n",
      "Epoch: 0, Loss: 0.5451468825340271\n",
      "Epoch: 0, Loss: 0.6351629495620728\n",
      "Epoch: 0, Loss: 0.6369349956512451\n",
      "Epoch: 0, Loss: 0.6105542778968811\n",
      "Epoch: 0, Loss: 0.6542360782623291\n",
      "Epoch: 0, Loss: 0.5308860540390015\n",
      "Epoch: 0, Loss: 0.6178562641143799\n",
      "Epoch: 0, Loss: 0.5772262811660767\n",
      "Epoch: 0, Loss: 0.6221488118171692\n",
      "Epoch: 0, Loss: 0.606839656829834\n",
      "Epoch: 0, Loss: 0.530081033706665\n",
      "Epoch: 0, Loss: 0.5763626098632812\n",
      "Epoch: 0, Loss: 0.5785151124000549\n",
      "Epoch: 0, Loss: 0.5885118246078491\n",
      "Epoch: 0, Loss: 0.5637285709381104\n",
      "Epoch: 0, Loss: 0.5522823929786682\n",
      "Epoch: 0, Loss: 0.639014720916748\n",
      "Epoch: 0, Loss: 0.5616552233695984\n",
      "Epoch: 0, Loss: 0.5894182324409485\n",
      "Epoch: 0, Loss: 0.6118132472038269\n",
      "Epoch: 0, Loss: 0.6704006195068359\n",
      "Epoch: 0, Loss: 0.5529085397720337\n",
      "Epoch: 0, Loss: 0.5789417028427124\n",
      "Epoch: 0, Loss: 0.5751451849937439\n",
      "Epoch: 0, Loss: 0.5986461639404297\n",
      "Epoch: 0, Loss: 0.5720913410186768\n",
      "Epoch: 0, Loss: 0.5349893569946289\n",
      "Epoch: 0, Loss: 0.6595902442932129\n",
      "Epoch: 0, Loss: 0.634809672832489\n",
      "Epoch: 0, Loss: 0.5697879791259766\n",
      "Epoch: 0, Loss: 0.549644410610199\n",
      "Epoch: 0, Loss: 0.5398914813995361\n",
      "Epoch: 0, Loss: 0.5961395502090454\n",
      "Epoch: 0, Loss: 0.5625014901161194\n",
      "Epoch: 0, Loss: 0.5601915717124939\n",
      "Epoch: 0, Loss: 0.6227168440818787\n",
      "Epoch: 0, Loss: 0.5818401575088501\n",
      "Epoch: 0, Loss: 0.5611129403114319\n",
      "Epoch: 0, Loss: 0.6228819489479065\n",
      "Epoch: 0, Loss: 0.6010577082633972\n",
      "Epoch: 0, Loss: 0.6244739294052124\n",
      "Epoch: 0, Loss: 0.5594896674156189\n",
      "Epoch: 0, Loss: 0.6414289474487305\n",
      "Epoch: 0, Loss: 0.535765528678894\n",
      "Epoch: 0, Loss: 0.6589533090591431\n",
      "Epoch: 0, Loss: 0.5665594339370728\n",
      "Epoch: 0, Loss: 0.6588269472122192\n",
      "Epoch: 0, Loss: 0.5600957274436951\n",
      "Epoch: 0, Loss: 0.5805445909500122\n",
      "Epoch: 0, Loss: 0.6223989725112915\n",
      "Epoch: 0, Loss: 0.5921599268913269\n",
      "Epoch: 0, Loss: 0.6361913681030273\n",
      "Epoch: 0, Loss: 0.579465925693512\n",
      "Epoch: 0, Loss: 0.5915742516517639\n",
      "Epoch: 0, Loss: 0.5885800719261169\n",
      "Epoch: 0, Loss: 0.5439243316650391\n",
      "Epoch: 0, Loss: 0.5355506539344788\n",
      "Epoch: 0, Loss: 0.6301496624946594\n",
      "Epoch: 0, Loss: 0.5834278464317322\n",
      "Epoch: 0, Loss: 0.5571791529655457\n",
      "Epoch: 0, Loss: 0.6261489987373352\n",
      "Epoch: 0, Loss: 0.6363938450813293\n",
      "Epoch: 0, Loss: 0.5800751447677612\n",
      "Epoch: 0, Loss: 0.543519914150238\n",
      "Epoch: 0, Loss: 0.5747111439704895\n",
      "Epoch: 0, Loss: 0.6275791525840759\n",
      "Epoch: 0, Loss: 0.6152938008308411\n",
      "Epoch: 0, Loss: 0.5138412117958069\n",
      "Epoch: 0, Loss: 0.5737563967704773\n",
      "Epoch: 0, Loss: 0.5464276671409607\n",
      "Epoch: 0, Loss: 0.5319446325302124\n",
      "Epoch: 0, Loss: 0.5408347249031067\n",
      "Epoch: 0, Loss: 0.5406126379966736\n",
      "Epoch: 0, Loss: 0.4943203628063202\n",
      "Epoch: 0, Loss: 0.5348418354988098\n",
      "Epoch: 0, Loss: 0.5319505333900452\n",
      "Epoch: 0, Loss: 0.6733024716377258\n",
      "Epoch: 0, Loss: 0.6602799892425537\n",
      "Epoch: 0, Loss: 0.655362606048584\n",
      "Epoch: 0, Loss: 0.6343561410903931\n",
      "Epoch: 0, Loss: 0.5688624382019043\n",
      "Epoch: 0, Loss: 0.5066214799880981\n",
      "Epoch: 0, Loss: 0.5662645101547241\n",
      "Epoch: 0, Loss: 0.5813763737678528\n",
      "Epoch: 0, Loss: 0.5061665177345276\n",
      "Epoch: 0, Loss: 0.5448934435844421\n",
      "Epoch: 0, Loss: 0.5572819113731384\n",
      "Epoch: 0, Loss: 0.5347655415534973\n",
      "Epoch: 0, Loss: 0.5343620181083679\n",
      "Epoch: 0, Loss: 0.6057339310646057\n",
      "Epoch: 0, Loss: 0.5008494257926941\n",
      "Epoch: 0, Loss: 0.5550261735916138\n",
      "Epoch: 0, Loss: 0.6137393712997437\n",
      "Epoch: 0, Loss: 0.4954889118671417\n",
      "Epoch: 0, Loss: 0.555742084980011\n",
      "Epoch: 0, Loss: 0.6163573265075684\n",
      "Epoch: 0, Loss: 0.5553011298179626\n",
      "Epoch: 0, Loss: 0.5033062696456909\n",
      "Epoch: 0, Loss: 0.5934810638427734\n",
      "Epoch: 0, Loss: 0.4551779627799988\n",
      "Epoch: 0, Loss: 0.49023616313934326\n",
      "Epoch: 0, Loss: 0.5662118196487427\n",
      "Epoch: 0, Loss: 0.6486402153968811\n",
      "Epoch: 0, Loss: 0.5188793540000916\n",
      "Epoch: 0, Loss: 0.5217095017433167\n",
      "Epoch: 0, Loss: 0.5643817186355591\n",
      "Epoch: 0, Loss: 0.5393787622451782\n",
      "Epoch: 0, Loss: 0.5932075381278992\n",
      "Epoch: 0, Loss: 0.5771791338920593\n",
      "Epoch: 0, Loss: 0.5612421035766602\n",
      "Epoch: 0, Loss: 0.5230212211608887\n",
      "Epoch: 0, Loss: 0.5516949892044067\n",
      "Epoch: 0, Loss: 0.61179518699646\n",
      "Epoch: 0, Loss: 0.6175264716148376\n",
      "Epoch: 0, Loss: 0.5344900488853455\n",
      "Epoch: 0, Loss: 0.5814322829246521\n",
      "Epoch: 0, Loss: 0.6010810732841492\n",
      "Epoch: 0, Loss: 0.5749675035476685\n",
      "Epoch: 0, Loss: 0.5260844826698303\n",
      "Epoch: 0, Loss: 0.5411802530288696\n",
      "Epoch: 0, Loss: 0.5481464266777039\n",
      "Epoch: 0, Loss: 0.5140751004219055\n",
      "Epoch: 0, Loss: 0.5555626153945923\n",
      "Epoch: 0, Loss: 0.5367746353149414\n",
      "Epoch: 0, Loss: 0.5176302194595337\n",
      "Epoch: 0, Loss: 0.5306926965713501\n",
      "Epoch: 0, Loss: 0.5111991763114929\n",
      "Epoch: 0, Loss: 0.5654965043067932\n",
      "Epoch: 0, Loss: 0.5846299529075623\n",
      "Epoch: 0, Loss: 0.5037466287612915\n",
      "Epoch: 0, Loss: 0.637683629989624\n",
      "Epoch: 0, Loss: 0.5484017133712769\n",
      "Epoch: 0, Loss: 0.6637011766433716\n",
      "Epoch: 0, Loss: 0.5406496524810791\n",
      "Epoch: 0, Loss: 0.5631932616233826\n",
      "Epoch: 0, Loss: 0.5007018446922302\n",
      "Epoch: 0, Loss: 0.565667986869812\n",
      "Epoch: 0, Loss: 0.5828596353530884\n",
      "Epoch: 0, Loss: 0.6012887954711914\n",
      "Epoch: 0, Loss: 0.5693272948265076\n",
      "Epoch: 0, Loss: 0.5410732626914978\n",
      "Epoch: 0, Loss: 0.6336714625358582\n",
      "Epoch: 0, Loss: 0.5468184351921082\n",
      "Epoch: 0, Loss: 0.586509108543396\n",
      "Epoch: 0, Loss: 0.6033921241760254\n",
      "Epoch: 0, Loss: 0.5435819029808044\n",
      "Epoch: 0, Loss: 0.4883253276348114\n",
      "Epoch: 0, Loss: 0.5521124005317688\n",
      "Epoch: 0, Loss: 0.5913146734237671\n",
      "Epoch: 0, Loss: 0.5185408592224121\n",
      "Epoch: 0, Loss: 0.44089585542678833\n",
      "Epoch: 0, Loss: 0.6115493774414062\n",
      "Epoch: 0, Loss: 0.483256995677948\n",
      "Epoch: 0, Loss: 0.5197611451148987\n",
      "Epoch: 0, Loss: 0.5773440599441528\n",
      "Epoch: 0, Loss: 0.4860771894454956\n",
      "Epoch: 0, Loss: 0.5551239252090454\n",
      "Epoch: 0, Loss: 0.564163088798523\n",
      "Epoch: 0, Loss: 0.6034409999847412\n",
      "Epoch: 0, Loss: 0.4915081262588501\n",
      "Epoch: 0, Loss: 0.6592004299163818\n",
      "Epoch: 0, Loss: 0.6047559976577759\n",
      "Epoch: 0, Loss: 0.5082500576972961\n",
      "Epoch: 0, Loss: 0.5268588662147522\n",
      "Epoch: 0, Loss: 0.6255186796188354\n",
      "Epoch: 0, Loss: 0.5911937952041626\n",
      "Epoch: 0, Loss: 0.6278920769691467\n",
      "Epoch: 0, Loss: 0.5631874799728394\n",
      "Epoch: 0, Loss: 0.5729976296424866\n",
      "Epoch: 0, Loss: 0.5335482358932495\n",
      "Epoch: 0, Loss: 0.47162115573883057\n",
      "Epoch: 0, Loss: 0.513609766960144\n",
      "Epoch: 0, Loss: 0.5426929593086243\n",
      "Epoch: 0, Loss: 0.5269873142242432\n",
      "Epoch: 0, Loss: 0.6093928217887878\n",
      "Epoch: 0, Loss: 0.583401620388031\n",
      "Epoch: 0, Loss: 0.5980659127235413\n",
      "Epoch: 0, Loss: 0.49229106307029724\n",
      "Epoch: 0, Loss: 0.5448062419891357\n",
      "Epoch: 0, Loss: 0.4300903081893921\n",
      "Epoch: 0, Loss: 0.5456621646881104\n",
      "Epoch: 0, Loss: 0.6010146737098694\n",
      "Epoch: 0, Loss: 0.5478942394256592\n",
      "Epoch: 0, Loss: 0.5097780227661133\n",
      "Epoch: 0, Loss: 0.45528799295425415\n",
      "Epoch: 0, Loss: 0.5376117825508118\n",
      "Epoch: 0, Loss: 0.5141692161560059\n",
      "Epoch: 0, Loss: 0.5439546704292297\n",
      "Epoch: 0, Loss: 0.6024864912033081\n",
      "Epoch: 0, Loss: 0.7170222997665405\n",
      "Epoch: 0, Loss: 0.5847148299217224\n",
      "Epoch: 0, Loss: 0.5404689311981201\n",
      "Epoch: 0, Loss: 0.533991813659668\n",
      "Epoch: 0, Loss: 0.5677811503410339\n",
      "Epoch: 0, Loss: 0.5120427012443542\n",
      "Epoch: 0, Loss: 0.498232901096344\n",
      "Epoch: 0, Loss: 0.5184358954429626\n",
      "Epoch: 0, Loss: 0.5295521020889282\n",
      "Epoch: 0, Loss: 0.665131688117981\n",
      "Epoch: 0, Loss: 0.5683158040046692\n",
      "Epoch: 0, Loss: 0.46926331520080566\n",
      "Epoch: 0, Loss: 0.5398102402687073\n",
      "Epoch: 0, Loss: 0.5230569243431091\n",
      "Epoch: 0, Loss: 0.5783650279045105\n",
      "Epoch: 0, Loss: 0.5395922064781189\n",
      "Epoch: 0, Loss: 0.4645822048187256\n",
      "Epoch: 0, Loss: 0.5742350816726685\n",
      "Epoch: 0, Loss: 0.5994581580162048\n",
      "Epoch: 0, Loss: 0.513174295425415\n",
      "Epoch: 0, Loss: 0.4438718557357788\n",
      "Epoch: 0, Loss: 0.6025988459587097\n",
      "Epoch: 0, Loss: 0.5600630044937134\n",
      "Epoch: 0, Loss: 0.47854581475257874\n",
      "Epoch: 0, Loss: 0.6025315523147583\n",
      "Epoch: 0, Loss: 0.5548591613769531\n",
      "Epoch: 0, Loss: 0.5238445401191711\n",
      "Epoch: 0, Loss: 0.4886741042137146\n",
      "Epoch: 0, Loss: 0.46867361664772034\n",
      "Epoch: 0, Loss: 0.5145577192306519\n",
      "Epoch: 0, Loss: 0.561028778553009\n",
      "Epoch: 0, Loss: 0.5901601314544678\n",
      "Epoch: 0, Loss: 0.5227728486061096\n",
      "Epoch: 0, Loss: 0.5188682675361633\n",
      "Epoch: 0, Loss: 0.5951359272003174\n",
      "Epoch: 0, Loss: 0.5532853007316589\n",
      "Epoch: 0, Loss: 0.5265604853630066\n",
      "Epoch: 0, Loss: 0.5557560324668884\n",
      "Epoch: 0, Loss: 0.6570252776145935\n",
      "Epoch: 0, Loss: 0.4595484137535095\n",
      "Epoch: 0, Loss: 0.5523744225502014\n",
      "Epoch: 0, Loss: 0.4973377287387848\n",
      "Epoch: 0, Loss: 0.5793087482452393\n",
      "Epoch: 0, Loss: 0.6695089340209961\n",
      "Epoch: 0, Loss: 0.5054791569709778\n",
      "Epoch: 0, Loss: 0.5404242873191833\n",
      "Epoch: 0, Loss: 0.5262628793716431\n",
      "Epoch: 0, Loss: 0.6231866478919983\n",
      "Epoch: 0, Loss: 0.5517048835754395\n",
      "Epoch: 0, Loss: 0.49470528960227966\n",
      "Epoch: 0, Loss: 0.48226162791252136\n",
      "Epoch: 0, Loss: 0.5090298652648926\n",
      "Epoch: 0, Loss: 0.4520793557167053\n",
      "Epoch: 0, Loss: 0.5437523722648621\n",
      "Epoch: 0, Loss: 0.4569319486618042\n",
      "Epoch: 0, Loss: 0.5766642093658447\n",
      "Epoch: 0, Loss: 0.6077477335929871\n",
      "Epoch: 0, Loss: 0.5793396234512329\n",
      "Epoch: 0, Loss: 0.5530886650085449\n",
      "Epoch: 0, Loss: 0.5600312352180481\n",
      "Epoch: 0, Loss: 0.5494722127914429\n",
      "Epoch: 0, Loss: 0.5467722415924072\n",
      "Epoch: 0, Loss: 0.5252202153205872\n",
      "Epoch: 0, Loss: 0.5835875868797302\n",
      "Epoch: 0, Loss: 0.4393129050731659\n",
      "Epoch: 0, Loss: 0.5087992548942566\n",
      "Epoch: 0, Loss: 0.5379397869110107\n",
      "Epoch: 0, Loss: 0.5273637175559998\n",
      "Epoch: 0, Loss: 0.45023712515830994\n",
      "Epoch: 0, Loss: 0.6543847322463989\n",
      "Epoch: 0, Loss: 0.5216275453567505\n",
      "Epoch: 0, Loss: 0.49780264496803284\n",
      "Epoch: 0, Loss: 0.6565393209457397\n",
      "Epoch: 0, Loss: 0.5443540215492249\n",
      "Epoch: 0, Loss: 0.4484109580516815\n",
      "Epoch: 0, Loss: 0.5256101489067078\n",
      "Epoch: 0, Loss: 0.5144503116607666\n",
      "Epoch: 0, Loss: 0.5033360719680786\n",
      "Epoch: 0, Loss: 0.584277331829071\n",
      "Epoch: 0, Loss: 0.5612420439720154\n",
      "Epoch: 0, Loss: 0.48078349232673645\n",
      "Epoch: 0, Loss: 0.47973692417144775\n",
      "Epoch: 0, Loss: 0.570479154586792\n",
      "Epoch: 0, Loss: 0.5626190304756165\n",
      "Epoch: 0, Loss: 0.5560426115989685\n",
      "Epoch: 0, Loss: 0.5135892629623413\n",
      "Epoch: 0, Loss: 0.6304321885108948\n",
      "Epoch: 0, Loss: 0.5412489175796509\n",
      "Epoch: 0, Loss: 0.48810821771621704\n",
      "Epoch: 0, Loss: 0.5201724767684937\n",
      "Epoch: 0, Loss: 0.5040712952613831\n",
      "Epoch: 0, Loss: 0.4577246904373169\n",
      "Epoch: 0, Loss: 0.56662517786026\n",
      "Epoch: 0, Loss: 0.5250073671340942\n",
      "Epoch: 0, Loss: 0.47219881415367126\n",
      "Epoch: 0, Loss: 0.5315923690795898\n",
      "Epoch: 0, Loss: 0.45143014192581177\n",
      "Epoch: 0, Loss: 0.4491269588470459\n",
      "Epoch: 0, Loss: 0.5152769088745117\n",
      "Epoch: 0, Loss: 0.5507416725158691\n",
      "Epoch: 0, Loss: 0.6507073640823364\n",
      "Epoch: 0, Loss: 0.5665910243988037\n",
      "Epoch: 0, Loss: 0.46664172410964966\n",
      "Epoch: 0, Loss: 0.4656308591365814\n",
      "Epoch: 0, Loss: 0.5375601649284363\n",
      "Epoch: 0, Loss: 0.6372883915901184\n",
      "Epoch: 0, Loss: 0.4336414635181427\n",
      "Epoch: 0, Loss: 0.4862436056137085\n",
      "Epoch: 0, Loss: 0.4385288655757904\n",
      "Epoch: 0, Loss: 0.5049439668655396\n",
      "Epoch: 0, Loss: 0.5792115926742554\n",
      "Epoch: 0, Loss: 0.4754977524280548\n",
      "Epoch: 0, Loss: 0.5885365009307861\n",
      "Epoch: 0, Loss: 0.45974087715148926\n",
      "Epoch: 0, Loss: 0.5617532730102539\n",
      "Epoch: 0, Loss: 0.46724432706832886\n",
      "Epoch: 0, Loss: 0.5112193822860718\n",
      "Epoch: 0, Loss: 0.5494412779808044\n",
      "Epoch: 0, Loss: 0.5609757304191589\n",
      "Epoch: 0, Loss: 0.6360911130905151\n",
      "Epoch: 0, Loss: 0.40057578682899475\n",
      "Epoch: 0, Loss: 0.4335656762123108\n",
      "Epoch: 0, Loss: 0.5078275799751282\n",
      "Epoch: 0, Loss: 0.4302697479724884\n",
      "Epoch: 0, Loss: 0.5258110761642456\n",
      "Epoch: 0, Loss: 0.4849608838558197\n",
      "Epoch: 0, Loss: 0.48554956912994385\n",
      "Epoch: 0, Loss: 0.5104578137397766\n",
      "Epoch: 0, Loss: 0.4551358222961426\n",
      "Epoch: 0, Loss: 0.5699979662895203\n",
      "Epoch: 0, Loss: 0.550774335861206\n",
      "Epoch: 0, Loss: 0.4393965005874634\n",
      "Epoch: 0, Loss: 0.4634166955947876\n",
      "Epoch: 0, Loss: 0.498805433511734\n",
      "Epoch: 0, Loss: 0.5030903220176697\n",
      "Epoch: 0, Loss: 0.5735107660293579\n",
      "Epoch: 0, Loss: 0.564410388469696\n",
      "Epoch: 0, Loss: 0.5830222368240356\n",
      "Epoch: 0, Loss: 0.5073564052581787\n",
      "Epoch: 0, Loss: 0.5139733552932739\n",
      "Epoch: 0, Loss: 0.5363038182258606\n",
      "Epoch: 0, Loss: 0.5024137496948242\n",
      "Epoch: 0, Loss: 0.4289364814758301\n",
      "Epoch: 0, Loss: 0.6379246711730957\n",
      "Epoch: 0, Loss: 0.4344887435436249\n",
      "Epoch: 0, Loss: 0.5142050385475159\n",
      "Epoch: 0, Loss: 0.5378995537757874\n",
      "Epoch: 0, Loss: 0.5924909114837646\n",
      "Epoch: 0, Loss: 0.46857377886772156\n",
      "Epoch: 0, Loss: 0.5462093949317932\n",
      "Epoch: 0, Loss: 0.5545479655265808\n",
      "Epoch: 0, Loss: 0.5364454388618469\n",
      "Epoch: 0, Loss: 0.5272385478019714\n",
      "Epoch: 0, Loss: 0.5423353314399719\n",
      "Epoch: 0, Loss: 0.5046553611755371\n",
      "Epoch: 0, Loss: 0.6774890422821045\n",
      "Epoch: 0, Loss: 0.597072422504425\n",
      "Epoch: 0, Loss: 0.5711968541145325\n",
      "Epoch: 0, Loss: 0.46486368775367737\n",
      "Epoch: 0, Loss: 0.5528897643089294\n",
      "Epoch: 0, Loss: 0.4280942380428314\n",
      "Epoch: 0, Loss: 0.46501168608665466\n",
      "Epoch: 0, Loss: 0.6384695768356323\n",
      "Epoch: 0, Loss: 0.39866921305656433\n",
      "Epoch: 0, Loss: 0.6309840679168701\n",
      "Epoch: 0, Loss: 0.5203753709793091\n",
      "Epoch: 0, Loss: 0.49823224544525146\n",
      "Epoch: 0, Loss: 0.40615251660346985\n",
      "Epoch: 0, Loss: 0.7834933996200562\n",
      "Epoch: 0, Loss: 0.524931013584137\n",
      "Epoch: 0, Loss: 0.6496812105178833\n",
      "Epoch: 0, Loss: 0.5825306177139282\n",
      "Epoch: 0, Loss: 0.5017251968383789\n",
      "Epoch: 0, Loss: 0.5185704231262207\n",
      "Epoch: 0, Loss: 0.5040374994277954\n",
      "Epoch: 0, Loss: 0.4684829115867615\n",
      "Epoch: 0, Loss: 0.6045143008232117\n",
      "Epoch: 0, Loss: 0.4807804822921753\n",
      "Epoch: 0, Loss: 0.44438397884368896\n",
      "Epoch: 0, Loss: 0.6263507008552551\n",
      "Epoch: 0, Loss: 0.5386610627174377\n",
      "Epoch: 0, Loss: 0.44902047514915466\n",
      "Epoch: 0, Loss: 0.6559473276138306\n",
      "Epoch: 0, Loss: 0.5554575324058533\n",
      "Epoch: 0, Loss: 0.5124465227127075\n",
      "Epoch: 0, Loss: 0.4616362750530243\n",
      "Epoch: 0, Loss: 0.46897637844085693\n",
      "Epoch: 0, Loss: 0.4381581246852875\n",
      "Epoch: 0, Loss: 0.6883898377418518\n",
      "Epoch: 0, Loss: 0.5407524108886719\n",
      "Epoch: 0, Loss: 0.5434671640396118\n",
      "Epoch: 0, Loss: 0.5147526860237122\n",
      "Epoch: 0, Loss: 0.53076171875\n",
      "Epoch: 0, Loss: 0.531660795211792\n",
      "Epoch: 0, Loss: 0.493468701839447\n",
      "Epoch: 0, Loss: 0.5566264390945435\n",
      "Epoch: 0, Loss: 0.5105371475219727\n",
      "Epoch: 0, Loss: 0.6302798986434937\n",
      "Epoch: 0, Loss: 0.5283123254776001\n",
      "Epoch: 0, Loss: 0.4843890070915222\n",
      "Epoch: 0, Loss: 0.4435137212276459\n",
      "Epoch: 0, Loss: 0.5097401738166809\n",
      "Epoch: 0, Loss: 0.478066623210907\n",
      "Epoch: 0, Loss: 0.51157546043396\n",
      "Epoch: 0, Loss: 0.5715339183807373\n",
      "Epoch: 0, Loss: 0.46874386072158813\n",
      "Epoch: 0, Loss: 0.502228856086731\n",
      "Epoch: 0, Loss: 0.464399516582489\n",
      "Epoch: 0, Loss: 0.5087495446205139\n",
      "Epoch: 0, Loss: 0.5087513327598572\n",
      "Epoch: 0, Loss: 0.5244603157043457\n",
      "Epoch: 0, Loss: 0.5204182863235474\n",
      "Epoch: 0, Loss: 0.47164028882980347\n",
      "Epoch: 0, Loss: 0.5722029805183411\n",
      "Epoch: 0, Loss: 0.4602603018283844\n",
      "Epoch: 0, Loss: 0.4807741940021515\n",
      "Epoch: 0, Loss: 0.3837182819843292\n",
      "Epoch: 0, Loss: 0.5043237805366516\n",
      "Epoch: 0, Loss: 0.4455040991306305\n",
      "Epoch: 0, Loss: 0.5645276308059692\n",
      "Epoch: 0, Loss: 0.5277878046035767\n",
      "Epoch: 0, Loss: 0.46770212054252625\n",
      "Epoch: 0, Loss: 0.5516694188117981\n",
      "Epoch: 0, Loss: 0.6551636457443237\n",
      "Epoch: 0, Loss: 0.5452687740325928\n",
      "Epoch: 0, Loss: 0.46536171436309814\n",
      "Epoch: 0, Loss: 0.5422614216804504\n",
      "Epoch: 0, Loss: 0.4320111870765686\n",
      "Epoch: 0, Loss: 0.4952360987663269\n",
      "Epoch: 0, Loss: 0.4828815162181854\n",
      "Epoch: 0, Loss: 0.4474318027496338\n",
      "Epoch: 0, Loss: 0.4281325042247772\n",
      "Epoch: 0, Loss: 0.4854585826396942\n",
      "Epoch: 0, Loss: 0.5935630202293396\n",
      "Epoch: 0, Loss: 0.42221641540527344\n",
      "Epoch: 0, Loss: 0.749867856502533\n",
      "Epoch: 0, Loss: 0.601766049861908\n",
      "Epoch: 0, Loss: 0.5446048974990845\n",
      "Epoch: 0, Loss: 0.49487701058387756\n",
      "Epoch: 0, Loss: 0.5667014718055725\n",
      "Epoch: 0, Loss: 0.47013574838638306\n",
      "Epoch: 0, Loss: 0.5543967485427856\n",
      "Epoch: 0, Loss: 0.5359919667243958\n",
      "Epoch: 0, Loss: 0.5168485045433044\n",
      "Epoch: 0, Loss: 0.49498552083969116\n",
      "Epoch: 0, Loss: 0.5013936758041382\n",
      "Epoch: 0, Loss: 0.6090823411941528\n",
      "Epoch: 0, Loss: 0.4340118169784546\n",
      "Epoch: 0, Loss: 0.5662893056869507\n",
      "Epoch: 0, Loss: 0.5124177932739258\n",
      "Epoch: 0, Loss: 0.5398810505867004\n",
      "Epoch: 0, Loss: 0.5304316878318787\n",
      "Epoch: 0, Loss: 0.5069966912269592\n",
      "Epoch: 0, Loss: 0.46116676926612854\n",
      "Epoch: 0, Loss: 0.562111496925354\n",
      "Epoch: 0, Loss: 0.489178329706192\n",
      "Epoch: 0, Loss: 0.4046792984008789\n",
      "Epoch: 0, Loss: 0.4647405445575714\n",
      "Epoch: 0, Loss: 0.627852201461792\n",
      "Epoch: 0, Loss: 0.5625811219215393\n",
      "Epoch: 0, Loss: 0.4555286765098572\n",
      "Epoch: 0, Loss: 0.4363172650337219\n",
      "Epoch: 0, Loss: 0.5111441016197205\n",
      "Epoch: 0, Loss: 0.5284978151321411\n",
      "Epoch: 0, Loss: 0.44139009714126587\n",
      "Epoch: 0, Loss: 0.45819559693336487\n",
      "Epoch: 0, Loss: 0.45736628770828247\n",
      "Epoch: 0, Loss: 0.5523025989532471\n",
      "Epoch: 0, Loss: 0.49351900815963745\n",
      "Epoch: 0, Loss: 0.5074892044067383\n",
      "Epoch: 0, Loss: 0.5556790828704834\n",
      "Epoch: 0, Loss: 0.4794482886791229\n",
      "Epoch: 0, Loss: 0.7456529140472412\n",
      "Epoch: 0, Loss: 0.5723903179168701\n",
      "Epoch: 0, Loss: 0.5320040583610535\n",
      "Epoch: 0, Loss: 0.489010751247406\n",
      "Epoch: 0, Loss: 0.42998895049095154\n",
      "Epoch: 0, Loss: 0.6171385645866394\n",
      "Epoch: 0, Loss: 0.5624747276306152\n",
      "Epoch: 0, Loss: 0.5604566335678101\n",
      "Epoch: 0, Loss: 0.48000550270080566\n",
      "Epoch: 0, Loss: 0.5909754037857056\n",
      "Epoch: 0, Loss: 0.5749585628509521\n",
      "Epoch: 0, Loss: 0.48572874069213867\n",
      "Epoch: 0, Loss: 0.4617402255535126\n",
      "Epoch: 0, Loss: 0.567030668258667\n",
      "Epoch: 0, Loss: 0.46277397871017456\n",
      "Epoch: 0, Loss: 0.49398401379585266\n",
      "Epoch: 0, Loss: 0.5242049694061279\n",
      "Epoch: 0, Loss: 0.5588420629501343\n",
      "Epoch: 0, Loss: 0.5237321853637695\n",
      "Epoch: 0, Loss: 0.5021094679832458\n",
      "Epoch: 0, Loss: 0.5194082260131836\n",
      "Epoch: 0, Loss: 0.4858102798461914\n",
      "Epoch: 0, Loss: 0.5184729695320129\n",
      "Epoch: 0, Loss: 0.7025381922721863\n",
      "Epoch: 0, Loss: 0.5301814675331116\n",
      "Epoch: 0, Loss: 0.4436595141887665\n",
      "Epoch: 0, Loss: 0.43968385457992554\n",
      "Epoch: 0, Loss: 0.41401928663253784\n",
      "Epoch: 0, Loss: 0.49324461817741394\n",
      "Epoch: 0, Loss: 0.41626855731010437\n",
      "Epoch: 0, Loss: 0.5695500373840332\n",
      "Epoch: 0, Loss: 0.41247037053108215\n",
      "Epoch: 0, Loss: 0.4682633876800537\n",
      "Epoch: 0, Loss: 0.5736753940582275\n",
      "Epoch: 0, Loss: 0.6466280817985535\n",
      "Epoch: 0, Loss: 0.6825133562088013\n",
      "Epoch: 0, Loss: 0.5090325474739075\n",
      "Epoch: 0, Loss: 0.6391018629074097\n",
      "Epoch: 0, Loss: 0.5048419833183289\n",
      "Epoch: 0, Loss: 0.43678709864616394\n",
      "Epoch: 0, Loss: 0.5273466110229492\n",
      "Epoch: 0, Loss: 0.41385021805763245\n",
      "Epoch: 0, Loss: 0.5006967782974243\n",
      "Epoch: 0, Loss: 0.4828929305076599\n",
      "Epoch: 0, Loss: 0.4406753182411194\n",
      "Epoch: 0, Loss: 0.42130622267723083\n",
      "Epoch: 0, Loss: 0.4509269893169403\n",
      "Epoch: 0, Loss: 0.46102243661880493\n",
      "Epoch: 0, Loss: 0.4279491901397705\n",
      "Epoch: 0, Loss: 0.5739117860794067\n",
      "Epoch: 0, Loss: 0.42584657669067383\n",
      "Epoch: 0, Loss: 0.4818560779094696\n",
      "Epoch: 0, Loss: 0.628749430179596\n",
      "Epoch: 0, Loss: 0.48094967007637024\n",
      "Epoch: 0, Loss: 0.5033228397369385\n",
      "Epoch: 0, Loss: 0.4743040204048157\n",
      "Epoch: 0, Loss: 0.4796946048736572\n",
      "Epoch: 0, Loss: 0.5460615754127502\n",
      "Epoch: 0, Loss: 0.437468945980072\n",
      "Epoch: 0, Loss: 0.5071910619735718\n",
      "Epoch: 0, Loss: 0.5006293654441833\n",
      "Epoch: 0, Loss: 0.4883730411529541\n",
      "Epoch: 0, Loss: 0.5027313232421875\n",
      "Epoch: 0, Loss: 0.46297016739845276\n",
      "Epoch: 0, Loss: 0.5148690342903137\n",
      "Epoch: 0, Loss: 0.6041663289070129\n",
      "Epoch: 0, Loss: 0.6452128291130066\n",
      "Epoch: 0, Loss: 0.5052517652511597\n",
      "Epoch: 0, Loss: 0.4939335584640503\n",
      "Epoch: 0, Loss: 0.4867136478424072\n",
      "Epoch: 0, Loss: 0.581260085105896\n",
      "Epoch: 0, Loss: 0.5109596848487854\n",
      "Epoch: 0, Loss: 0.6765101552009583\n",
      "Epoch: 0, Loss: 0.49784499406814575\n",
      "Epoch: 0, Loss: 0.43963655829429626\n",
      "Epoch: 0, Loss: 0.5074389576911926\n",
      "Epoch: 0, Loss: 0.5359843373298645\n",
      "Epoch: 0, Loss: 0.450228750705719\n",
      "Epoch: 0, Loss: 0.5529517531394958\n",
      "Epoch: 0, Loss: 0.5128337144851685\n",
      "Epoch: 0, Loss: 0.488797128200531\n",
      "Epoch: 0, Loss: 0.4920080900192261\n",
      "Epoch: 0, Loss: 0.5422457456588745\n",
      "Epoch: 0, Loss: 0.5214521884918213\n",
      "Epoch: 0, Loss: 0.44207385182380676\n",
      "Epoch: 0, Loss: 0.6368058919906616\n",
      "Epoch: 0, Loss: 0.4998700022697449\n",
      "Epoch: 0, Loss: 0.4728999733924866\n",
      "Epoch: 0, Loss: 0.49331486225128174\n",
      "Epoch: 0, Loss: 0.461689829826355\n",
      "Epoch: 0, Loss: 0.5330222249031067\n",
      "Epoch: 0, Loss: 0.46270978450775146\n",
      "Epoch: 0, Loss: 0.5404537320137024\n",
      "Epoch: 0, Loss: 0.5590248107910156\n",
      "Epoch: 0, Loss: 0.5379341840744019\n",
      "Epoch: 0, Loss: 0.5654472708702087\n",
      "Epoch: 0, Loss: 0.5351462364196777\n",
      "Epoch: 0, Loss: 0.4207715690135956\n",
      "Epoch: 0, Loss: 0.5693345069885254\n",
      "Epoch: 0, Loss: 0.43839114904403687\n",
      "Epoch: 0, Loss: 0.5482436418533325\n",
      "Epoch: 0, Loss: 0.5269948244094849\n",
      "Epoch: 0, Loss: 0.4772852063179016\n",
      "Epoch: 0, Loss: 0.5431902408599854\n",
      "Epoch: 0, Loss: 0.529569685459137\n",
      "Epoch: 0, Loss: 0.40954869985580444\n",
      "Epoch: 0, Loss: 0.5059893727302551\n",
      "Epoch: 0, Loss: 0.5753841400146484\n",
      "Epoch: 0, Loss: 0.4557289481163025\n",
      "Epoch: 0, Loss: 0.5513784289360046\n",
      "Epoch: 0, Loss: 0.5525968074798584\n",
      "Epoch: 0, Loss: 0.549960732460022\n",
      "Epoch: 0, Loss: 0.46561968326568604\n",
      "Epoch: 0, Loss: 0.4612131118774414\n",
      "Epoch: 0, Loss: 0.5447840690612793\n",
      "Epoch: 0, Loss: 0.5775111317634583\n",
      "Epoch: 0, Loss: 0.5640965700149536\n",
      "Epoch: 0, Loss: 0.44877609610557556\n",
      "Epoch: 0, Loss: 0.49243423342704773\n",
      "Epoch: 0, Loss: 0.4693525433540344\n",
      "Epoch: 0, Loss: 0.5026392340660095\n",
      "Epoch: 0, Loss: 0.5010675191879272\n",
      "Epoch: 0, Loss: 0.46031588315963745\n",
      "Epoch: 0, Loss: 0.5139971971511841\n",
      "Epoch: 0, Loss: 0.46348342299461365\n",
      "Epoch: 0, Loss: 0.5055946111679077\n",
      "Epoch: 0, Loss: 0.4119243323802948\n",
      "Epoch: 0, Loss: 0.5044316649436951\n",
      "Epoch: 0, Loss: 0.5625391006469727\n",
      "Epoch: 0, Loss: 0.5535222291946411\n",
      "Epoch: 0, Loss: 0.46300098299980164\n",
      "Epoch: 0, Loss: 0.49174991250038147\n",
      "Epoch: 0, Loss: 0.43104642629623413\n",
      "Epoch: 0, Loss: 0.47671830654144287\n",
      "Epoch: 0, Loss: 0.4975735545158386\n",
      "Epoch: 0, Loss: 0.4707496166229248\n",
      "Epoch: 0, Loss: 0.4568563997745514\n",
      "Epoch: 0, Loss: 0.6240915060043335\n",
      "Epoch: 0, Loss: 0.501718282699585\n",
      "Epoch: 0, Loss: 0.5819156765937805\n",
      "Epoch: 0, Loss: 0.507368266582489\n",
      "Epoch: 0, Loss: 0.5294561982154846\n",
      "Epoch: 0, Loss: 0.4719868302345276\n",
      "Epoch: 0, Loss: 0.5439628958702087\n",
      "Epoch: 0, Loss: 0.6031314730644226\n",
      "Epoch: 0, Loss: 0.40218284726142883\n",
      "Epoch: 0, Loss: 0.48453325033187866\n",
      "Epoch: 0, Loss: 0.5306062698364258\n",
      "Epoch: 0, Loss: 0.5466365814208984\n",
      "Epoch: 0, Loss: 0.39692920446395874\n",
      "Epoch: 0, Loss: 0.617485523223877\n",
      "Epoch: 0, Loss: 0.5997633934020996\n",
      "Epoch: 0, Loss: 0.47856420278549194\n",
      "Epoch: 0, Loss: 0.456665575504303\n",
      "Epoch: 0, Loss: 0.49522891640663147\n",
      "Epoch: 0, Loss: 0.5556902289390564\n",
      "Epoch: 0, Loss: 0.49035683274269104\n",
      "Epoch: 0, Loss: 0.4394942820072174\n",
      "Epoch: 0, Loss: 0.5494871735572815\n",
      "Epoch: 0, Loss: 0.49990472197532654\n",
      "Epoch: 0, Loss: 0.6036016941070557\n",
      "Epoch: 0, Loss: 0.5094352960586548\n",
      "Epoch: 0, Loss: 0.4841257333755493\n",
      "Epoch: 0, Loss: 0.4409240484237671\n",
      "Epoch: 0, Loss: 0.40072524547576904\n",
      "Epoch: 0, Loss: 0.46204131841659546\n",
      "Epoch: 0, Loss: 0.5820087194442749\n",
      "Epoch: 0, Loss: 0.5379130840301514\n",
      "Epoch: 0, Loss: 0.44466617703437805\n",
      "Epoch: 0, Loss: 0.5458487868309021\n",
      "Epoch: 0, Loss: 0.5518715381622314\n",
      "Epoch: 0, Loss: 0.6353089809417725\n",
      "Epoch: 0, Loss: 0.5402388572692871\n",
      "Epoch: 0, Loss: 0.46608394384384155\n",
      "Epoch: 0, Loss: 0.5332348942756653\n",
      "Epoch: 0, Loss: 0.4069175124168396\n",
      "Epoch: 0, Loss: 0.4730369448661804\n",
      "Epoch: 0, Loss: 0.6179994940757751\n",
      "Epoch: 0, Loss: 0.4811316728591919\n",
      "Epoch: 0, Loss: 0.5274182558059692\n",
      "Epoch: 0, Loss: 0.43972063064575195\n",
      "Epoch: 0, Loss: 0.4975200295448303\n",
      "Epoch: 0, Loss: 0.5176780223846436\n",
      "Epoch: 0, Loss: 0.6149712204933167\n",
      "Epoch: 0, Loss: 0.599342405796051\n",
      "Epoch: 0, Loss: 0.5595604777336121\n",
      "Epoch: 0, Loss: 0.5543408989906311\n",
      "Epoch: 0, Loss: 0.5299719572067261\n",
      "Epoch: 0, Loss: 0.4551897346973419\n",
      "Epoch: 0, Loss: 0.5992070436477661\n",
      "Epoch: 0, Loss: 0.5089743137359619\n",
      "Epoch: 0, Loss: 0.5470647811889648\n",
      "Epoch: 0, Loss: 0.6245766282081604\n",
      "Epoch: 0, Loss: 0.5279439687728882\n",
      "Epoch: 0, Loss: 0.46726155281066895\n",
      "Epoch: 0, Loss: 0.6172952651977539\n",
      "Epoch: 0, Loss: 0.46988797187805176\n",
      "Epoch: 0, Loss: 0.4799026846885681\n",
      "Epoch: 0, Loss: 0.5003293752670288\n",
      "Epoch: 0, Loss: 0.5914979577064514\n",
      "Epoch: 0, Loss: 0.47432178258895874\n",
      "Epoch: 0, Loss: 0.542220950126648\n",
      "Epoch: 0, Loss: 0.5844782590866089\n",
      "Epoch: 0, Loss: 0.4425181448459625\n",
      "Epoch: 0, Loss: 0.4656302332878113\n",
      "Epoch: 0, Loss: 0.4683375954627991\n",
      "Epoch: 0, Loss: 0.5494946241378784\n",
      "Epoch: 0, Loss: 0.4889680743217468\n",
      "Epoch: 0, Loss: 0.44299566745758057\n",
      "Epoch: 0, Loss: 0.6164954900741577\n",
      "Epoch: 0, Loss: 0.5100427865982056\n",
      "Epoch: 0, Loss: 0.420568585395813\n",
      "Epoch: 0, Loss: 0.5272344946861267\n",
      "Epoch: 0, Loss: 0.6113631725311279\n",
      "Epoch: 0, Loss: 0.4515158534049988\n",
      "Epoch: 0, Loss: 0.4290408492088318\n",
      "Epoch: 0, Loss: 0.5777128338813782\n",
      "Epoch: 0, Loss: 0.417589396238327\n",
      "Epoch: 0, Loss: 0.46267393231391907\n",
      "Epoch: 0, Loss: 0.5331459641456604\n",
      "Epoch: 0, Loss: 0.47246482968330383\n",
      "Epoch: 0, Loss: 0.3764410614967346\n",
      "Epoch: 0, Loss: 0.4810153543949127\n",
      "Epoch: 0, Loss: 0.5400094985961914\n",
      "Epoch: 0, Loss: 0.49589893221855164\n",
      "Epoch: 0, Loss: 0.4655701518058777\n",
      "Epoch: 0, Loss: 0.5511453151702881\n",
      "Epoch: 0, Loss: 0.4468684196472168\n",
      "Epoch: 0, Loss: 0.43928423523902893\n",
      "Epoch: 0, Loss: 0.6334261894226074\n",
      "Epoch: 0, Loss: 0.6006209850311279\n",
      "Epoch: 0, Loss: 0.47960060834884644\n",
      "Epoch: 0, Loss: 0.4058472514152527\n",
      "Epoch: 0, Loss: 0.49622291326522827\n",
      "Epoch: 0, Loss: 0.5213616490364075\n",
      "Epoch: 0, Loss: 0.6156235933303833\n",
      "Epoch: 0, Loss: 0.4879518151283264\n",
      "Epoch: 0, Loss: 0.4703946113586426\n",
      "Epoch: 0, Loss: 0.5292890667915344\n",
      "Epoch: 0, Loss: 0.5681583285331726\n",
      "Epoch: 0, Loss: 0.5142045617103577\n",
      "Epoch: 0, Loss: 0.4807053506374359\n",
      "Epoch: 0, Loss: 0.6279182434082031\n",
      "Epoch: 0, Loss: 0.5041135549545288\n",
      "Epoch: 0, Loss: 0.484444260597229\n",
      "Epoch: 0, Loss: 0.4401206374168396\n",
      "Epoch: 0, Loss: 0.4462953507900238\n",
      "Epoch: 0, Loss: 0.4572257399559021\n",
      "Epoch: 0, Loss: 0.4228630065917969\n",
      "Epoch: 0, Loss: 0.4636409282684326\n",
      "Epoch: 0, Loss: 0.46750396490097046\n",
      "Epoch: 0, Loss: 0.5157742500305176\n",
      "Epoch: 0, Loss: 0.5628740787506104\n",
      "Epoch: 0, Loss: 0.5830869674682617\n",
      "Epoch: 0, Loss: 0.5067571997642517\n",
      "Epoch: 0, Loss: 0.4180317223072052\n",
      "Epoch: 0, Loss: 0.5600817203521729\n",
      "Epoch: 0, Loss: 0.5705366134643555\n",
      "Epoch: 0, Loss: 0.448726087808609\n",
      "Epoch: 0, Loss: 0.5036002993583679\n",
      "Epoch: 0, Loss: 0.5009647011756897\n",
      "Epoch: 0, Loss: 0.6851966381072998\n",
      "Epoch: 0, Loss: 0.47788697481155396\n",
      "Epoch: 0, Loss: 0.5678426027297974\n",
      "Epoch: 0, Loss: 0.49876919388771057\n",
      "Epoch: 0, Loss: 0.44680678844451904\n",
      "Epoch: 0, Loss: 0.5156009793281555\n",
      "Epoch: 0, Loss: 0.4921529293060303\n",
      "Epoch: 0, Loss: 0.6676390767097473\n",
      "Epoch: 0, Loss: 0.43890678882598877\n",
      "Epoch: 0, Loss: 0.6031551957130432\n",
      "Epoch: 0, Loss: 0.41414037346839905\n",
      "Epoch: 0, Loss: 0.5206720232963562\n",
      "Epoch: 0, Loss: 0.6512175798416138\n",
      "Epoch: 0, Loss: 0.4513884484767914\n",
      "Epoch: 0, Loss: 0.4487041234970093\n",
      "Epoch: 0, Loss: 0.4619187116622925\n",
      "Epoch: 0, Loss: 0.5338883399963379\n",
      "Epoch: 0, Loss: 0.5224977135658264\n",
      "Epoch: 0, Loss: 0.548861026763916\n",
      "Epoch: 0, Loss: 0.48951035737991333\n",
      "Epoch: 0, Loss: 0.5173557996749878\n",
      "Epoch: 0, Loss: 0.46746698021888733\n",
      "Epoch: 0, Loss: 0.43241509795188904\n",
      "Epoch: 0, Loss: 0.4793391227722168\n",
      "Epoch: 0, Loss: 0.4668568968772888\n",
      "Epoch: 0, Loss: 0.55768221616745\n",
      "Epoch: 0, Loss: 0.5890515446662903\n",
      "Epoch: 0, Loss: 0.48773887753486633\n",
      "Epoch: 0, Loss: 0.5420621633529663\n",
      "Epoch: 0, Loss: 0.5538296103477478\n",
      "Epoch: 0, Loss: 0.48645079135894775\n",
      "Epoch: 0, Loss: 0.4754706025123596\n",
      "Epoch: 0, Loss: 0.48682931065559387\n",
      "Epoch: 0, Loss: 0.5176117420196533\n",
      "Epoch: 0, Loss: 0.47126540541648865\n",
      "Epoch: 0, Loss: 0.5321842432022095\n",
      "Epoch: 0, Loss: 0.4740460216999054\n",
      "Epoch: 0, Loss: 0.4673900902271271\n",
      "Epoch: 0, Loss: 0.4798361659049988\n",
      "Epoch: 0, Loss: 0.6542428135871887\n",
      "Epoch: 0, Loss: 0.4331171214580536\n",
      "Epoch: 0, Loss: 0.36997658014297485\n",
      "Epoch: 0, Loss: 0.4081152081489563\n",
      "Epoch: 0, Loss: 0.46782386302948\n",
      "Epoch: 0, Loss: 0.5185202956199646\n",
      "Epoch: 0, Loss: 0.4912041425704956\n",
      "Epoch: 0, Loss: 0.5612025856971741\n",
      "Epoch: 0, Loss: 0.4652934968471527\n",
      "Epoch: 0, Loss: 0.5768777132034302\n",
      "Epoch: 0, Loss: 0.49509334564208984\n",
      "Epoch: 0, Loss: 0.5352142453193665\n",
      "Epoch: 0, Loss: 0.4526822865009308\n",
      "Epoch: 0, Loss: 0.4712175130844116\n",
      "Epoch: 0, Loss: 0.5544182658195496\n",
      "Epoch: 0, Loss: 0.45771539211273193\n",
      "Epoch: 0, Loss: 0.4894915223121643\n",
      "Epoch: 0, Loss: 0.4627307653427124\n",
      "Epoch: 0, Loss: 0.5123507976531982\n",
      "Epoch: 0, Loss: 0.526282548904419\n",
      "Epoch: 0, Loss: 0.431161105632782\n",
      "Epoch: 0, Loss: 0.5085847973823547\n",
      "Epoch: 0, Loss: 0.42718708515167236\n",
      "Epoch: 0, Loss: 0.5762432217597961\n",
      "Epoch: 0, Loss: 0.575234591960907\n",
      "Epoch: 0, Loss: 0.4279026687145233\n",
      "Epoch: 0, Loss: 0.4742874503135681\n",
      "Epoch: 0, Loss: 0.5211819410324097\n",
      "Epoch: 0, Loss: 0.5095726251602173\n",
      "Epoch: 0, Loss: 0.44330674409866333\n",
      "Epoch: 0, Loss: 0.5334165692329407\n",
      "Epoch: 0, Loss: 0.4900498390197754\n",
      "Epoch: 0, Loss: 0.4707077145576477\n",
      "Epoch: 0, Loss: 0.5616831183433533\n",
      "Epoch: 0, Loss: 0.5497880578041077\n",
      "Epoch: 0, Loss: 0.5395840406417847\n",
      "Epoch: 0, Loss: 0.514575719833374\n",
      "Epoch: 0, Loss: 0.6322627067565918\n",
      "Epoch: 0, Loss: 0.6212238669395447\n",
      "Epoch: 0, Loss: 0.4383317232131958\n",
      "Epoch: 0, Loss: 0.45853281021118164\n",
      "Epoch: 0, Loss: 0.5397659540176392\n",
      "Epoch: 0, Loss: 0.5241547226905823\n",
      "Epoch: 0, Loss: 0.654129147529602\n",
      "Epoch: 0, Loss: 0.46539878845214844\n",
      "Epoch: 0, Loss: 0.5476464033126831\n",
      "Epoch: 0, Loss: 0.45212507247924805\n",
      "Epoch: 0, Loss: 0.6015268564224243\n",
      "Epoch: 0, Loss: 0.4872167706489563\n",
      "Epoch: 0, Loss: 0.6267002820968628\n",
      "Epoch: 0, Loss: 0.5644795298576355\n",
      "Epoch: 0, Loss: 0.449427992105484\n",
      "Epoch: 0, Loss: 0.5512779951095581\n",
      "Epoch: 0, Loss: 0.4326515197753906\n",
      "Epoch: 0, Loss: 0.4475187361240387\n",
      "Epoch: 0, Loss: 0.4894546866416931\n",
      "Epoch: 0, Loss: 0.4804430603981018\n",
      "Epoch: 0, Loss: 0.4626537263393402\n",
      "Epoch: 0, Loss: 0.44114217162132263\n",
      "Epoch: 0, Loss: 0.5658228993415833\n",
      "Epoch: 0, Loss: 0.565502405166626\n",
      "Epoch: 0, Loss: 0.6168773770332336\n",
      "Epoch: 0, Loss: 0.6213537454605103\n",
      "Epoch: 0, Loss: 0.5361852645874023\n",
      "Epoch: 0, Loss: 0.5069527626037598\n",
      "Epoch: 0, Loss: 0.5747323036193848\n",
      "Epoch: 0, Loss: 0.5814687013626099\n",
      "Epoch: 0, Loss: 0.40880393981933594\n",
      "Epoch: 0, Loss: 0.49732041358947754\n",
      "Epoch: 0, Loss: 0.4703291654586792\n",
      "Epoch: 0, Loss: 0.49276232719421387\n",
      "Epoch: 0, Loss: 0.47199687361717224\n",
      "Epoch: 0, Loss: 0.5241292715072632\n",
      "Epoch: 0, Loss: 0.5160660147666931\n",
      "Epoch: 0, Loss: 0.48527806997299194\n",
      "Epoch: 0, Loss: 0.5124908089637756\n",
      "Epoch: 0, Loss: 0.6422011852264404\n",
      "Epoch: 0, Loss: 0.49976614117622375\n",
      "Epoch: 0, Loss: 0.6558223366737366\n",
      "Epoch: 0, Loss: 0.6308810114860535\n",
      "Epoch: 0, Loss: 0.5174952149391174\n",
      "Epoch: 0, Loss: 0.5581473708152771\n",
      "Epoch: 0, Loss: 0.5555728077888489\n",
      "Epoch: 0, Loss: 0.5314702987670898\n",
      "Epoch: 0, Loss: 0.5584070682525635\n",
      "Epoch: 0, Loss: 0.5855695605278015\n",
      "Epoch: 0, Loss: 0.43771064281463623\n",
      "Epoch: 0, Loss: 0.5048108696937561\n",
      "Epoch: 0, Loss: 0.4598776698112488\n",
      "Epoch: 0, Loss: 0.42289942502975464\n",
      "Epoch: 0, Loss: 0.4378143548965454\n",
      "Epoch: 0, Loss: 0.4921056032180786\n",
      "Epoch: 0, Loss: 0.4461142420768738\n",
      "Epoch: 0, Loss: 0.47357845306396484\n",
      "Epoch: 0, Loss: 0.412276029586792\n",
      "Epoch: 0, Loss: 0.39954912662506104\n",
      "Epoch: 0, Loss: 0.5218749046325684\n",
      "Epoch: 0, Loss: 0.4155882000923157\n",
      "Epoch: 0, Loss: 0.6175172328948975\n",
      "Epoch: 0, Loss: 0.5208141803741455\n",
      "Epoch: 0, Loss: 0.41062653064727783\n",
      "Epoch: 0, Loss: 0.5515732765197754\n",
      "Epoch: 0, Loss: 0.46057799458503723\n",
      "Epoch: 0, Loss: 0.4593488872051239\n",
      "Epoch: 0, Loss: 0.512383222579956\n",
      "Epoch: 0, Loss: 0.43930408358573914\n",
      "Epoch: 0, Loss: 0.5886067748069763\n",
      "Epoch: 0, Loss: 0.5541443824768066\n",
      "Epoch: 0, Loss: 0.5334818959236145\n",
      "Epoch: 0, Loss: 0.48954588174819946\n",
      "Epoch: 0, Loss: 0.4052533805370331\n",
      "Epoch: 0, Loss: 0.4953058958053589\n",
      "Epoch: 0, Loss: 0.5606492161750793\n",
      "Epoch: 0, Loss: 0.5128984451293945\n",
      "Epoch: 0, Loss: 0.5635073184967041\n",
      "Epoch: 0, Loss: 0.44368478655815125\n",
      "Epoch: 0, Loss: 0.45057564973831177\n",
      "Epoch: 0, Loss: 0.5643061399459839\n",
      "Epoch: 0, Loss: 0.5083909034729004\n",
      "Epoch: 0, Loss: 0.5079323649406433\n",
      "Epoch: 0, Loss: 0.5547623634338379\n",
      "Epoch: 0, Loss: 0.5929563641548157\n",
      "Epoch: 0, Loss: 0.466839075088501\n",
      "Epoch: 0, Loss: 0.5314937233924866\n",
      "Epoch: 0, Loss: 0.4517192542552948\n",
      "Epoch: 0, Loss: 0.670728862285614\n",
      "Epoch: 0, Loss: 0.5294619798660278\n",
      "Epoch: 0, Loss: 0.47919541597366333\n",
      "Epoch: 0, Loss: 0.4803718328475952\n",
      "Epoch: 0, Loss: 0.4726446866989136\n",
      "Epoch: 0, Loss: 0.49284037947654724\n",
      "Epoch: 0, Loss: 0.48328661918640137\n",
      "Epoch: 0, Loss: 0.5759628415107727\n",
      "Epoch: 0, Loss: 0.5004245638847351\n",
      "Epoch: 0, Loss: 0.5520107746124268\n",
      "Epoch: 0, Loss: 0.47020047903060913\n",
      "Epoch: 0, Loss: 0.5506243705749512\n",
      "Epoch: 0, Loss: 0.4672810733318329\n",
      "Epoch: 0, Loss: 0.6893675327301025\n",
      "Epoch: 0, Loss: 0.42556267976760864\n",
      "Epoch: 0, Loss: 0.6350947618484497\n",
      "Epoch: 0, Loss: 0.557571291923523\n",
      "Epoch: 0, Loss: 0.5419669151306152\n",
      "Epoch: 0, Loss: 0.4900728464126587\n",
      "Epoch: 0, Loss: 0.4523221254348755\n",
      "Epoch: 0, Loss: 0.44607168436050415\n",
      "Epoch: 0, Loss: 0.5915803909301758\n",
      "Epoch: 0, Loss: 0.5136321783065796\n",
      "Epoch: 0, Loss: 0.5668159127235413\n",
      "Epoch: 0, Loss: 0.5991873741149902\n",
      "Epoch: 0, Loss: 0.5544856190681458\n",
      "Epoch: 0, Loss: 0.5286219120025635\n",
      "Epoch: 0, Loss: 0.42980968952178955\n",
      "Epoch: 0, Loss: 0.6070672869682312\n",
      "Epoch: 0, Loss: 0.4711650609970093\n",
      "Epoch: 0, Loss: 0.5572565793991089\n",
      "Epoch: 0, Loss: 0.39166826009750366\n",
      "Epoch: 0, Loss: 0.45427024364471436\n",
      "Epoch: 0, Loss: 0.5765340924263\n",
      "Epoch: 0, Loss: 0.4403350353240967\n",
      "Epoch: 0, Loss: 0.4296373128890991\n",
      "Epoch: 0, Loss: 0.45431095361709595\n",
      "Epoch: 0, Loss: 0.5344253182411194\n",
      "Epoch: 0, Loss: 0.6796717047691345\n",
      "Epoch: 0, Loss: 0.4257413446903229\n",
      "Epoch: 0, Loss: 0.5149320363998413\n",
      "Epoch: 0, Loss: 0.49862897396087646\n",
      "Epoch: 0, Loss: 0.5324565172195435\n",
      "Epoch: 0, Loss: 0.4640469551086426\n",
      "Epoch: 0, Loss: 0.5177614092826843\n",
      "Epoch: 0, Loss: 0.4906797409057617\n",
      "Epoch: 0, Loss: 0.4971565008163452\n",
      "Epoch: 0, Loss: 0.5134523510932922\n",
      "Epoch: 0, Loss: 0.500207245349884\n",
      "Epoch: 0, Loss: 0.5064840316772461\n",
      "Epoch: 0, Loss: 0.589817464351654\n",
      "Epoch: 0, Loss: 0.5950515270233154\n",
      "Epoch: 0, Loss: 0.5199531316757202\n",
      "Epoch: 0, Loss: 0.5717332363128662\n",
      "Epoch: 0, Loss: 0.48065614700317383\n",
      "Epoch: 0, Loss: 0.4982540011405945\n",
      "Epoch: 0, Loss: 0.4837270975112915\n",
      "Epoch: 0, Loss: 0.5026786923408508\n",
      "Epoch: 0, Loss: 0.5842593312263489\n",
      "Epoch: 0, Loss: 0.5248909592628479\n",
      "Epoch: 0, Loss: 0.5798767805099487\n",
      "Epoch: 0, Loss: 0.5537803769111633\n",
      "Epoch: 0, Loss: 0.520954430103302\n",
      "Epoch: 0, Loss: 0.5429912805557251\n",
      "Epoch: 0, Loss: 0.44854122400283813\n",
      "Epoch: 0, Loss: 0.45495784282684326\n",
      "Epoch: 0, Loss: 0.49199479818344116\n",
      "Epoch: 0, Loss: 0.540241539478302\n",
      "Epoch: 0, Loss: 0.48647549748420715\n",
      "Epoch: 0, Loss: 0.5885506868362427\n",
      "Epoch: 0, Loss: 0.4590272009372711\n",
      "Epoch: 0, Loss: 0.4231422245502472\n",
      "Epoch: 0, Loss: 0.4310219883918762\n",
      "Epoch: 0, Loss: 0.5337244272232056\n",
      "Epoch: 0, Loss: 0.4528002142906189\n",
      "Epoch: 0, Loss: 0.5682232975959778\n",
      "Epoch: 0, Loss: 0.5117090940475464\n",
      "Epoch: 0, Loss: 0.4783158302307129\n",
      "Epoch: 0, Loss: 0.46388062834739685\n",
      "Epoch: 0, Loss: 0.5118078589439392\n",
      "Epoch: 0, Loss: 0.5466801524162292\n",
      "Epoch: 0, Loss: 0.4842597246170044\n",
      "Epoch: 0, Loss: 0.6273742914199829\n",
      "Epoch: 0, Loss: 0.5421938896179199\n",
      "Epoch: 0, Loss: 0.610061764717102\n",
      "Epoch: 0, Loss: 0.5202398300170898\n",
      "Epoch: 0, Loss: 0.5933021903038025\n",
      "Epoch: 0, Loss: 0.6458451151847839\n",
      "Epoch: 0, Loss: 0.5117405652999878\n",
      "Epoch: 0, Loss: 0.6877169013023376\n",
      "Epoch: 0, Loss: 0.49238574504852295\n",
      "Epoch: 0, Loss: 0.4350060522556305\n",
      "Epoch: 0, Loss: 0.4887958765029907\n",
      "Epoch: 0, Loss: 0.47985637187957764\n",
      "Epoch: 0, Loss: 0.5694030523300171\n",
      "Epoch: 0, Loss: 0.43266749382019043\n",
      "Epoch: 0, Loss: 0.5362905263900757\n",
      "Epoch: 0, Loss: 0.5892765522003174\n",
      "Epoch: 0, Loss: 0.5240012407302856\n",
      "Epoch: 0, Loss: 0.520047664642334\n",
      "Epoch: 0, Loss: 0.3966864049434662\n",
      "Epoch: 0, Loss: 0.48352283239364624\n",
      "Epoch: 0, Loss: 0.5571715235710144\n",
      "Epoch: 0, Loss: 0.534187376499176\n",
      "Epoch: 0, Loss: 0.4411156177520752\n",
      "Epoch: 0, Loss: 0.5528356432914734\n",
      "Epoch: 0, Loss: 0.5353929400444031\n",
      "Epoch: 0, Loss: 0.5408247113227844\n",
      "Epoch: 0, Loss: 0.4616939425468445\n",
      "Epoch: 0, Loss: 0.4474419057369232\n",
      "Epoch: 0, Loss: 0.5203717947006226\n",
      "Epoch: 0, Loss: 0.4586893320083618\n",
      "Epoch: 0, Loss: 0.49282383918762207\n",
      "Epoch: 0, Loss: 0.4511992633342743\n",
      "Epoch: 0, Loss: 0.4438645839691162\n",
      "Epoch: 0, Loss: 0.5546164512634277\n",
      "Epoch: 0, Loss: 0.4508931338787079\n",
      "Epoch: 0, Loss: 0.556272566318512\n",
      "Epoch: 0, Loss: 0.5734309554100037\n",
      "Epoch: 0, Loss: 0.5006252527236938\n",
      "Epoch: 0, Loss: 0.49882274866104126\n",
      "Epoch: 0, Loss: 0.49871742725372314\n",
      "Epoch: 0, Loss: 0.5907382965087891\n",
      "Epoch: 0, Loss: 0.5303362011909485\n",
      "Epoch: 0, Loss: 0.5152856111526489\n",
      "Epoch: 0, Loss: 0.43894320726394653\n",
      "Epoch: 0, Loss: 0.4837304949760437\n",
      "Epoch: 0, Loss: 0.5311521291732788\n",
      "Epoch: 0, Loss: 0.4648587703704834\n",
      "Epoch: 0, Loss: 0.5160020589828491\n",
      "Epoch: 0, Loss: 0.43482205271720886\n",
      "Epoch: 0, Loss: 0.4485781192779541\n",
      "Epoch: 0, Loss: 0.4097304344177246\n",
      "Epoch: 0, Loss: 0.5025969743728638\n",
      "Epoch: 0, Loss: 0.4960153102874756\n",
      "Epoch: 0, Loss: 0.6341444849967957\n",
      "Epoch: 0, Loss: 0.4648969769477844\n",
      "Epoch: 0, Loss: 0.5042572617530823\n",
      "Epoch: 0, Loss: 0.45779216289520264\n",
      "Epoch: 0, Loss: 0.5013119578361511\n",
      "Epoch: 0, Loss: 0.5102732181549072\n",
      "Epoch: 0, Loss: 0.4579678177833557\n",
      "Epoch: 0, Loss: 0.48381441831588745\n",
      "Epoch: 0, Loss: 0.5296580195426941\n",
      "Epoch: 0, Loss: 0.6453668475151062\n",
      "Epoch: 0, Loss: 0.6173585653305054\n",
      "Epoch: 0, Loss: 0.5627549290657043\n",
      "Epoch: 0, Loss: 0.5135669112205505\n",
      "Epoch: 0, Loss: 0.4494844973087311\n",
      "Epoch: 0, Loss: 0.4288882613182068\n",
      "Epoch: 0, Loss: 0.46352750062942505\n",
      "Epoch: 0, Loss: 0.4663775563240051\n",
      "Epoch: 0, Loss: 0.6059612035751343\n",
      "Epoch: 0, Loss: 0.4161044955253601\n",
      "Epoch: 0, Loss: 0.592391848564148\n",
      "Epoch: 0, Loss: 0.499764621257782\n",
      "Epoch: 0, Loss: 0.5272234678268433\n",
      "Epoch: 0, Loss: 0.4364432692527771\n",
      "Epoch: 0, Loss: 0.47162315249443054\n",
      "Epoch: 0, Loss: 0.4930598735809326\n",
      "Epoch: 0, Loss: 0.5657711625099182\n",
      "Epoch: 0, Loss: 0.4755592346191406\n",
      "Epoch: 0, Loss: 0.44798344373703003\n",
      "Epoch: 0, Loss: 0.5607701539993286\n",
      "Epoch: 0, Loss: 0.460737943649292\n",
      "Epoch: 0, Loss: 0.5055082440376282\n",
      "Epoch: 0, Loss: 0.5313398241996765\n",
      "Epoch: 0, Loss: 0.45901745557785034\n",
      "Epoch: 0, Loss: 0.4777042269706726\n",
      "Epoch: 0, Loss: 0.5352841019630432\n",
      "Epoch: 0, Loss: 0.49345412850379944\n",
      "Epoch: 0, Loss: 0.5612977147102356\n",
      "Epoch: 0, Loss: 0.4764242172241211\n",
      "Epoch: 0, Loss: 0.5375769138336182\n",
      "Epoch: 0, Loss: 0.4449300467967987\n",
      "Epoch: 0, Loss: 0.46806037425994873\n",
      "Epoch: 0, Loss: 0.4880863428115845\n",
      "Epoch: 0, Loss: 0.5415071249008179\n",
      "Epoch: 0, Loss: 0.4448342025279999\n",
      "Epoch: 0, Loss: 0.4791041612625122\n",
      "Epoch: 0, Loss: 0.7155007123947144\n",
      "Epoch: 0, Loss: 0.4816651940345764\n",
      "Epoch: 0, Loss: 0.47397369146347046\n",
      "Epoch: 0, Loss: 0.45597511529922485\n",
      "Epoch: 0, Loss: 0.48244285583496094\n",
      "Epoch: 0, Loss: 0.4773975610733032\n",
      "Epoch: 0, Loss: 0.5541881322860718\n",
      "Epoch: 0, Loss: 0.5854013562202454\n",
      "Epoch: 0, Loss: 0.525542676448822\n",
      "Epoch: 0, Loss: 0.41197025775909424\n",
      "Epoch: 0, Loss: 0.47862380743026733\n",
      "Epoch: 0, Loss: 0.5156424045562744\n",
      "Epoch: 0, Loss: 0.46530577540397644\n",
      "Epoch: 0, Loss: 0.6318653225898743\n",
      "Epoch: 0, Loss: 0.43509453535079956\n",
      "Epoch: 0, Loss: 0.5302051901817322\n",
      "Epoch: 0, Loss: 0.4275730848312378\n",
      "Epoch: 0, Loss: 0.6784944534301758\n",
      "Epoch: 0, Loss: 0.49031731486320496\n",
      "Epoch: 0, Loss: 0.5252641439437866\n",
      "Epoch: 0, Loss: 0.5077636241912842\n",
      "Epoch: 0, Loss: 0.5338334441184998\n",
      "Epoch: 0, Loss: 0.5432003736495972\n",
      "Epoch: 0, Loss: 0.46336084604263306\n",
      "Epoch: 0, Loss: 0.5157095193862915\n",
      "Epoch: 0, Loss: 0.538399875164032\n",
      "Epoch: 0, Loss: 0.5193707346916199\n",
      "Epoch: 0, Loss: 0.49406108260154724\n",
      "Epoch: 0, Loss: 0.5051830410957336\n",
      "Epoch: 0, Loss: 0.5014522671699524\n",
      "Epoch: 0, Loss: 0.5478262901306152\n",
      "Epoch: 0, Loss: 0.526643693447113\n",
      "Epoch: 0, Loss: 0.5262699723243713\n",
      "Epoch: 0, Loss: 0.5179232954978943\n",
      "Epoch: 0, Loss: 0.48816752433776855\n",
      "Epoch: 0, Loss: 0.5690622925758362\n",
      "Epoch: 0, Loss: 0.4523470103740692\n",
      "Epoch: 0, Loss: 0.4651818871498108\n",
      "Epoch: 0, Loss: 0.4744277596473694\n",
      "Epoch: 0, Loss: 0.4809989333152771\n",
      "Epoch: 0, Loss: 0.5453493595123291\n",
      "Epoch: 0, Loss: 0.47827062010765076\n",
      "Epoch: 0, Loss: 0.540787935256958\n",
      "Epoch: 0, Loss: 0.4904795289039612\n",
      "Epoch: 0, Loss: 0.5031926035881042\n",
      "Epoch: 0, Loss: 0.4514067769050598\n",
      "Epoch: 0, Loss: 0.5586820840835571\n",
      "Epoch: 0, Loss: 0.4742736220359802\n",
      "Epoch: 0, Loss: 0.542563259601593\n",
      "Epoch: 0, Loss: 0.402561217546463\n",
      "Epoch: 0, Loss: 0.4528927803039551\n",
      "Epoch: 0, Loss: 0.5317140221595764\n",
      "Epoch: 0, Loss: 0.5123738050460815\n",
      "Epoch: 0, Loss: 0.5665311217308044\n",
      "Epoch: 0, Loss: 0.5431233644485474\n",
      "Epoch: 0, Loss: 0.39962565898895264\n",
      "Epoch: 0, Loss: 0.5744397640228271\n",
      "Epoch: 0, Loss: 0.5920169353485107\n",
      "Epoch: 0, Loss: 0.4811164438724518\n",
      "Epoch: 0, Loss: 0.48296838998794556\n",
      "Epoch: 0, Loss: 0.4836295545101166\n",
      "Epoch: 0, Loss: 0.4742271602153778\n",
      "Epoch: 0, Loss: 0.49307286739349365\n",
      "Epoch: 0, Loss: 0.4635283648967743\n",
      "Epoch: 0, Loss: 0.5615270733833313\n",
      "Epoch: 0, Loss: 0.5734912157058716\n",
      "Epoch: 0, Loss: 0.5215237140655518\n",
      "Epoch: 0, Loss: 0.4772249460220337\n",
      "Epoch: 0, Loss: 0.46546366810798645\n",
      "Epoch: 0, Loss: 0.5490001440048218\n",
      "Epoch: 0, Loss: 0.38778331875801086\n",
      "Epoch: 0, Loss: 0.5264431834220886\n",
      "Epoch: 0, Loss: 0.5148864984512329\n",
      "Epoch: 0, Loss: 0.4240473508834839\n",
      "Epoch: 0, Loss: 0.4945162534713745\n",
      "Epoch: 0, Loss: 0.5210290551185608\n",
      "Epoch: 0, Loss: 0.4427659511566162\n",
      "Epoch: 0, Loss: 0.5135220289230347\n",
      "Epoch: 0, Loss: 0.4960384964942932\n",
      "Epoch: 0, Loss: 0.38564473390579224\n",
      "Epoch: 0, Loss: 0.49806010723114014\n",
      "Epoch: 0, Loss: 0.5354167222976685\n",
      "Epoch: 0, Loss: 0.477092981338501\n",
      "Epoch: 0, Loss: 0.42118972539901733\n",
      "Epoch: 0, Loss: 0.5318708419799805\n",
      "Epoch: 0, Loss: 0.49962878227233887\n",
      "Epoch: 0, Loss: 0.4720745384693146\n",
      "Epoch: 0, Loss: 0.6206899881362915\n",
      "Epoch: 0, Loss: 0.5562896132469177\n",
      "Epoch: 0, Loss: 0.568165123462677\n",
      "Epoch: 0, Loss: 0.3898382782936096\n",
      "Epoch: 0, Loss: 0.5703352689743042\n",
      "Epoch: 0, Loss: 0.5633939504623413\n",
      "Epoch: 0, Loss: 0.5088919997215271\n",
      "Epoch: 0, Loss: 0.5405139923095703\n",
      "Epoch: 0, Loss: 0.5242555141448975\n",
      "Epoch: 0, Loss: 0.45833641290664673\n",
      "Epoch: 0, Loss: 0.4981614947319031\n",
      "Epoch: 0, Loss: 0.4403795599937439\n",
      "Epoch: 0, Loss: 0.45099806785583496\n",
      "Epoch: 0, Loss: 0.4151002764701843\n",
      "Epoch: 0, Loss: 0.5055299401283264\n",
      "Epoch: 0, Loss: 0.4888259768486023\n",
      "Epoch: 0, Loss: 0.5442269444465637\n",
      "Epoch: 0, Loss: 0.46770355105400085\n",
      "Epoch: 0, Loss: 0.518906831741333\n",
      "Epoch: 0, Loss: 0.5651472806930542\n",
      "Epoch: 0, Loss: 0.4496709704399109\n",
      "Epoch: 0, Loss: 0.4580998122692108\n",
      "Epoch: 0, Loss: 0.4366120398044586\n",
      "Epoch: 0, Loss: 0.48349907994270325\n",
      "Epoch: 0, Loss: 0.4286922216415405\n",
      "Epoch: 0, Loss: 0.553231954574585\n",
      "Epoch: 0, Loss: 0.5624481439590454\n",
      "Epoch: 0, Loss: 0.5700443387031555\n",
      "Epoch: 0, Loss: 0.48452848196029663\n",
      "Epoch: 0, Loss: 0.617786705493927\n",
      "Epoch: 0, Loss: 0.5515632033348083\n",
      "Epoch: 0, Loss: 0.5194317102432251\n",
      "Epoch: 0, Loss: 0.46779340505599976\n",
      "Epoch: 0, Loss: 0.6438172459602356\n",
      "Epoch: 0, Loss: 0.5352378487586975\n",
      "Epoch: 0, Loss: 0.3903873562812805\n",
      "Epoch: 0, Loss: 0.4980463981628418\n",
      "Epoch: 0, Loss: 0.4471677243709564\n",
      "Epoch: 0, Loss: 0.507495105266571\n",
      "Epoch: 0, Loss: 0.5666546821594238\n",
      "Epoch: 0, Loss: 0.49442946910858154\n",
      "Epoch: 0, Loss: 0.4045836925506592\n",
      "Epoch: 0, Loss: 0.5534852147102356\n",
      "Epoch: 0, Loss: 0.5079180002212524\n",
      "Epoch: 0, Loss: 0.47616899013519287\n",
      "Epoch: 0, Loss: 0.5278167724609375\n",
      "Epoch: 0, Loss: 0.4196438789367676\n",
      "Epoch: 0, Loss: 0.6124585270881653\n",
      "Epoch: 0, Loss: 0.507706344127655\n",
      "Epoch: 0, Loss: 0.5022303462028503\n",
      "Epoch: 0, Loss: 0.5727309584617615\n",
      "Epoch: 0, Loss: 0.44663092494010925\n",
      "Epoch: 0, Loss: 0.6025077104568481\n",
      "Epoch: 0, Loss: 0.41780567169189453\n",
      "Epoch: 0, Loss: 0.5217984318733215\n",
      "Epoch: 0, Loss: 0.4582425355911255\n",
      "Epoch: 0, Loss: 0.5022789239883423\n",
      "Epoch: 0, Loss: 0.5974045395851135\n",
      "Epoch: 0, Loss: 0.6044436097145081\n",
      "Epoch: 0, Loss: 0.5978720784187317\n",
      "Epoch: 0, Loss: 0.6125482320785522\n",
      "Epoch: 0, Loss: 0.530948281288147\n",
      "Epoch: 0, Loss: 0.43209904432296753\n",
      "Epoch: 0, Loss: 0.4795885384082794\n",
      "Epoch: 0, Loss: 0.45683205127716064\n",
      "Epoch: 0, Loss: 0.5177398920059204\n",
      "Epoch: 0, Loss: 0.4918355643749237\n",
      "Epoch: 0, Loss: 0.5057798624038696\n",
      "Epoch: 0, Loss: 0.44716453552246094\n",
      "Epoch: 0, Loss: 0.4932956099510193\n",
      "Epoch: 0, Loss: 0.5478251576423645\n",
      "Epoch: 0, Loss: 0.6055669784545898\n",
      "Epoch: 0, Loss: 0.5681720972061157\n",
      "Epoch: 0, Loss: 0.5107743740081787\n",
      "Epoch: 0, Loss: 0.5413132905960083\n",
      "Epoch: 0, Loss: 0.4218205213546753\n",
      "Epoch: 0, Loss: 0.5341275930404663\n",
      "Epoch: 0, Loss: 0.43845176696777344\n",
      "Epoch: 0, Loss: 0.5125399827957153\n",
      "Epoch: 0, Loss: 0.42004409432411194\n",
      "Epoch: 0, Loss: 0.5159652233123779\n",
      "Epoch: 0, Loss: 0.453968346118927\n",
      "Epoch: 0, Loss: 0.5114305019378662\n",
      "Epoch: 0, Loss: 0.4970899522304535\n",
      "Epoch: 0, Loss: 0.5226154327392578\n",
      "Epoch: 0, Loss: 0.474076509475708\n",
      "Epoch: 0, Loss: 0.4542910158634186\n",
      "Epoch: 0, Loss: 0.4969252943992615\n",
      "Epoch: 0, Loss: 0.568617582321167\n",
      "Epoch: 0, Loss: 0.4105461537837982\n",
      "Epoch: 0, Loss: 0.5219535231590271\n",
      "Epoch: 0, Loss: 0.38701173663139343\n",
      "Epoch: 0, Loss: 0.5443054437637329\n",
      "Epoch: 0, Loss: 0.45167261362075806\n",
      "Epoch: 0, Loss: 0.5215140581130981\n",
      "Epoch: 0, Loss: 0.5015715956687927\n",
      "Epoch: 0, Loss: 0.40429431200027466\n",
      "Epoch: 0, Loss: 0.5949981212615967\n",
      "Epoch: 0, Loss: 0.49311941862106323\n",
      "Epoch: 0, Loss: 0.47700345516204834\n",
      "Epoch: 0, Loss: 0.4192236661911011\n",
      "Epoch: 0, Loss: 0.4660454988479614\n",
      "Epoch: 0, Loss: 0.5123839378356934\n",
      "Epoch: 0, Loss: 0.6418768167495728\n",
      "Epoch: 0, Loss: 0.5666613578796387\n",
      "Epoch: 0, Loss: 0.486316978931427\n",
      "Epoch: 0, Loss: 0.4941295087337494\n",
      "Epoch: 0, Loss: 0.410129189491272\n",
      "Epoch: 0, Loss: 0.49747782945632935\n",
      "Epoch: 0, Loss: 0.47850415110588074\n",
      "Epoch: 0, Loss: 0.4667910039424896\n",
      "Epoch: 1, Loss: 0.5116139054298401\n",
      "Epoch: 1, Loss: 0.49917852878570557\n",
      "Epoch: 1, Loss: 0.5448858141899109\n",
      "Epoch: 1, Loss: 0.4875492453575134\n",
      "Epoch: 1, Loss: 0.4524645209312439\n",
      "Epoch: 1, Loss: 0.46992120146751404\n",
      "Epoch: 1, Loss: 0.43266332149505615\n",
      "Epoch: 1, Loss: 0.43722277879714966\n",
      "Epoch: 1, Loss: 0.49612346291542053\n",
      "Epoch: 1, Loss: 0.5324307084083557\n",
      "Epoch: 1, Loss: 0.44842657446861267\n",
      "Epoch: 1, Loss: 0.45283955335617065\n",
      "Epoch: 1, Loss: 0.6824491024017334\n",
      "Epoch: 1, Loss: 0.5053807497024536\n",
      "Epoch: 1, Loss: 0.6443145275115967\n",
      "Epoch: 1, Loss: 0.4558614492416382\n",
      "Epoch: 1, Loss: 0.480939656496048\n",
      "Epoch: 1, Loss: 0.4137491285800934\n",
      "Epoch: 1, Loss: 0.46923670172691345\n",
      "Epoch: 1, Loss: 0.49009349942207336\n",
      "Epoch: 1, Loss: 0.48314887285232544\n",
      "Epoch: 1, Loss: 0.4458516240119934\n",
      "Epoch: 1, Loss: 0.42601245641708374\n",
      "Epoch: 1, Loss: 0.5793638229370117\n",
      "Epoch: 1, Loss: 0.4755650758743286\n",
      "Epoch: 1, Loss: 0.6220434308052063\n",
      "Epoch: 1, Loss: 0.49192294478416443\n",
      "Epoch: 1, Loss: 0.501089334487915\n",
      "Epoch: 1, Loss: 0.5098860263824463\n",
      "Epoch: 1, Loss: 0.5364761352539062\n",
      "Epoch: 1, Loss: 0.4798052906990051\n",
      "Epoch: 1, Loss: 0.6033271551132202\n",
      "Epoch: 1, Loss: 0.4360406696796417\n",
      "Epoch: 1, Loss: 0.4557529091835022\n",
      "Epoch: 1, Loss: 0.4845203459262848\n",
      "Epoch: 1, Loss: 0.6491597890853882\n",
      "Epoch: 1, Loss: 0.4226554036140442\n",
      "Epoch: 1, Loss: 0.4219500720500946\n",
      "Epoch: 1, Loss: 0.6060724258422852\n",
      "Epoch: 1, Loss: 0.4881829619407654\n",
      "Epoch: 1, Loss: 0.4767152965068817\n",
      "Epoch: 1, Loss: 0.48485708236694336\n",
      "Epoch: 1, Loss: 0.49923574924468994\n",
      "Epoch: 1, Loss: 0.556443452835083\n",
      "Epoch: 1, Loss: 0.6406692266464233\n",
      "Epoch: 1, Loss: 0.5089269280433655\n",
      "Epoch: 1, Loss: 0.4757207930088043\n",
      "Epoch: 1, Loss: 0.4942343831062317\n",
      "Epoch: 1, Loss: 0.4795740842819214\n",
      "Epoch: 1, Loss: 0.549659013748169\n",
      "Epoch: 1, Loss: 0.4406295418739319\n",
      "Epoch: 1, Loss: 0.5441400408744812\n",
      "Epoch: 1, Loss: 0.5424323081970215\n",
      "Epoch: 1, Loss: 0.5055570602416992\n",
      "Epoch: 1, Loss: 0.4497292637825012\n",
      "Epoch: 1, Loss: 0.5815548300743103\n",
      "Epoch: 1, Loss: 0.529405415058136\n",
      "Epoch: 1, Loss: 0.5518659353256226\n",
      "Epoch: 1, Loss: 0.4716179668903351\n",
      "Epoch: 1, Loss: 0.4368734657764435\n",
      "Epoch: 1, Loss: 0.48720842599868774\n",
      "Epoch: 1, Loss: 0.6321629285812378\n",
      "Epoch: 1, Loss: 0.5936126112937927\n",
      "Epoch: 1, Loss: 0.49091583490371704\n",
      "Epoch: 1, Loss: 0.4251725673675537\n",
      "Epoch: 1, Loss: 0.5375835299491882\n",
      "Epoch: 1, Loss: 0.3925938606262207\n",
      "Epoch: 1, Loss: 0.48393431305885315\n",
      "Epoch: 1, Loss: 0.5001007914543152\n",
      "Epoch: 1, Loss: 0.41551995277404785\n",
      "Epoch: 1, Loss: 0.6162043809890747\n",
      "Epoch: 1, Loss: 0.4661719799041748\n",
      "Epoch: 1, Loss: 0.48218995332717896\n",
      "Epoch: 1, Loss: 0.5155127048492432\n",
      "Epoch: 1, Loss: 0.4816097617149353\n",
      "Epoch: 1, Loss: 0.5385589003562927\n",
      "Epoch: 1, Loss: 0.4706880748271942\n",
      "Epoch: 1, Loss: 0.5481629371643066\n",
      "Epoch: 1, Loss: 0.507107675075531\n",
      "Epoch: 1, Loss: 0.5776486396789551\n",
      "Epoch: 1, Loss: 0.5064153671264648\n",
      "Epoch: 1, Loss: 0.4806729853153229\n",
      "Epoch: 1, Loss: 0.594304084777832\n",
      "Epoch: 1, Loss: 0.46379685401916504\n",
      "Epoch: 1, Loss: 0.5006064176559448\n",
      "Epoch: 1, Loss: 0.5367025136947632\n",
      "Epoch: 1, Loss: 0.48092442750930786\n",
      "Epoch: 1, Loss: 0.4576604664325714\n",
      "Epoch: 1, Loss: 0.4911508560180664\n",
      "Epoch: 1, Loss: 0.4063326120376587\n",
      "Epoch: 1, Loss: 0.48837918043136597\n",
      "Epoch: 1, Loss: 0.46724921464920044\n",
      "Epoch: 1, Loss: 0.44866323471069336\n",
      "Epoch: 1, Loss: 0.44954317808151245\n",
      "Epoch: 1, Loss: 0.40989387035369873\n",
      "Epoch: 1, Loss: 0.5050837397575378\n",
      "Epoch: 1, Loss: 0.45839667320251465\n",
      "Epoch: 1, Loss: 0.48173633217811584\n",
      "Epoch: 1, Loss: 0.5626404285430908\n",
      "Epoch: 1, Loss: 0.4644756019115448\n",
      "Epoch: 1, Loss: 0.4946479797363281\n",
      "Epoch: 1, Loss: 0.48406699299812317\n",
      "Epoch: 1, Loss: 0.5929829478263855\n",
      "Epoch: 1, Loss: 0.4403252899646759\n",
      "Epoch: 1, Loss: 0.6000081896781921\n",
      "Epoch: 1, Loss: 0.3981449007987976\n",
      "Epoch: 1, Loss: 0.4627760648727417\n",
      "Epoch: 1, Loss: 0.41648510098457336\n",
      "Epoch: 1, Loss: 0.4643975496292114\n",
      "Epoch: 1, Loss: 0.48232758045196533\n",
      "Epoch: 1, Loss: 0.5895885825157166\n",
      "Epoch: 1, Loss: 0.617391049861908\n",
      "Epoch: 1, Loss: 0.4744411110877991\n",
      "Epoch: 1, Loss: 0.45029976963996887\n",
      "Epoch: 1, Loss: 0.40047240257263184\n",
      "Epoch: 1, Loss: 0.5749253630638123\n",
      "Epoch: 1, Loss: 0.5029318928718567\n",
      "Epoch: 1, Loss: 0.5329792499542236\n",
      "Epoch: 1, Loss: 0.44260722398757935\n",
      "Epoch: 1, Loss: 0.46914806962013245\n",
      "Epoch: 1, Loss: 0.4823862314224243\n",
      "Epoch: 1, Loss: 0.4796684980392456\n",
      "Epoch: 1, Loss: 0.4674585461616516\n",
      "Epoch: 1, Loss: 0.49035894870758057\n",
      "Epoch: 1, Loss: 0.4122673571109772\n",
      "Epoch: 1, Loss: 0.6149472594261169\n",
      "Epoch: 1, Loss: 0.4995250701904297\n",
      "Epoch: 1, Loss: 0.45200929045677185\n",
      "Epoch: 1, Loss: 0.4798324406147003\n",
      "Epoch: 1, Loss: 0.4993528723716736\n",
      "Epoch: 1, Loss: 0.4643823504447937\n",
      "Epoch: 1, Loss: 0.6006333827972412\n",
      "Epoch: 1, Loss: 0.45797353982925415\n",
      "Epoch: 1, Loss: 0.420641154050827\n",
      "Epoch: 1, Loss: 0.4798972010612488\n",
      "Epoch: 1, Loss: 0.6007082462310791\n",
      "Epoch: 1, Loss: 0.40680211782455444\n",
      "Epoch: 1, Loss: 0.4390697479248047\n",
      "Epoch: 1, Loss: 0.6006243228912354\n",
      "Epoch: 1, Loss: 0.4257442057132721\n",
      "Epoch: 1, Loss: 0.5263196229934692\n",
      "Epoch: 1, Loss: 0.520412266254425\n",
      "Epoch: 1, Loss: 0.46774035692214966\n",
      "Epoch: 1, Loss: 0.5326554775238037\n",
      "Epoch: 1, Loss: 0.565130889415741\n",
      "Epoch: 1, Loss: 0.38674527406692505\n",
      "Epoch: 1, Loss: 0.4999263286590576\n",
      "Epoch: 1, Loss: 0.46464744210243225\n",
      "Epoch: 1, Loss: 0.5207902193069458\n",
      "Epoch: 1, Loss: 0.4816112518310547\n",
      "Epoch: 1, Loss: 0.6045536398887634\n",
      "Epoch: 1, Loss: 0.4358082413673401\n",
      "Epoch: 1, Loss: 0.48383331298828125\n",
      "Epoch: 1, Loss: 0.5959019064903259\n",
      "Epoch: 1, Loss: 0.48598331212997437\n",
      "Epoch: 1, Loss: 0.5184621214866638\n",
      "Epoch: 1, Loss: 0.4337110221385956\n",
      "Epoch: 1, Loss: 0.44872331619262695\n",
      "Epoch: 1, Loss: 0.41523975133895874\n",
      "Epoch: 1, Loss: 0.4770314395427704\n",
      "Epoch: 1, Loss: 0.5516543984413147\n",
      "Epoch: 1, Loss: 0.4211462140083313\n",
      "Epoch: 1, Loss: 0.46740466356277466\n",
      "Epoch: 1, Loss: 0.40566250681877136\n",
      "Epoch: 1, Loss: 0.4633762240409851\n",
      "Epoch: 1, Loss: 0.4373486340045929\n",
      "Epoch: 1, Loss: 0.5658270120620728\n",
      "Epoch: 1, Loss: 0.4500049352645874\n",
      "Epoch: 1, Loss: 0.5576484799385071\n",
      "Epoch: 1, Loss: 0.5805378556251526\n",
      "Epoch: 1, Loss: 0.5578517913818359\n",
      "Epoch: 1, Loss: 0.4470391869544983\n",
      "Epoch: 1, Loss: 0.43137675523757935\n",
      "Epoch: 1, Loss: 0.4416084885597229\n",
      "Epoch: 1, Loss: 0.5403861403465271\n",
      "Epoch: 1, Loss: 0.48024025559425354\n",
      "Epoch: 1, Loss: 0.5556563138961792\n",
      "Epoch: 1, Loss: 0.5212074518203735\n",
      "Epoch: 1, Loss: 0.47333282232284546\n",
      "Epoch: 1, Loss: 0.3787001669406891\n",
      "Epoch: 1, Loss: 0.4184926748275757\n",
      "Epoch: 1, Loss: 0.526594877243042\n",
      "Epoch: 1, Loss: 0.3966448903083801\n",
      "Epoch: 1, Loss: 0.41235798597335815\n",
      "Epoch: 1, Loss: 0.480447381734848\n",
      "Epoch: 1, Loss: 0.642816960811615\n",
      "Epoch: 1, Loss: 0.5468938946723938\n",
      "Epoch: 1, Loss: 0.5253045558929443\n",
      "Epoch: 1, Loss: 0.4820827841758728\n",
      "Epoch: 1, Loss: 0.49038955569267273\n",
      "Epoch: 1, Loss: 0.4433557093143463\n",
      "Epoch: 1, Loss: 0.478135883808136\n",
      "Epoch: 1, Loss: 0.36112451553344727\n",
      "Epoch: 1, Loss: 0.4458639323711395\n",
      "Epoch: 1, Loss: 0.4095359742641449\n",
      "Epoch: 1, Loss: 0.5279113054275513\n",
      "Epoch: 1, Loss: 0.5321266651153564\n",
      "Epoch: 1, Loss: 0.4844239056110382\n",
      "Epoch: 1, Loss: 0.5767426490783691\n",
      "Epoch: 1, Loss: 0.5426678657531738\n",
      "Epoch: 1, Loss: 0.49145087599754333\n",
      "Epoch: 1, Loss: 0.43637314438819885\n",
      "Epoch: 1, Loss: 0.5170784592628479\n",
      "Epoch: 1, Loss: 0.6716357469558716\n",
      "Epoch: 1, Loss: 0.5504891872406006\n",
      "Epoch: 1, Loss: 0.5287061333656311\n",
      "Epoch: 1, Loss: 0.5629680156707764\n",
      "Epoch: 1, Loss: 0.4368264377117157\n",
      "Epoch: 1, Loss: 0.45723646879196167\n",
      "Epoch: 1, Loss: 0.5343497395515442\n",
      "Epoch: 1, Loss: 0.44135648012161255\n",
      "Epoch: 1, Loss: 0.41121938824653625\n",
      "Epoch: 1, Loss: 0.5314075946807861\n",
      "Epoch: 1, Loss: 0.48929473757743835\n",
      "Epoch: 1, Loss: 0.47254660725593567\n",
      "Epoch: 1, Loss: 0.447950154542923\n",
      "Epoch: 1, Loss: 0.4785691201686859\n",
      "Epoch: 1, Loss: 0.6109030842781067\n",
      "Epoch: 1, Loss: 0.4526152014732361\n",
      "Epoch: 1, Loss: 0.534357488155365\n",
      "Epoch: 1, Loss: 0.4997040927410126\n",
      "Epoch: 1, Loss: 0.4478800892829895\n",
      "Epoch: 1, Loss: 0.5103144645690918\n",
      "Epoch: 1, Loss: 0.5047439932823181\n",
      "Epoch: 1, Loss: 0.48380839824676514\n",
      "Epoch: 1, Loss: 0.5331591367721558\n",
      "Epoch: 1, Loss: 0.45454666018486023\n",
      "Epoch: 1, Loss: 0.6890092492103577\n",
      "Epoch: 1, Loss: 0.4387146532535553\n",
      "Epoch: 1, Loss: 0.6090329885482788\n",
      "Epoch: 1, Loss: 0.5849772691726685\n",
      "Epoch: 1, Loss: 0.5567296147346497\n",
      "Epoch: 1, Loss: 0.5041177272796631\n",
      "Epoch: 1, Loss: 0.4249236583709717\n",
      "Epoch: 1, Loss: 0.557550311088562\n",
      "Epoch: 1, Loss: 0.4685085415840149\n",
      "Epoch: 1, Loss: 0.5092964768409729\n",
      "Epoch: 1, Loss: 0.5698662996292114\n",
      "Epoch: 1, Loss: 0.43187767267227173\n",
      "Epoch: 1, Loss: 0.4989504814147949\n",
      "Epoch: 1, Loss: 0.5579559803009033\n",
      "Epoch: 1, Loss: 0.4925116002559662\n",
      "Epoch: 1, Loss: 0.46492883563041687\n",
      "Epoch: 1, Loss: 0.4998794198036194\n",
      "Epoch: 1, Loss: 0.4122520685195923\n",
      "Epoch: 1, Loss: 0.5343821048736572\n",
      "Epoch: 1, Loss: 0.6334812045097351\n",
      "Epoch: 1, Loss: 0.6611080169677734\n",
      "Epoch: 1, Loss: 0.44088250398635864\n",
      "Epoch: 1, Loss: 0.45208072662353516\n",
      "Epoch: 1, Loss: 0.5750537514686584\n",
      "Epoch: 1, Loss: 0.490945041179657\n",
      "Epoch: 1, Loss: 0.45949262380599976\n",
      "Epoch: 1, Loss: 0.5096477270126343\n",
      "Epoch: 1, Loss: 0.583725094795227\n",
      "Epoch: 1, Loss: 0.5025864243507385\n",
      "Epoch: 1, Loss: 0.6640353798866272\n",
      "Epoch: 1, Loss: 0.4957805573940277\n",
      "Epoch: 1, Loss: 0.5306394696235657\n",
      "Epoch: 1, Loss: 0.5841017961502075\n",
      "Epoch: 1, Loss: 0.46021056175231934\n",
      "Epoch: 1, Loss: 0.5972495675086975\n",
      "Epoch: 1, Loss: 0.4301193058490753\n",
      "Epoch: 1, Loss: 0.5277118682861328\n",
      "Epoch: 1, Loss: 0.42020756006240845\n",
      "Epoch: 1, Loss: 0.5123937129974365\n",
      "Epoch: 1, Loss: 0.49350491166114807\n",
      "Epoch: 1, Loss: 0.5006462931632996\n",
      "Epoch: 1, Loss: 0.45865514874458313\n",
      "Epoch: 1, Loss: 0.47421735525131226\n",
      "Epoch: 1, Loss: 0.4803549647331238\n",
      "Epoch: 1, Loss: 0.4064914882183075\n",
      "Epoch: 1, Loss: 0.491391658782959\n",
      "Epoch: 1, Loss: 0.5174601078033447\n",
      "Epoch: 1, Loss: 0.42909759283065796\n",
      "Epoch: 1, Loss: 0.5124240517616272\n",
      "Epoch: 1, Loss: 0.47834429144859314\n",
      "Epoch: 1, Loss: 0.5133176445960999\n",
      "Epoch: 1, Loss: 0.48607873916625977\n",
      "Epoch: 1, Loss: 0.4985002279281616\n",
      "Epoch: 1, Loss: 0.46851176023483276\n",
      "Epoch: 1, Loss: 0.49688228964805603\n",
      "Epoch: 1, Loss: 0.4450346827507019\n",
      "Epoch: 1, Loss: 0.5550042986869812\n",
      "Epoch: 1, Loss: 0.41282954812049866\n",
      "Epoch: 1, Loss: 0.6207035779953003\n",
      "Epoch: 1, Loss: 0.5282450914382935\n",
      "Epoch: 1, Loss: 0.5758395195007324\n",
      "Epoch: 1, Loss: 0.44577914476394653\n",
      "Epoch: 1, Loss: 0.4072786271572113\n",
      "Epoch: 1, Loss: 0.45869314670562744\n",
      "Epoch: 1, Loss: 0.4073074460029602\n",
      "Epoch: 1, Loss: 0.5519293546676636\n",
      "Epoch: 1, Loss: 0.6538448333740234\n",
      "Epoch: 1, Loss: 0.47433918714523315\n",
      "Epoch: 1, Loss: 0.5349376201629639\n",
      "Epoch: 1, Loss: 0.43900570273399353\n",
      "Epoch: 1, Loss: 0.644986093044281\n",
      "Epoch: 1, Loss: 0.4953754246234894\n",
      "Epoch: 1, Loss: 0.46101993322372437\n",
      "Epoch: 1, Loss: 0.4636416435241699\n",
      "Epoch: 1, Loss: 0.489290326833725\n",
      "Epoch: 1, Loss: 0.4434550106525421\n",
      "Epoch: 1, Loss: 0.46037182211875916\n",
      "Epoch: 1, Loss: 0.5325486660003662\n",
      "Epoch: 1, Loss: 0.5032093524932861\n",
      "Epoch: 1, Loss: 0.5011801719665527\n",
      "Epoch: 1, Loss: 0.5861561298370361\n",
      "Epoch: 1, Loss: 0.5903366208076477\n",
      "Epoch: 1, Loss: 0.5562056303024292\n",
      "Epoch: 1, Loss: 0.49762752652168274\n",
      "Epoch: 1, Loss: 0.4472915828227997\n",
      "Epoch: 1, Loss: 0.42028072476387024\n",
      "Epoch: 1, Loss: 0.3983765244483948\n",
      "Epoch: 1, Loss: 0.5113831758499146\n",
      "Epoch: 1, Loss: 0.4945882558822632\n",
      "Epoch: 1, Loss: 0.4883987307548523\n",
      "Epoch: 1, Loss: 0.7236549258232117\n",
      "Epoch: 1, Loss: 0.5057976841926575\n",
      "Epoch: 1, Loss: 0.5189530253410339\n",
      "Epoch: 1, Loss: 0.45648306608200073\n",
      "Epoch: 1, Loss: 0.6122990250587463\n",
      "Epoch: 1, Loss: 0.5136069059371948\n",
      "Epoch: 1, Loss: 0.4884333908557892\n",
      "Epoch: 1, Loss: 0.4577363431453705\n",
      "Epoch: 1, Loss: 0.4995254874229431\n",
      "Epoch: 1, Loss: 0.5359362959861755\n",
      "Epoch: 1, Loss: 0.4314582347869873\n",
      "Epoch: 1, Loss: 0.5754928588867188\n",
      "Epoch: 1, Loss: 0.5015695691108704\n",
      "Epoch: 1, Loss: 0.5444631576538086\n",
      "Epoch: 1, Loss: 0.5336691737174988\n",
      "Epoch: 1, Loss: 0.46391957998275757\n",
      "Epoch: 1, Loss: 0.46804171800613403\n",
      "Epoch: 1, Loss: 0.5953032374382019\n",
      "Epoch: 1, Loss: 0.39997416734695435\n",
      "Epoch: 1, Loss: 0.5015725493431091\n",
      "Epoch: 1, Loss: 0.41553765535354614\n",
      "Epoch: 1, Loss: 0.47397589683532715\n",
      "Epoch: 1, Loss: 0.5860723257064819\n",
      "Epoch: 1, Loss: 0.5743720531463623\n",
      "Epoch: 1, Loss: 0.5699212551116943\n",
      "Epoch: 1, Loss: 0.45926010608673096\n",
      "Epoch: 1, Loss: 0.4635777175426483\n",
      "Epoch: 1, Loss: 0.5705565214157104\n",
      "Epoch: 1, Loss: 0.4962612986564636\n",
      "Epoch: 1, Loss: 0.5471487045288086\n",
      "Epoch: 1, Loss: 0.5353469848632812\n",
      "Epoch: 1, Loss: 0.5036588907241821\n",
      "Epoch: 1, Loss: 0.45534631609916687\n",
      "Epoch: 1, Loss: 0.4910781979560852\n",
      "Epoch: 1, Loss: 0.5905570983886719\n",
      "Epoch: 1, Loss: 0.4787614941596985\n",
      "Epoch: 1, Loss: 0.6488218307495117\n",
      "Epoch: 1, Loss: 0.41015735268592834\n",
      "Epoch: 1, Loss: 0.4244733452796936\n",
      "Epoch: 1, Loss: 0.48302850127220154\n",
      "Epoch: 1, Loss: 0.45237991213798523\n",
      "Epoch: 1, Loss: 0.4360441267490387\n",
      "Epoch: 1, Loss: 0.6055126190185547\n",
      "Epoch: 1, Loss: 0.40556079149246216\n",
      "Epoch: 1, Loss: 0.4867645800113678\n",
      "Epoch: 1, Loss: 0.5449891090393066\n",
      "Epoch: 1, Loss: 0.4195324182510376\n",
      "Epoch: 1, Loss: 0.42040950059890747\n",
      "Epoch: 1, Loss: 0.5383813381195068\n",
      "Epoch: 1, Loss: 0.5132129788398743\n",
      "Epoch: 1, Loss: 0.5110877156257629\n",
      "Epoch: 1, Loss: 0.4216240644454956\n",
      "Epoch: 1, Loss: 0.5520246624946594\n",
      "Epoch: 1, Loss: 0.4789045751094818\n",
      "Epoch: 1, Loss: 0.5185268521308899\n",
      "Epoch: 1, Loss: 0.5590042471885681\n",
      "Epoch: 1, Loss: 0.551263689994812\n",
      "Epoch: 1, Loss: 0.4485098123550415\n",
      "Epoch: 1, Loss: 0.4785449504852295\n",
      "Epoch: 1, Loss: 0.5035208463668823\n",
      "Epoch: 1, Loss: 0.4281337857246399\n",
      "Epoch: 1, Loss: 0.4389428198337555\n",
      "Epoch: 1, Loss: 0.5363712310791016\n",
      "Epoch: 1, Loss: 0.555345892906189\n",
      "Epoch: 1, Loss: 0.4067111015319824\n",
      "Epoch: 1, Loss: 0.5251625776290894\n",
      "Epoch: 1, Loss: 0.43213731050491333\n",
      "Epoch: 1, Loss: 0.5556045770645142\n",
      "Epoch: 1, Loss: 0.552165687084198\n",
      "Epoch: 1, Loss: 0.3899497985839844\n",
      "Epoch: 1, Loss: 0.48499929904937744\n",
      "Epoch: 1, Loss: 0.4507702887058258\n",
      "Epoch: 1, Loss: 0.4626508355140686\n",
      "Epoch: 1, Loss: 0.5853649377822876\n",
      "Epoch: 1, Loss: 0.5431552529335022\n",
      "Epoch: 1, Loss: 0.5026001930236816\n",
      "Epoch: 1, Loss: 0.4171874523162842\n",
      "Epoch: 1, Loss: 0.5732128024101257\n",
      "Epoch: 1, Loss: 0.40828952193260193\n",
      "Epoch: 1, Loss: 0.5660229921340942\n",
      "Epoch: 1, Loss: 0.3741430342197418\n",
      "Epoch: 1, Loss: 0.42796552181243896\n",
      "Epoch: 1, Loss: 0.514237105846405\n",
      "Epoch: 1, Loss: 0.559563398361206\n",
      "Epoch: 1, Loss: 0.47076064348220825\n",
      "Epoch: 1, Loss: 0.6466830968856812\n",
      "Epoch: 1, Loss: 0.4066975712776184\n",
      "Epoch: 1, Loss: 0.49427953362464905\n",
      "Epoch: 1, Loss: 0.5169527530670166\n",
      "Epoch: 1, Loss: 0.5040058493614197\n",
      "Epoch: 1, Loss: 0.5813811421394348\n",
      "Epoch: 1, Loss: 0.523147702217102\n",
      "Epoch: 1, Loss: 0.4460434317588806\n",
      "Epoch: 1, Loss: 0.4331304728984833\n",
      "Epoch: 1, Loss: 0.46434956789016724\n",
      "Epoch: 1, Loss: 0.4943443834781647\n",
      "Epoch: 1, Loss: 0.604593813419342\n",
      "Epoch: 1, Loss: 0.44068634510040283\n",
      "Epoch: 1, Loss: 0.4700026512145996\n",
      "Epoch: 1, Loss: 0.4701695442199707\n",
      "Epoch: 1, Loss: 0.5196515321731567\n",
      "Epoch: 1, Loss: 0.48693224787712097\n",
      "Epoch: 1, Loss: 0.45345351099967957\n",
      "Epoch: 1, Loss: 0.4817936420440674\n",
      "Epoch: 1, Loss: 0.5594667792320251\n",
      "Epoch: 1, Loss: 0.4811696708202362\n",
      "Epoch: 1, Loss: 0.45516902208328247\n",
      "Epoch: 1, Loss: 0.3778077960014343\n",
      "Epoch: 1, Loss: 0.5128458738327026\n",
      "Epoch: 1, Loss: 0.5460864901542664\n",
      "Epoch: 1, Loss: 0.41861680150032043\n",
      "Epoch: 1, Loss: 0.46397560834884644\n",
      "Epoch: 1, Loss: 0.4184874892234802\n",
      "Epoch: 1, Loss: 0.4250966012477875\n",
      "Epoch: 1, Loss: 0.5324404835700989\n",
      "Epoch: 1, Loss: 0.5216830968856812\n",
      "Epoch: 1, Loss: 0.5270981192588806\n",
      "Epoch: 1, Loss: 0.41436395049095154\n",
      "Epoch: 1, Loss: 0.46157869696617126\n",
      "Epoch: 1, Loss: 0.43342673778533936\n",
      "Epoch: 1, Loss: 0.47114166617393494\n",
      "Epoch: 1, Loss: 0.44271403551101685\n",
      "Epoch: 1, Loss: 0.45445558428764343\n",
      "Epoch: 1, Loss: 0.48528212308883667\n",
      "Epoch: 1, Loss: 0.5421901941299438\n",
      "Epoch: 1, Loss: 0.37051957845687866\n",
      "Epoch: 1, Loss: 0.5730878710746765\n",
      "Epoch: 1, Loss: 0.4709317982196808\n",
      "Epoch: 1, Loss: 0.4315110146999359\n",
      "Epoch: 1, Loss: 0.4451996684074402\n",
      "Epoch: 1, Loss: 0.49992531538009644\n",
      "Epoch: 1, Loss: 0.5752256512641907\n",
      "Epoch: 1, Loss: 0.4874514937400818\n",
      "Epoch: 1, Loss: 0.6077492237091064\n",
      "Epoch: 1, Loss: 0.7435237765312195\n",
      "Epoch: 1, Loss: 0.4595339596271515\n",
      "Epoch: 1, Loss: 0.4852079749107361\n",
      "Epoch: 1, Loss: 0.49713724851608276\n",
      "Epoch: 1, Loss: 0.464356392621994\n",
      "Epoch: 1, Loss: 0.5805903673171997\n",
      "Epoch: 1, Loss: 0.498160719871521\n",
      "Epoch: 1, Loss: 0.46231675148010254\n",
      "Epoch: 1, Loss: 0.471174031496048\n",
      "Epoch: 1, Loss: 0.442848265171051\n",
      "Epoch: 1, Loss: 0.44495871663093567\n",
      "Epoch: 1, Loss: 0.5574188232421875\n",
      "Epoch: 1, Loss: 0.402296245098114\n",
      "Epoch: 1, Loss: 0.436437726020813\n",
      "Epoch: 1, Loss: 0.46803104877471924\n",
      "Epoch: 1, Loss: 0.46134817600250244\n",
      "Epoch: 1, Loss: 0.4121014475822449\n",
      "Epoch: 1, Loss: 0.47492220997810364\n",
      "Epoch: 1, Loss: 0.5835676789283752\n",
      "Epoch: 1, Loss: 0.4868755042552948\n",
      "Epoch: 1, Loss: 0.6111863255500793\n",
      "Epoch: 1, Loss: 0.5738400220870972\n",
      "Epoch: 1, Loss: 0.447699636220932\n",
      "Epoch: 1, Loss: 0.4381983280181885\n",
      "Epoch: 1, Loss: 0.412297785282135\n",
      "Epoch: 1, Loss: 0.4637044668197632\n",
      "Epoch: 1, Loss: 0.5103831887245178\n",
      "Epoch: 1, Loss: 0.39248138666152954\n",
      "Epoch: 1, Loss: 0.40922215580940247\n",
      "Epoch: 1, Loss: 0.5512471795082092\n",
      "Epoch: 1, Loss: 0.3845726549625397\n",
      "Epoch: 1, Loss: 0.5417218208312988\n",
      "Epoch: 1, Loss: 0.7030001878738403\n",
      "Epoch: 1, Loss: 0.5554717779159546\n",
      "Epoch: 1, Loss: 0.5213655829429626\n",
      "Epoch: 1, Loss: 0.44380509853363037\n",
      "Epoch: 1, Loss: 0.44703468680381775\n",
      "Epoch: 1, Loss: 0.40506094694137573\n",
      "Epoch: 1, Loss: 0.47462406754493713\n",
      "Epoch: 1, Loss: 0.4270932674407959\n",
      "Epoch: 1, Loss: 0.5172863602638245\n",
      "Epoch: 1, Loss: 0.43167972564697266\n",
      "Epoch: 1, Loss: 0.49422502517700195\n",
      "Epoch: 1, Loss: 0.44896242022514343\n",
      "Epoch: 1, Loss: 0.4716612696647644\n",
      "Epoch: 1, Loss: 0.45814675092697144\n",
      "Epoch: 1, Loss: 0.5655606985092163\n",
      "Epoch: 1, Loss: 0.43817269802093506\n",
      "Epoch: 1, Loss: 0.4868757426738739\n",
      "Epoch: 1, Loss: 0.52239590883255\n",
      "Epoch: 1, Loss: 0.4690781831741333\n",
      "Epoch: 1, Loss: 0.47085633873939514\n",
      "Epoch: 1, Loss: 0.46674394607543945\n",
      "Epoch: 1, Loss: 0.5256814956665039\n",
      "Epoch: 1, Loss: 0.5395228862762451\n",
      "Epoch: 1, Loss: 0.6294775009155273\n",
      "Epoch: 1, Loss: 0.48146480321884155\n",
      "Epoch: 1, Loss: 0.43142956495285034\n",
      "Epoch: 1, Loss: 0.4070729613304138\n",
      "Epoch: 1, Loss: 0.4296881854534149\n",
      "Epoch: 1, Loss: 0.4304957985877991\n",
      "Epoch: 1, Loss: 0.45192965865135193\n",
      "Epoch: 1, Loss: 0.445987343788147\n",
      "Epoch: 1, Loss: 0.582644522190094\n",
      "Epoch: 1, Loss: 0.49232980608940125\n",
      "Epoch: 1, Loss: 0.5302743911743164\n",
      "Epoch: 1, Loss: 0.3984248638153076\n",
      "Epoch: 1, Loss: 0.4293285012245178\n",
      "Epoch: 1, Loss: 0.43133848905563354\n",
      "Epoch: 1, Loss: 0.5070271492004395\n",
      "Epoch: 1, Loss: 0.5186700224876404\n",
      "Epoch: 1, Loss: 0.4445028603076935\n",
      "Epoch: 1, Loss: 0.5306498408317566\n",
      "Epoch: 1, Loss: 0.466198205947876\n",
      "Epoch: 1, Loss: 0.4848290681838989\n",
      "Epoch: 1, Loss: 0.46570342779159546\n",
      "Epoch: 1, Loss: 0.5487556457519531\n",
      "Epoch: 1, Loss: 0.5508052110671997\n",
      "Epoch: 1, Loss: 0.5581378936767578\n",
      "Epoch: 1, Loss: 0.4503982961177826\n",
      "Epoch: 1, Loss: 0.4415106773376465\n",
      "Epoch: 1, Loss: 0.45605820417404175\n",
      "Epoch: 1, Loss: 0.5726327896118164\n",
      "Epoch: 1, Loss: 0.4874524176120758\n",
      "Epoch: 1, Loss: 0.43456414341926575\n",
      "Epoch: 1, Loss: 0.4538660943508148\n",
      "Epoch: 1, Loss: 0.4602535367012024\n",
      "Epoch: 1, Loss: 0.5001540184020996\n",
      "Epoch: 1, Loss: 0.4312705397605896\n",
      "Epoch: 1, Loss: 0.4399448037147522\n",
      "Epoch: 1, Loss: 0.5025712847709656\n",
      "Epoch: 1, Loss: 0.4356412887573242\n",
      "Epoch: 1, Loss: 0.5066445469856262\n",
      "Epoch: 1, Loss: 0.5509554743766785\n",
      "Epoch: 1, Loss: 0.7409714460372925\n",
      "Epoch: 1, Loss: 0.43852168321609497\n",
      "Epoch: 1, Loss: 0.4148504137992859\n",
      "Epoch: 1, Loss: 0.48926395177841187\n",
      "Epoch: 1, Loss: 0.4300681948661804\n",
      "Epoch: 1, Loss: 0.480632483959198\n",
      "Epoch: 1, Loss: 0.4274682402610779\n",
      "Epoch: 1, Loss: 0.5962180495262146\n",
      "Epoch: 1, Loss: 0.43208205699920654\n",
      "Epoch: 1, Loss: 0.49239104986190796\n",
      "Epoch: 1, Loss: 0.6308470368385315\n",
      "Epoch: 1, Loss: 0.5216031670570374\n",
      "Epoch: 1, Loss: 0.6623681783676147\n",
      "Epoch: 1, Loss: 0.3931853175163269\n",
      "Epoch: 1, Loss: 0.4071388244628906\n",
      "Epoch: 1, Loss: 0.49525463581085205\n",
      "Epoch: 1, Loss: 0.4781002700328827\n",
      "Epoch: 1, Loss: 0.4818822145462036\n",
      "Epoch: 1, Loss: 0.4255818724632263\n",
      "Epoch: 1, Loss: 0.40514618158340454\n",
      "Epoch: 1, Loss: 0.4925869405269623\n",
      "Epoch: 1, Loss: 0.5118473172187805\n",
      "Epoch: 1, Loss: 0.4654952883720398\n",
      "Epoch: 1, Loss: 0.5007209181785583\n",
      "Epoch: 1, Loss: 0.5178507566452026\n",
      "Epoch: 1, Loss: 0.39504170417785645\n",
      "Epoch: 1, Loss: 0.44631195068359375\n",
      "Epoch: 1, Loss: 0.5608462691307068\n",
      "Epoch: 1, Loss: 0.49100497364997864\n",
      "Epoch: 1, Loss: 0.4814029932022095\n",
      "Epoch: 1, Loss: 0.5323469042778015\n",
      "Epoch: 1, Loss: 0.6237678527832031\n",
      "Epoch: 1, Loss: 0.4977250397205353\n",
      "Epoch: 1, Loss: 0.45770716667175293\n",
      "Epoch: 1, Loss: 0.5202882289886475\n",
      "Epoch: 1, Loss: 0.585265040397644\n",
      "Epoch: 1, Loss: 0.4801773726940155\n",
      "Epoch: 1, Loss: 0.5436671376228333\n",
      "Epoch: 1, Loss: 0.5469381213188171\n",
      "Epoch: 1, Loss: 0.5100764632225037\n",
      "Epoch: 1, Loss: 0.4759449362754822\n",
      "Epoch: 1, Loss: 0.43335750699043274\n",
      "Epoch: 1, Loss: 0.4371180236339569\n",
      "Epoch: 1, Loss: 0.408939003944397\n",
      "Epoch: 1, Loss: 0.48081502318382263\n",
      "Epoch: 1, Loss: 0.6928701996803284\n",
      "Epoch: 1, Loss: 0.39444756507873535\n",
      "Epoch: 1, Loss: 0.4901467561721802\n",
      "Epoch: 1, Loss: 0.41908180713653564\n",
      "Epoch: 1, Loss: 0.6441931128501892\n",
      "Epoch: 1, Loss: 0.5535801649093628\n",
      "Epoch: 1, Loss: 0.613510012626648\n",
      "Epoch: 1, Loss: 0.394461065530777\n",
      "Epoch: 1, Loss: 0.44886064529418945\n",
      "Epoch: 1, Loss: 0.46320146322250366\n",
      "Epoch: 1, Loss: 0.5493141412734985\n",
      "Epoch: 1, Loss: 0.4423312544822693\n",
      "Epoch: 1, Loss: 0.4696500599384308\n",
      "Epoch: 1, Loss: 0.4643484354019165\n",
      "Epoch: 1, Loss: 0.46432995796203613\n",
      "Epoch: 1, Loss: 0.4315938353538513\n",
      "Epoch: 1, Loss: 0.4088105857372284\n",
      "Epoch: 1, Loss: 0.4415014982223511\n",
      "Epoch: 1, Loss: 0.539868175983429\n",
      "Epoch: 1, Loss: 0.5899285078048706\n",
      "Epoch: 1, Loss: 0.42238539457321167\n",
      "Epoch: 1, Loss: 0.4388963282108307\n",
      "Epoch: 1, Loss: 0.4639303982257843\n",
      "Epoch: 1, Loss: 0.4454372823238373\n",
      "Epoch: 1, Loss: 0.4721084535121918\n",
      "Epoch: 1, Loss: 0.42895400524139404\n",
      "Epoch: 1, Loss: 0.5002290606498718\n",
      "Epoch: 1, Loss: 0.515137791633606\n",
      "Epoch: 1, Loss: 0.6634325385093689\n",
      "Epoch: 1, Loss: 0.5180059671401978\n",
      "Epoch: 1, Loss: 0.4937809407711029\n",
      "Epoch: 1, Loss: 0.4063217341899872\n",
      "Epoch: 1, Loss: 0.5097887516021729\n",
      "Epoch: 1, Loss: 0.411018431186676\n",
      "Epoch: 1, Loss: 0.5241525173187256\n",
      "Epoch: 1, Loss: 0.5018543601036072\n",
      "Epoch: 1, Loss: 0.5674431324005127\n",
      "Epoch: 1, Loss: 0.4960286617279053\n",
      "Epoch: 1, Loss: 0.4825781285762787\n",
      "Epoch: 1, Loss: 0.4743187427520752\n",
      "Epoch: 1, Loss: 0.5207993984222412\n",
      "Epoch: 1, Loss: 0.4819734990596771\n",
      "Epoch: 1, Loss: 0.5964174866676331\n",
      "Epoch: 1, Loss: 0.3829800486564636\n",
      "Epoch: 1, Loss: 0.694334089756012\n",
      "Epoch: 1, Loss: 0.6016718149185181\n",
      "Epoch: 1, Loss: 0.5299873352050781\n",
      "Epoch: 1, Loss: 0.5291144251823425\n",
      "Epoch: 1, Loss: 0.41899803280830383\n",
      "Epoch: 1, Loss: 0.6253674626350403\n",
      "Epoch: 1, Loss: 0.4840135872364044\n",
      "Epoch: 1, Loss: 0.5729014277458191\n",
      "Epoch: 1, Loss: 0.4359266757965088\n",
      "Epoch: 1, Loss: 0.4829941391944885\n",
      "Epoch: 1, Loss: 0.5465614199638367\n",
      "Epoch: 1, Loss: 0.4755436182022095\n",
      "Epoch: 1, Loss: 0.44127970933914185\n",
      "Epoch: 1, Loss: 0.5037959218025208\n",
      "Epoch: 1, Loss: 0.5394267439842224\n",
      "Epoch: 1, Loss: 0.385503351688385\n",
      "Epoch: 1, Loss: 0.4064168334007263\n",
      "Epoch: 1, Loss: 0.5276926755905151\n",
      "Epoch: 1, Loss: 0.4944494962692261\n",
      "Epoch: 1, Loss: 0.5602821111679077\n",
      "Epoch: 1, Loss: 0.45179933309555054\n",
      "Epoch: 1, Loss: 0.4968339800834656\n",
      "Epoch: 1, Loss: 0.4699915051460266\n",
      "Epoch: 1, Loss: 0.41478586196899414\n",
      "Epoch: 1, Loss: 0.46055838465690613\n",
      "Epoch: 1, Loss: 0.3956887722015381\n",
      "Epoch: 1, Loss: 0.44171828031539917\n",
      "Epoch: 1, Loss: 0.5048472285270691\n",
      "Epoch: 1, Loss: 0.6804314255714417\n",
      "Epoch: 1, Loss: 0.43804243206977844\n",
      "Epoch: 1, Loss: 0.4813838005065918\n",
      "Epoch: 1, Loss: 0.40133410692214966\n",
      "Epoch: 1, Loss: 0.438135027885437\n",
      "Epoch: 1, Loss: 0.45150768756866455\n",
      "Epoch: 1, Loss: 0.5069097876548767\n",
      "Epoch: 1, Loss: 0.41799139976501465\n",
      "Epoch: 1, Loss: 0.390627920627594\n",
      "Epoch: 1, Loss: 0.48001426458358765\n",
      "Epoch: 1, Loss: 0.43158185482025146\n",
      "Epoch: 1, Loss: 0.5302799344062805\n",
      "Epoch: 1, Loss: 0.4332805871963501\n",
      "Epoch: 1, Loss: 0.5728517770767212\n",
      "Epoch: 1, Loss: 0.5013605952262878\n",
      "Epoch: 1, Loss: 0.4582172632217407\n",
      "Epoch: 1, Loss: 0.5048842430114746\n",
      "Epoch: 1, Loss: 0.5325891375541687\n",
      "Epoch: 1, Loss: 0.44827261567115784\n",
      "Epoch: 1, Loss: 0.49019333720207214\n",
      "Epoch: 1, Loss: 0.5063530206680298\n",
      "Epoch: 1, Loss: 0.4473564028739929\n",
      "Epoch: 1, Loss: 0.5875850319862366\n",
      "Epoch: 1, Loss: 0.41823023557662964\n",
      "Epoch: 1, Loss: 0.6084578037261963\n",
      "Epoch: 1, Loss: 0.5522567629814148\n",
      "Epoch: 1, Loss: 0.4259844720363617\n",
      "Epoch: 1, Loss: 0.6087996959686279\n",
      "Epoch: 1, Loss: 0.518965482711792\n",
      "Epoch: 1, Loss: 0.713346540927887\n",
      "Epoch: 1, Loss: 0.5109826922416687\n",
      "Epoch: 1, Loss: 0.6184357404708862\n",
      "Epoch: 1, Loss: 0.5549678802490234\n",
      "Epoch: 1, Loss: 0.46010059118270874\n",
      "Epoch: 1, Loss: 0.49878260493278503\n",
      "Epoch: 1, Loss: 0.5099939107894897\n",
      "Epoch: 1, Loss: 0.3788152039051056\n",
      "Epoch: 1, Loss: 0.4746951460838318\n",
      "Epoch: 1, Loss: 0.45302534103393555\n",
      "Epoch: 1, Loss: 0.46570485830307007\n",
      "Epoch: 1, Loss: 0.41157907247543335\n",
      "Epoch: 1, Loss: 0.461825430393219\n",
      "Epoch: 1, Loss: 0.5297043323516846\n",
      "Epoch: 1, Loss: 0.4751747250556946\n",
      "Epoch: 1, Loss: 0.5763399004936218\n",
      "Epoch: 1, Loss: 0.5522589683532715\n",
      "Epoch: 1, Loss: 0.45528078079223633\n",
      "Epoch: 1, Loss: 0.4868358373641968\n",
      "Epoch: 1, Loss: 0.5081871151924133\n",
      "Epoch: 1, Loss: 0.47980472445487976\n",
      "Epoch: 1, Loss: 0.4877268075942993\n",
      "Epoch: 1, Loss: 0.5429209470748901\n",
      "Epoch: 1, Loss: 0.392110139131546\n",
      "Epoch: 1, Loss: 0.47308632731437683\n",
      "Epoch: 1, Loss: 0.5117761492729187\n",
      "Epoch: 1, Loss: 0.4465399980545044\n",
      "Epoch: 1, Loss: 0.388430118560791\n",
      "Epoch: 1, Loss: 0.5298359990119934\n",
      "Epoch: 1, Loss: 0.4594680368900299\n",
      "Epoch: 1, Loss: 0.4756563901901245\n",
      "Epoch: 1, Loss: 0.4863567054271698\n",
      "Epoch: 1, Loss: 0.5202715992927551\n",
      "Epoch: 1, Loss: 0.4968997538089752\n",
      "Epoch: 1, Loss: 0.42536672949790955\n",
      "Epoch: 1, Loss: 0.38918066024780273\n",
      "Epoch: 1, Loss: 0.5436802506446838\n",
      "Epoch: 1, Loss: 0.6043915152549744\n",
      "Epoch: 1, Loss: 0.5490312576293945\n",
      "Epoch: 1, Loss: 0.5510460734367371\n",
      "Epoch: 1, Loss: 0.8383420705795288\n",
      "Epoch: 1, Loss: 0.4604158103466034\n",
      "Epoch: 1, Loss: 0.6218652725219727\n",
      "Epoch: 1, Loss: 0.44939154386520386\n",
      "Epoch: 1, Loss: 0.5288984179496765\n",
      "Epoch: 1, Loss: 0.6458393335342407\n",
      "Epoch: 1, Loss: 0.44680795073509216\n",
      "Epoch: 1, Loss: 0.4325447380542755\n",
      "Epoch: 1, Loss: 0.5827043056488037\n",
      "Epoch: 1, Loss: 0.44812050461769104\n",
      "Epoch: 1, Loss: 0.4241037666797638\n",
      "Epoch: 1, Loss: 0.45483067631721497\n",
      "Epoch: 1, Loss: 0.5412964224815369\n",
      "Epoch: 1, Loss: 0.5256161689758301\n",
      "Epoch: 1, Loss: 0.4844714105129242\n",
      "Epoch: 1, Loss: 0.5886260271072388\n",
      "Epoch: 1, Loss: 0.5325719118118286\n",
      "Epoch: 1, Loss: 0.5049247145652771\n",
      "Epoch: 1, Loss: 0.6613686680793762\n",
      "Epoch: 1, Loss: 0.48253095149993896\n",
      "Epoch: 1, Loss: 0.3863116502761841\n",
      "Epoch: 1, Loss: 0.4966124892234802\n",
      "Epoch: 1, Loss: 0.5716021060943604\n",
      "Epoch: 1, Loss: 0.4440566897392273\n",
      "Epoch: 1, Loss: 0.5427478551864624\n",
      "Epoch: 1, Loss: 0.3883569836616516\n",
      "Epoch: 1, Loss: 0.4870011806488037\n",
      "Epoch: 1, Loss: 0.49848026037216187\n",
      "Epoch: 1, Loss: 0.4239879250526428\n",
      "Epoch: 1, Loss: 0.5279966592788696\n",
      "Epoch: 1, Loss: 0.4829765856266022\n",
      "Epoch: 1, Loss: 0.41075262427330017\n",
      "Epoch: 1, Loss: 0.5540379285812378\n",
      "Epoch: 1, Loss: 0.5064586997032166\n",
      "Epoch: 1, Loss: 0.491678386926651\n",
      "Epoch: 1, Loss: 0.5096056461334229\n",
      "Epoch: 1, Loss: 0.5421839952468872\n",
      "Epoch: 1, Loss: 0.575844407081604\n",
      "Epoch: 1, Loss: 0.4739432632923126\n",
      "Epoch: 1, Loss: 0.510412871837616\n",
      "Epoch: 1, Loss: 0.45443958044052124\n",
      "Epoch: 1, Loss: 0.48997294902801514\n",
      "Epoch: 1, Loss: 0.4603245258331299\n",
      "Epoch: 1, Loss: 0.5129621624946594\n",
      "Epoch: 1, Loss: 0.5593951344490051\n",
      "Epoch: 1, Loss: 0.5468168258666992\n",
      "Epoch: 1, Loss: 0.42129838466644287\n",
      "Epoch: 1, Loss: 0.5409756898880005\n",
      "Epoch: 1, Loss: 0.4196649193763733\n",
      "Epoch: 1, Loss: 0.4453997015953064\n",
      "Epoch: 1, Loss: 0.5282811522483826\n",
      "Epoch: 1, Loss: 0.4246595501899719\n",
      "Epoch: 1, Loss: 0.4686158001422882\n",
      "Epoch: 1, Loss: 0.4611521363258362\n",
      "Epoch: 1, Loss: 0.4533608555793762\n",
      "Epoch: 1, Loss: 0.5462770462036133\n",
      "Epoch: 1, Loss: 0.5348265171051025\n",
      "Epoch: 1, Loss: 0.5322690010070801\n",
      "Epoch: 1, Loss: 0.4651332199573517\n",
      "Epoch: 1, Loss: 0.4839664697647095\n",
      "Epoch: 1, Loss: 0.4845806956291199\n",
      "Epoch: 1, Loss: 0.47686371207237244\n",
      "Epoch: 1, Loss: 0.5522875785827637\n",
      "Epoch: 1, Loss: 0.5028225183486938\n",
      "Epoch: 1, Loss: 0.593277096748352\n",
      "Epoch: 1, Loss: 0.5835594534873962\n",
      "Epoch: 1, Loss: 0.49933093786239624\n",
      "Epoch: 1, Loss: 0.4066779911518097\n",
      "Epoch: 1, Loss: 0.4141201674938202\n",
      "Epoch: 1, Loss: 0.5642395615577698\n",
      "Epoch: 1, Loss: 0.49857640266418457\n",
      "Epoch: 1, Loss: 0.43881016969680786\n",
      "Epoch: 1, Loss: 0.4110839366912842\n",
      "Epoch: 1, Loss: 0.45508909225463867\n",
      "Epoch: 1, Loss: 0.4340575933456421\n",
      "Epoch: 1, Loss: 0.4572100043296814\n",
      "Epoch: 1, Loss: 0.48083165287971497\n",
      "Epoch: 1, Loss: 0.4536357820034027\n",
      "Epoch: 1, Loss: 0.42559751868247986\n",
      "Epoch: 1, Loss: 0.463858962059021\n",
      "Epoch: 1, Loss: 0.5011657476425171\n",
      "Epoch: 1, Loss: 0.43360012769699097\n",
      "Epoch: 1, Loss: 0.4339922070503235\n",
      "Epoch: 1, Loss: 0.6456174850463867\n",
      "Epoch: 1, Loss: 0.521602988243103\n",
      "Epoch: 1, Loss: 0.5342337489128113\n",
      "Epoch: 1, Loss: 0.4469715356826782\n",
      "Epoch: 1, Loss: 0.40124747157096863\n",
      "Epoch: 1, Loss: 0.4213433265686035\n",
      "Epoch: 1, Loss: 0.5006757974624634\n",
      "Epoch: 1, Loss: 0.5372624397277832\n",
      "Epoch: 1, Loss: 0.48696956038475037\n",
      "Epoch: 1, Loss: 0.38980501890182495\n",
      "Epoch: 1, Loss: 0.5030485987663269\n",
      "Epoch: 1, Loss: 0.4735179543495178\n",
      "Epoch: 1, Loss: 0.41193887591362\n",
      "Epoch: 1, Loss: 0.5264222025871277\n",
      "Epoch: 1, Loss: 0.5591875910758972\n",
      "Epoch: 1, Loss: 0.3818494379520416\n",
      "Epoch: 1, Loss: 0.48652517795562744\n",
      "Epoch: 1, Loss: 0.407884806394577\n",
      "Epoch: 1, Loss: 0.4184182286262512\n",
      "Epoch: 1, Loss: 0.4563341736793518\n",
      "Epoch: 1, Loss: 0.5187265276908875\n",
      "Epoch: 1, Loss: 0.4172218441963196\n",
      "Epoch: 1, Loss: 0.41679227352142334\n",
      "Epoch: 1, Loss: 0.6016945838928223\n",
      "Epoch: 1, Loss: 0.5241309404373169\n",
      "Epoch: 1, Loss: 0.6031378507614136\n",
      "Epoch: 1, Loss: 0.5753259658813477\n",
      "Epoch: 1, Loss: 0.45558691024780273\n",
      "Epoch: 1, Loss: 0.49833551049232483\n",
      "Epoch: 1, Loss: 0.4243446886539459\n",
      "Epoch: 1, Loss: 0.48465031385421753\n",
      "Epoch: 1, Loss: 0.43538209795951843\n",
      "Epoch: 1, Loss: 0.46827173233032227\n",
      "Epoch: 1, Loss: 0.5777654647827148\n",
      "Epoch: 1, Loss: 0.47910821437835693\n",
      "Epoch: 1, Loss: 0.4019967019557953\n",
      "Epoch: 1, Loss: 0.407774418592453\n",
      "Epoch: 1, Loss: 0.4760568141937256\n",
      "Epoch: 1, Loss: 0.4847140610218048\n",
      "Epoch: 1, Loss: 0.5647146701812744\n",
      "Epoch: 1, Loss: 0.5143346190452576\n",
      "Epoch: 1, Loss: 0.47607770562171936\n",
      "Epoch: 1, Loss: 0.506950318813324\n",
      "Epoch: 1, Loss: 0.44127318263053894\n",
      "Epoch: 1, Loss: 0.548668622970581\n",
      "Epoch: 1, Loss: 0.4243696928024292\n",
      "Epoch: 1, Loss: 0.5531972050666809\n",
      "Epoch: 1, Loss: 0.4503524601459503\n",
      "Epoch: 1, Loss: 0.4406570792198181\n",
      "Epoch: 1, Loss: 0.5685746669769287\n",
      "Epoch: 1, Loss: 0.4726242423057556\n",
      "Epoch: 1, Loss: 0.43616846203804016\n",
      "Epoch: 1, Loss: 0.5744667649269104\n",
      "Epoch: 1, Loss: 0.4522976577281952\n",
      "Epoch: 1, Loss: 0.5878183245658875\n",
      "Epoch: 1, Loss: 0.3829230070114136\n",
      "Epoch: 1, Loss: 0.4577459692955017\n",
      "Epoch: 1, Loss: 0.49356338381767273\n",
      "Epoch: 1, Loss: 0.4688827097415924\n",
      "Epoch: 1, Loss: 0.5174323320388794\n",
      "Epoch: 1, Loss: 0.36786410212516785\n",
      "Epoch: 1, Loss: 0.5998113751411438\n",
      "Epoch: 1, Loss: 0.4909537434577942\n",
      "Epoch: 1, Loss: 0.456294983625412\n",
      "Epoch: 1, Loss: 0.5566932559013367\n",
      "Epoch: 1, Loss: 0.5606994032859802\n",
      "Epoch: 1, Loss: 0.41806405782699585\n",
      "Epoch: 1, Loss: 0.46678626537323\n",
      "Epoch: 1, Loss: 0.5759093761444092\n",
      "Epoch: 1, Loss: 0.5007124543190002\n",
      "Epoch: 1, Loss: 0.4298859238624573\n",
      "Epoch: 1, Loss: 0.49351081252098083\n",
      "Epoch: 1, Loss: 0.5532771348953247\n",
      "Epoch: 1, Loss: 0.4114478826522827\n",
      "Epoch: 1, Loss: 0.5100510120391846\n",
      "Epoch: 1, Loss: 0.5186405181884766\n",
      "Epoch: 1, Loss: 0.5012471079826355\n",
      "Epoch: 1, Loss: 0.47925427556037903\n",
      "Epoch: 1, Loss: 0.49838611483573914\n",
      "Epoch: 1, Loss: 0.44634419679641724\n",
      "Epoch: 1, Loss: 0.5001716613769531\n",
      "Epoch: 1, Loss: 0.48469388484954834\n",
      "Epoch: 1, Loss: 0.5776752829551697\n",
      "Epoch: 1, Loss: 0.512700617313385\n",
      "Epoch: 1, Loss: 0.6246845722198486\n",
      "Epoch: 1, Loss: 0.5974383354187012\n",
      "Epoch: 1, Loss: 0.5525325536727905\n",
      "Epoch: 1, Loss: 0.48320603370666504\n",
      "Epoch: 1, Loss: 0.4756563603878021\n",
      "Epoch: 1, Loss: 0.4826798141002655\n",
      "Epoch: 1, Loss: 0.502255380153656\n",
      "Epoch: 1, Loss: 0.40185409784317017\n",
      "Epoch: 1, Loss: 0.46864092350006104\n",
      "Epoch: 1, Loss: 0.4568317234516144\n",
      "Epoch: 1, Loss: 0.4570631980895996\n",
      "Epoch: 1, Loss: 0.5417743921279907\n",
      "Epoch: 1, Loss: 0.41159719228744507\n",
      "Epoch: 1, Loss: 0.40294981002807617\n",
      "Epoch: 1, Loss: 0.5555534958839417\n",
      "Epoch: 1, Loss: 0.4234151244163513\n",
      "Epoch: 1, Loss: 0.5619721412658691\n",
      "Epoch: 1, Loss: 0.5123203992843628\n",
      "Epoch: 1, Loss: 0.4832717478275299\n",
      "Epoch: 1, Loss: 0.5221843719482422\n",
      "Epoch: 1, Loss: 0.5670543313026428\n",
      "Epoch: 1, Loss: 0.4709816575050354\n",
      "Epoch: 1, Loss: 0.4074728488922119\n",
      "Epoch: 1, Loss: 0.4525700807571411\n",
      "Epoch: 1, Loss: 0.4706770181655884\n",
      "Epoch: 1, Loss: 0.5707066059112549\n",
      "Epoch: 1, Loss: 0.5055860280990601\n",
      "Epoch: 1, Loss: 0.6536298990249634\n",
      "Epoch: 1, Loss: 0.42603421211242676\n",
      "Epoch: 1, Loss: 0.5024452209472656\n",
      "Epoch: 1, Loss: 0.4270595908164978\n",
      "Epoch: 1, Loss: 0.4757940173149109\n",
      "Epoch: 1, Loss: 0.46041303873062134\n",
      "Epoch: 1, Loss: 0.49226540327072144\n",
      "Epoch: 1, Loss: 0.4768354594707489\n",
      "Epoch: 1, Loss: 0.5165748000144958\n",
      "Epoch: 1, Loss: 0.5639402866363525\n",
      "Epoch: 1, Loss: 0.4559553265571594\n",
      "Epoch: 1, Loss: 0.45200493931770325\n",
      "Epoch: 1, Loss: 0.403039813041687\n",
      "Epoch: 1, Loss: 0.4179396629333496\n",
      "Epoch: 1, Loss: 0.47681254148483276\n",
      "Epoch: 1, Loss: 0.5282694101333618\n",
      "Epoch: 1, Loss: 0.5657655000686646\n",
      "Epoch: 1, Loss: 0.6189844012260437\n",
      "Epoch: 1, Loss: 0.4290297031402588\n",
      "Epoch: 1, Loss: 0.42804890871047974\n",
      "Epoch: 1, Loss: 0.498946875333786\n",
      "Epoch: 1, Loss: 0.4951927661895752\n",
      "Epoch: 1, Loss: 0.39445507526397705\n",
      "Epoch: 1, Loss: 0.47609561681747437\n",
      "Epoch: 1, Loss: 0.5103713870048523\n",
      "Epoch: 1, Loss: 0.3758717477321625\n",
      "Epoch: 1, Loss: 0.43740391731262207\n",
      "Epoch: 1, Loss: 0.5435741543769836\n",
      "Epoch: 1, Loss: 0.5830878615379333\n",
      "Epoch: 1, Loss: 0.4758855700492859\n",
      "Epoch: 1, Loss: 0.48178043961524963\n",
      "Epoch: 1, Loss: 0.52988600730896\n",
      "Epoch: 1, Loss: 0.5260109901428223\n",
      "Epoch: 1, Loss: 0.5283247828483582\n",
      "Epoch: 1, Loss: 0.42593032121658325\n",
      "Epoch: 1, Loss: 0.4130094349384308\n",
      "Epoch: 1, Loss: 0.6532126069068909\n",
      "Epoch: 1, Loss: 0.4230973720550537\n",
      "Epoch: 1, Loss: 0.4389919936656952\n",
      "Epoch: 1, Loss: 0.5111998915672302\n",
      "Epoch: 1, Loss: 0.4368060231208801\n",
      "Epoch: 1, Loss: 0.5975318551063538\n",
      "Epoch: 1, Loss: 0.5047889947891235\n",
      "Epoch: 1, Loss: 0.49106115102767944\n",
      "Epoch: 1, Loss: 0.37696602940559387\n",
      "Epoch: 1, Loss: 0.5793861150741577\n",
      "Epoch: 1, Loss: 0.47702980041503906\n",
      "Epoch: 1, Loss: 0.379152774810791\n",
      "Epoch: 1, Loss: 0.4875563085079193\n",
      "Epoch: 1, Loss: 0.5886409282684326\n",
      "Epoch: 1, Loss: 0.5075476169586182\n",
      "Epoch: 1, Loss: 0.5678344368934631\n",
      "Epoch: 1, Loss: 0.5245482325553894\n",
      "Epoch: 1, Loss: 0.43842813372612\n",
      "Epoch: 1, Loss: 0.4817022383213043\n",
      "Epoch: 1, Loss: 0.430314838886261\n",
      "Epoch: 1, Loss: 0.49469590187072754\n",
      "Epoch: 1, Loss: 0.6067588329315186\n",
      "Epoch: 1, Loss: 0.4525899589061737\n",
      "Epoch: 1, Loss: 0.5045603513717651\n",
      "Epoch: 1, Loss: 0.40515661239624023\n",
      "Epoch: 1, Loss: 0.48504990339279175\n",
      "Epoch: 1, Loss: 0.49344295263290405\n",
      "Epoch: 1, Loss: 0.5166595578193665\n",
      "Epoch: 1, Loss: 0.5712524652481079\n",
      "Epoch: 1, Loss: 0.49228018522262573\n",
      "Epoch: 1, Loss: 0.43005141615867615\n",
      "Epoch: 1, Loss: 0.5030701160430908\n",
      "Epoch: 1, Loss: 0.46849489212036133\n",
      "Epoch: 1, Loss: 0.4446677565574646\n",
      "Epoch: 1, Loss: 0.4692462086677551\n",
      "Epoch: 1, Loss: 0.47463950514793396\n",
      "Epoch: 1, Loss: 0.4698057174682617\n",
      "Epoch: 1, Loss: 0.6320136189460754\n",
      "Epoch: 1, Loss: 0.559451699256897\n",
      "Epoch: 1, Loss: 0.5898903012275696\n",
      "Epoch: 1, Loss: 0.48787936568260193\n",
      "Epoch: 1, Loss: 0.547714114189148\n",
      "Epoch: 1, Loss: 0.6524370908737183\n",
      "Epoch: 1, Loss: 0.500826895236969\n",
      "Epoch: 1, Loss: 0.42808419466018677\n",
      "Epoch: 1, Loss: 0.43885213136672974\n",
      "Epoch: 1, Loss: 0.4436066448688507\n",
      "Epoch: 1, Loss: 0.39550548791885376\n",
      "Epoch: 1, Loss: 0.4224816560745239\n",
      "Epoch: 1, Loss: 0.5013411045074463\n",
      "Epoch: 1, Loss: 0.3923741281032562\n",
      "Epoch: 1, Loss: 0.4448472857475281\n",
      "Epoch: 1, Loss: 0.5165576338768005\n",
      "Epoch: 1, Loss: 0.45289885997772217\n",
      "Epoch: 1, Loss: 0.44763612747192383\n",
      "Epoch: 1, Loss: 0.49196314811706543\n",
      "Epoch: 1, Loss: 0.45695751905441284\n",
      "Epoch: 1, Loss: 0.47483354806900024\n",
      "Epoch: 1, Loss: 0.43096113204956055\n",
      "Epoch: 1, Loss: 0.5936052203178406\n",
      "Epoch: 1, Loss: 0.4021305739879608\n",
      "Epoch: 1, Loss: 0.5217854976654053\n",
      "Epoch: 1, Loss: 0.4899437129497528\n",
      "Epoch: 1, Loss: 0.46484923362731934\n",
      "Epoch: 1, Loss: 0.49000662565231323\n",
      "Epoch: 1, Loss: 0.5521615743637085\n",
      "Epoch: 1, Loss: 0.5146355628967285\n",
      "Epoch: 1, Loss: 0.5466911792755127\n",
      "Epoch: 1, Loss: 0.4667072296142578\n",
      "Epoch: 1, Loss: 0.45126962661743164\n",
      "Epoch: 1, Loss: 0.43849971890449524\n",
      "Epoch: 1, Loss: 0.40019476413726807\n",
      "Epoch: 1, Loss: 0.4719095826148987\n",
      "Epoch: 1, Loss: 0.5305581092834473\n",
      "Epoch: 1, Loss: 0.5158861875534058\n",
      "Epoch: 1, Loss: 0.6253795623779297\n",
      "Epoch: 1, Loss: 0.5131613612174988\n",
      "Epoch: 1, Loss: 0.5481006503105164\n",
      "Epoch: 1, Loss: 0.41921016573905945\n",
      "Epoch: 1, Loss: 0.5253622531890869\n",
      "Epoch: 1, Loss: 0.5709443092346191\n",
      "Epoch: 1, Loss: 0.4704169034957886\n",
      "Epoch: 1, Loss: 0.4419364035129547\n",
      "Epoch: 1, Loss: 0.5244407653808594\n",
      "Epoch: 1, Loss: 0.5254085063934326\n",
      "Epoch: 1, Loss: 0.44160398840904236\n",
      "Epoch: 1, Loss: 0.4807867109775543\n",
      "Epoch: 1, Loss: 0.547685444355011\n",
      "Epoch: 1, Loss: 0.3999715745449066\n",
      "Epoch: 1, Loss: 0.45452386140823364\n",
      "Epoch: 1, Loss: 0.5233663320541382\n",
      "Epoch: 1, Loss: 0.4464266896247864\n",
      "Epoch: 1, Loss: 0.5503592491149902\n",
      "Epoch: 1, Loss: 0.43000781536102295\n",
      "Epoch: 1, Loss: 0.4966205358505249\n",
      "Epoch: 1, Loss: 0.47424420714378357\n",
      "Epoch: 1, Loss: 0.4015377163887024\n",
      "Epoch: 1, Loss: 0.4867224097251892\n",
      "Epoch: 1, Loss: 0.5139768719673157\n",
      "Epoch: 1, Loss: 0.4955660402774811\n",
      "Epoch: 1, Loss: 0.44304803013801575\n",
      "Epoch: 1, Loss: 0.42894446849823\n",
      "Epoch: 1, Loss: 0.4873161315917969\n",
      "Epoch: 1, Loss: 0.48697566986083984\n",
      "Epoch: 1, Loss: 0.5058043599128723\n",
      "Epoch: 1, Loss: 0.504565417766571\n",
      "Epoch: 1, Loss: 0.47027742862701416\n",
      "Epoch: 1, Loss: 0.5651960372924805\n",
      "Epoch: 1, Loss: 0.44098132848739624\n",
      "Epoch: 1, Loss: 0.4096046984195709\n",
      "Epoch: 1, Loss: 0.49049660563468933\n",
      "Epoch: 1, Loss: 0.5064566731452942\n",
      "Epoch: 1, Loss: 0.4694070518016815\n",
      "Epoch: 1, Loss: 0.4092283844947815\n",
      "Epoch: 1, Loss: 0.4316866397857666\n",
      "Epoch: 1, Loss: 0.5290348529815674\n",
      "Epoch: 1, Loss: 0.4471438527107239\n",
      "Epoch: 1, Loss: 0.5011114478111267\n",
      "Epoch: 1, Loss: 0.5306962132453918\n",
      "Epoch: 1, Loss: 0.6611543297767639\n",
      "Epoch: 1, Loss: 0.5870924592018127\n",
      "Epoch: 1, Loss: 0.4023049473762512\n",
      "Epoch: 1, Loss: 0.517515242099762\n",
      "Epoch: 1, Loss: 0.5038134455680847\n",
      "Epoch: 1, Loss: 0.487412691116333\n",
      "Epoch: 1, Loss: 0.490913987159729\n",
      "Epoch: 1, Loss: 0.5979183912277222\n",
      "Epoch: 1, Loss: 0.40452590584754944\n",
      "Epoch: 1, Loss: 0.48651570081710815\n",
      "Epoch: 1, Loss: 0.49215614795684814\n",
      "Epoch: 1, Loss: 0.47887811064720154\n",
      "Epoch: 1, Loss: 0.43543216586112976\n",
      "Epoch: 1, Loss: 0.37328314781188965\n",
      "Epoch: 1, Loss: 0.4203558564186096\n",
      "Epoch: 1, Loss: 0.49490347504615784\n",
      "Epoch: 1, Loss: 0.46200138330459595\n",
      "Epoch: 1, Loss: 0.49988728761672974\n",
      "Epoch: 1, Loss: 0.542794942855835\n",
      "Epoch: 1, Loss: 0.44390586018562317\n",
      "Epoch: 1, Loss: 0.49572354555130005\n",
      "Epoch: 1, Loss: 0.4961181581020355\n",
      "Epoch: 1, Loss: 0.599623441696167\n",
      "Epoch: 1, Loss: 0.49906378984451294\n",
      "Epoch: 1, Loss: 0.40862298011779785\n",
      "Epoch: 1, Loss: 0.5565052032470703\n",
      "Epoch: 1, Loss: 0.5227439403533936\n",
      "Epoch: 1, Loss: 0.5138463973999023\n",
      "Epoch: 1, Loss: 0.47878795862197876\n",
      "Epoch: 1, Loss: 0.49129584431648254\n",
      "Epoch: 1, Loss: 0.46193069219589233\n",
      "Epoch: 1, Loss: 0.4944378733634949\n",
      "Epoch: 1, Loss: 0.44675955176353455\n",
      "Epoch: 1, Loss: 0.40964987874031067\n",
      "Epoch: 1, Loss: 0.4436773955821991\n",
      "Epoch: 1, Loss: 0.5227822065353394\n",
      "Epoch: 1, Loss: 0.5565502643585205\n",
      "Epoch: 1, Loss: 0.37589243054389954\n",
      "Epoch: 1, Loss: 0.47749093174934387\n",
      "Epoch: 1, Loss: 0.4748349189758301\n",
      "Epoch: 1, Loss: 0.5524359345436096\n",
      "Epoch: 1, Loss: 0.585005521774292\n",
      "Epoch: 1, Loss: 0.4276830554008484\n",
      "Epoch: 1, Loss: 0.5327950119972229\n",
      "Epoch: 1, Loss: 0.5333441495895386\n",
      "Epoch: 1, Loss: 0.4096781015396118\n",
      "Epoch: 1, Loss: 0.4668874740600586\n",
      "Epoch: 1, Loss: 0.43932878971099854\n",
      "Epoch: 1, Loss: 0.4343169331550598\n",
      "Epoch: 1, Loss: 0.4807383418083191\n",
      "Epoch: 1, Loss: 0.5369513630867004\n",
      "Epoch: 1, Loss: 0.57831871509552\n",
      "Epoch: 1, Loss: 0.41533994674682617\n",
      "Epoch: 1, Loss: 0.5507214069366455\n",
      "Epoch: 1, Loss: 0.6107187271118164\n",
      "Epoch: 1, Loss: 0.4587460458278656\n",
      "Epoch: 1, Loss: 0.42926937341690063\n",
      "Epoch: 1, Loss: 0.43818700313568115\n",
      "Epoch: 1, Loss: 0.4264724850654602\n",
      "Epoch: 1, Loss: 0.5304921269416809\n",
      "Epoch: 1, Loss: 0.6048161387443542\n",
      "Epoch: 1, Loss: 0.5672335028648376\n",
      "Epoch: 1, Loss: 0.3907029628753662\n",
      "Epoch: 1, Loss: 0.48337188363075256\n",
      "Epoch: 1, Loss: 0.4567704200744629\n",
      "Epoch: 1, Loss: 0.43564313650131226\n",
      "Epoch: 1, Loss: 0.43678945302963257\n",
      "Epoch: 1, Loss: 0.46158018708229065\n",
      "Epoch: 1, Loss: 0.5141546726226807\n",
      "Epoch: 1, Loss: 0.4440261721611023\n",
      "Epoch: 1, Loss: 0.4795396029949188\n",
      "Epoch: 1, Loss: 0.39682745933532715\n",
      "Epoch: 1, Loss: 0.43963944911956787\n",
      "Epoch: 1, Loss: 0.522479772567749\n",
      "Epoch: 1, Loss: 0.5244321227073669\n",
      "Epoch: 1, Loss: 0.40390270948410034\n",
      "Epoch: 1, Loss: 0.4730052053928375\n",
      "Epoch: 1, Loss: 0.5428727269172668\n",
      "Epoch: 1, Loss: 0.4850975275039673\n",
      "Epoch: 1, Loss: 0.47533461451530457\n",
      "Epoch: 1, Loss: 0.440104603767395\n",
      "Epoch: 1, Loss: 0.5418839454650879\n",
      "Epoch: 1, Loss: 0.37885239720344543\n",
      "Epoch: 1, Loss: 0.4111776351928711\n",
      "Epoch: 1, Loss: 0.45290714502334595\n",
      "Epoch: 1, Loss: 0.5223323106765747\n",
      "Epoch: 1, Loss: 0.5013136267662048\n",
      "Epoch: 1, Loss: 0.4882444143295288\n",
      "Epoch: 1, Loss: 0.6323703527450562\n",
      "Epoch: 1, Loss: 0.42612743377685547\n",
      "Epoch: 1, Loss: 0.42163142561912537\n",
      "Epoch: 1, Loss: 0.5518701076507568\n",
      "Epoch: 1, Loss: 0.5564814805984497\n",
      "Epoch: 1, Loss: 0.42098933458328247\n",
      "Epoch: 1, Loss: 0.6342015862464905\n",
      "Epoch: 1, Loss: 0.42504197359085083\n",
      "Epoch: 1, Loss: 0.3998439908027649\n",
      "Epoch: 1, Loss: 0.36929839849472046\n",
      "Epoch: 1, Loss: 0.5511388182640076\n",
      "Epoch: 1, Loss: 0.5298295617103577\n",
      "Epoch: 1, Loss: 0.6280158758163452\n",
      "Epoch: 1, Loss: 0.5394985675811768\n",
      "Epoch: 1, Loss: 0.5618079304695129\n",
      "Epoch: 1, Loss: 0.5099817514419556\n",
      "Epoch: 1, Loss: 0.5966599583625793\n",
      "Epoch: 1, Loss: 0.47786086797714233\n",
      "Epoch: 1, Loss: 0.5511348843574524\n",
      "Epoch: 1, Loss: 0.4971556067466736\n",
      "Epoch: 1, Loss: 0.5988953709602356\n",
      "Epoch: 1, Loss: 0.46934616565704346\n",
      "Epoch: 1, Loss: 0.46892809867858887\n",
      "Epoch: 1, Loss: 0.5712562799453735\n",
      "Epoch: 1, Loss: 0.4660950005054474\n",
      "Epoch: 1, Loss: 0.4942307770252228\n",
      "Epoch: 1, Loss: 0.42054247856140137\n",
      "Epoch: 1, Loss: 0.7176932692527771\n",
      "Epoch: 1, Loss: 0.44794997572898865\n",
      "Epoch: 1, Loss: 0.45194506645202637\n",
      "Epoch: 1, Loss: 0.5274759531021118\n",
      "Epoch: 1, Loss: 0.5009982585906982\n",
      "Epoch: 1, Loss: 0.3989829421043396\n",
      "Epoch: 1, Loss: 0.4355161786079407\n",
      "Epoch: 1, Loss: 0.5502982139587402\n",
      "Epoch: 1, Loss: 0.5427329540252686\n",
      "Epoch: 1, Loss: 0.4391656219959259\n",
      "Epoch: 1, Loss: 0.4139091670513153\n",
      "Epoch: 1, Loss: 0.5187335014343262\n",
      "Epoch: 1, Loss: 0.5966815948486328\n",
      "Epoch: 1, Loss: 0.4445631206035614\n",
      "Epoch: 1, Loss: 0.43394148349761963\n",
      "Epoch: 1, Loss: 0.423831045627594\n",
      "Epoch: 1, Loss: 0.5624449849128723\n",
      "Epoch: 1, Loss: 0.42907410860061646\n",
      "Epoch: 1, Loss: 0.46705907583236694\n",
      "Epoch: 1, Loss: 0.5274788737297058\n",
      "Epoch: 1, Loss: 0.5648936033248901\n",
      "Epoch: 1, Loss: 0.5924171209335327\n",
      "Epoch: 1, Loss: 0.43073269724845886\n",
      "Epoch: 1, Loss: 0.5039509534835815\n",
      "Epoch: 1, Loss: 0.5100213885307312\n",
      "Epoch: 1, Loss: 0.457975834608078\n",
      "Epoch: 1, Loss: 0.632722795009613\n",
      "Epoch: 1, Loss: 0.5079443454742432\n",
      "Epoch: 1, Loss: 0.4733023941516876\n",
      "Epoch: 1, Loss: 0.5392369031906128\n",
      "Epoch: 1, Loss: 0.4442636966705322\n",
      "Epoch: 1, Loss: 0.5884355306625366\n",
      "Epoch: 1, Loss: 0.5928875207901001\n",
      "Epoch: 1, Loss: 0.41095277667045593\n",
      "Epoch: 1, Loss: 0.5135319232940674\n",
      "Epoch: 1, Loss: 0.5041341185569763\n",
      "Epoch: 1, Loss: 0.5192980766296387\n",
      "Epoch: 1, Loss: 0.5801801681518555\n",
      "Epoch: 1, Loss: 0.4706871807575226\n",
      "Epoch: 1, Loss: 0.5946338176727295\n",
      "Epoch: 1, Loss: 0.4987955689430237\n",
      "Epoch: 1, Loss: 0.49534645676612854\n",
      "Epoch: 1, Loss: 0.4906701147556305\n",
      "Epoch: 1, Loss: 0.42672809958457947\n",
      "Epoch: 1, Loss: 0.6244470477104187\n",
      "Epoch: 1, Loss: 0.5110600590705872\n",
      "Epoch: 1, Loss: 0.45135796070098877\n",
      "Epoch: 1, Loss: 0.46483129262924194\n",
      "Epoch: 1, Loss: 0.5059630870819092\n",
      "Epoch: 1, Loss: 0.4338189959526062\n",
      "Epoch: 1, Loss: 0.5317269563674927\n",
      "Epoch: 1, Loss: 0.42017149925231934\n",
      "Epoch: 1, Loss: 0.4888696074485779\n",
      "Epoch: 1, Loss: 0.439909428358078\n",
      "Epoch: 1, Loss: 0.5726539492607117\n",
      "Epoch: 1, Loss: 0.4728632867336273\n",
      "Epoch: 1, Loss: 0.6148509383201599\n",
      "Epoch: 1, Loss: 0.468267023563385\n",
      "Epoch: 1, Loss: 0.49920904636383057\n",
      "Epoch: 1, Loss: 0.51170414686203\n",
      "Epoch: 1, Loss: 0.567211389541626\n",
      "Epoch: 1, Loss: 0.552213191986084\n",
      "Epoch: 1, Loss: 0.5895485877990723\n",
      "Epoch: 1, Loss: 0.464766263961792\n",
      "Epoch: 1, Loss: 0.45104649662971497\n",
      "Epoch: 1, Loss: 0.5055374503135681\n",
      "Epoch: 1, Loss: 0.5333427786827087\n",
      "Epoch: 1, Loss: 0.44571438431739807\n",
      "Epoch: 1, Loss: 0.49991774559020996\n",
      "Epoch: 1, Loss: 0.47808074951171875\n",
      "Epoch: 1, Loss: 0.4594317674636841\n",
      "Epoch: 1, Loss: 0.5649229288101196\n",
      "Epoch: 1, Loss: 0.5641429424285889\n",
      "Epoch: 1, Loss: 0.5658127665519714\n",
      "Epoch: 1, Loss: 0.4832192063331604\n",
      "Epoch: 1, Loss: 0.44612592458724976\n",
      "Epoch: 1, Loss: 0.4326574206352234\n",
      "Epoch: 1, Loss: 0.4177546203136444\n",
      "Epoch: 1, Loss: 0.4198960065841675\n",
      "Epoch: 1, Loss: 0.4946262836456299\n",
      "Epoch: 1, Loss: 0.4999416768550873\n",
      "Epoch: 1, Loss: 0.5261471271514893\n",
      "Epoch: 1, Loss: 0.5012211203575134\n",
      "Epoch: 1, Loss: 0.41218966245651245\n",
      "Epoch: 1, Loss: 0.47801822423934937\n",
      "Epoch: 1, Loss: 0.4649750292301178\n",
      "Epoch: 1, Loss: 0.46870988607406616\n",
      "Epoch: 1, Loss: 0.4372982680797577\n",
      "Epoch: 1, Loss: 0.5830400586128235\n",
      "Epoch: 1, Loss: 0.5704630017280579\n",
      "Epoch: 1, Loss: 0.4954734444618225\n",
      "Epoch: 1, Loss: 0.5107967257499695\n",
      "Epoch: 1, Loss: 0.45190125703811646\n",
      "Epoch: 1, Loss: 0.5524743795394897\n",
      "Epoch: 1, Loss: 0.4195626974105835\n",
      "Epoch: 1, Loss: 0.43456703424453735\n",
      "Epoch: 1, Loss: 0.4971769452095032\n",
      "Epoch: 1, Loss: 0.5292097926139832\n",
      "Epoch: 1, Loss: 0.3905567228794098\n",
      "Epoch: 1, Loss: 0.4426857531070709\n",
      "Epoch: 1, Loss: 0.41015148162841797\n",
      "Epoch: 1, Loss: 0.47155630588531494\n",
      "Epoch: 1, Loss: 0.42020583152770996\n",
      "Epoch: 1, Loss: 0.5413663387298584\n",
      "Epoch: 1, Loss: 0.4879840612411499\n",
      "Epoch: 1, Loss: 0.4408385455608368\n",
      "Epoch: 1, Loss: 0.6503989100456238\n",
      "Epoch: 1, Loss: 0.5490951538085938\n",
      "Epoch: 1, Loss: 0.4867796301841736\n",
      "Epoch: 1, Loss: 0.47520536184310913\n",
      "Epoch: 1, Loss: 0.5154144763946533\n",
      "Epoch: 1, Loss: 0.4959818720817566\n",
      "Epoch: 1, Loss: 0.5385392308235168\n",
      "Epoch: 1, Loss: 0.502092719078064\n",
      "Epoch: 1, Loss: 0.5143266916275024\n",
      "Epoch: 1, Loss: 0.5149760246276855\n",
      "Epoch: 1, Loss: 0.4132107198238373\n",
      "Epoch: 1, Loss: 0.38174954056739807\n",
      "Epoch: 1, Loss: 0.5816845893859863\n",
      "Epoch: 1, Loss: 0.42163074016571045\n",
      "Epoch: 1, Loss: 0.36851978302001953\n",
      "Epoch: 1, Loss: 0.4411929249763489\n",
      "Epoch: 1, Loss: 0.46078750491142273\n",
      "Epoch: 1, Loss: 0.5834996700286865\n",
      "Epoch: 1, Loss: 0.475394070148468\n",
      "Epoch: 1, Loss: 0.548223614692688\n",
      "Epoch: 1, Loss: 0.560084879398346\n",
      "Epoch: 1, Loss: 0.5235186815261841\n",
      "Epoch: 1, Loss: 0.4079834818840027\n",
      "Epoch: 1, Loss: 0.5262026786804199\n",
      "Epoch: 1, Loss: 0.5533479452133179\n",
      "Epoch: 1, Loss: 0.6954048275947571\n",
      "Epoch: 1, Loss: 0.40913236141204834\n",
      "Epoch: 1, Loss: 0.5542471408843994\n",
      "Epoch: 1, Loss: 0.6795558333396912\n",
      "Epoch: 1, Loss: 0.6150001287460327\n",
      "Epoch: 1, Loss: 0.5630491971969604\n",
      "Epoch: 1, Loss: 0.4539698362350464\n",
      "Epoch: 1, Loss: 0.4216015338897705\n",
      "Epoch: 1, Loss: 0.45425504446029663\n",
      "Epoch: 1, Loss: 0.4569539725780487\n",
      "Epoch: 1, Loss: 0.4998566508293152\n",
      "Epoch: 1, Loss: 0.4961269497871399\n",
      "Epoch: 1, Loss: 0.5079877376556396\n",
      "Epoch: 1, Loss: 0.44141310453414917\n",
      "Epoch: 1, Loss: 0.43512094020843506\n",
      "Epoch: 1, Loss: 0.4532751739025116\n",
      "Epoch: 1, Loss: 0.5687806606292725\n",
      "Epoch: 1, Loss: 0.4981808662414551\n",
      "Epoch: 1, Loss: 0.4740191102027893\n",
      "Epoch: 1, Loss: 0.5075216293334961\n",
      "Epoch: 1, Loss: 0.52732914686203\n",
      "Epoch: 1, Loss: 0.4886624813079834\n",
      "Epoch: 1, Loss: 0.6958680748939514\n",
      "Epoch: 1, Loss: 0.5125761032104492\n",
      "Epoch: 1, Loss: 0.38902971148490906\n",
      "Epoch: 1, Loss: 0.4799637794494629\n",
      "Epoch: 1, Loss: 0.4975443482398987\n",
      "Epoch: 1, Loss: 0.4909084439277649\n",
      "Epoch: 1, Loss: 0.6003762483596802\n",
      "Epoch: 1, Loss: 0.5868244767189026\n",
      "Epoch: 1, Loss: 0.5627911686897278\n",
      "Epoch: 1, Loss: 0.45382097363471985\n",
      "Epoch: 1, Loss: 0.5468087196350098\n",
      "Epoch: 1, Loss: 0.4274035394191742\n",
      "Epoch: 1, Loss: 0.4480857253074646\n",
      "Epoch: 1, Loss: 0.6760396957397461\n",
      "Epoch: 1, Loss: 0.5197850465774536\n",
      "Epoch: 1, Loss: 0.4089488387107849\n",
      "Epoch: 1, Loss: 0.5216110944747925\n",
      "Epoch: 1, Loss: 0.5189048647880554\n",
      "Epoch: 1, Loss: 0.43743598461151123\n",
      "Epoch: 1, Loss: 0.5000249147415161\n",
      "Epoch: 1, Loss: 0.4980359673500061\n",
      "Epoch: 1, Loss: 0.5540763735771179\n",
      "Epoch: 1, Loss: 0.44277098774909973\n",
      "Epoch: 1, Loss: 0.47885221242904663\n",
      "Epoch: 1, Loss: 0.4638015031814575\n",
      "Epoch: 1, Loss: 0.4187609553337097\n",
      "Epoch: 1, Loss: 0.5195739269256592\n",
      "Epoch: 1, Loss: 0.5770494937896729\n",
      "Epoch: 1, Loss: 0.4758267104625702\n",
      "Epoch: 1, Loss: 0.687505841255188\n",
      "Epoch: 1, Loss: 0.49349039793014526\n",
      "Epoch: 1, Loss: 0.5540932416915894\n",
      "Epoch: 1, Loss: 0.5938205718994141\n",
      "Epoch: 1, Loss: 0.48192834854125977\n",
      "Epoch: 1, Loss: 0.4633019268512726\n",
      "Epoch: 1, Loss: 0.5180046558380127\n",
      "Epoch: 1, Loss: 0.5328817367553711\n",
      "Epoch: 1, Loss: 0.5130066871643066\n",
      "Epoch: 1, Loss: 0.5299767255783081\n",
      "Epoch: 1, Loss: 0.5481984615325928\n",
      "Epoch: 1, Loss: 0.4453262984752655\n",
      "Epoch: 1, Loss: 0.45107007026672363\n",
      "Epoch: 1, Loss: 0.5022149682044983\n",
      "Epoch: 1, Loss: 0.4909706711769104\n",
      "Epoch: 1, Loss: 0.3998132050037384\n",
      "Epoch: 1, Loss: 0.5565041303634644\n",
      "Epoch: 1, Loss: 0.4049926996231079\n",
      "Epoch: 1, Loss: 0.5518574714660645\n",
      "Epoch: 1, Loss: 0.46635663509368896\n",
      "Epoch: 1, Loss: 0.4432179033756256\n",
      "Epoch: 1, Loss: 0.383009672164917\n",
      "Epoch: 1, Loss: 0.40580588579177856\n",
      "Epoch: 1, Loss: 0.41982850432395935\n",
      "Epoch: 1, Loss: 0.47570398449897766\n",
      "Epoch: 1, Loss: 0.4393184781074524\n",
      "Epoch: 1, Loss: 0.44254210591316223\n",
      "Epoch: 1, Loss: 0.41056886315345764\n",
      "Epoch: 1, Loss: 0.49665534496307373\n",
      "Epoch: 1, Loss: 0.4469958245754242\n",
      "Epoch: 1, Loss: 0.46863922476768494\n",
      "Epoch: 1, Loss: 0.38062846660614014\n",
      "Epoch: 1, Loss: 0.39094483852386475\n",
      "Epoch: 1, Loss: 0.5143520832061768\n",
      "Epoch: 1, Loss: 0.49824774265289307\n",
      "Epoch: 1, Loss: 0.42219477891921997\n",
      "Epoch: 1, Loss: 0.6433851718902588\n",
      "Epoch: 1, Loss: 0.5376474857330322\n",
      "Epoch: 1, Loss: 0.5611290335655212\n",
      "Epoch: 1, Loss: 0.4537500739097595\n",
      "Epoch: 1, Loss: 0.5034262537956238\n",
      "Epoch: 1, Loss: 0.5158241987228394\n",
      "Epoch: 1, Loss: 0.5662490725517273\n",
      "Epoch: 1, Loss: 0.5329822301864624\n",
      "Epoch: 1, Loss: 0.452267050743103\n",
      "Epoch: 1, Loss: 0.38397496938705444\n",
      "Epoch: 1, Loss: 0.43049299716949463\n",
      "Epoch: 1, Loss: 0.41822415590286255\n",
      "Epoch: 1, Loss: 0.41463273763656616\n",
      "Epoch: 1, Loss: 0.41719305515289307\n",
      "Epoch: 1, Loss: 0.36486467719078064\n",
      "Epoch: 1, Loss: 0.5464522838592529\n",
      "Epoch: 1, Loss: 0.5741027593612671\n",
      "Epoch: 1, Loss: 0.5393315553665161\n",
      "Epoch: 1, Loss: 0.5338908433914185\n",
      "Epoch: 1, Loss: 0.45520442724227905\n",
      "Epoch: 1, Loss: 0.5247067213058472\n",
      "Epoch: 1, Loss: 0.4884740710258484\n",
      "Epoch: 1, Loss: 0.44013839960098267\n",
      "Epoch: 1, Loss: 0.6036614179611206\n",
      "Epoch: 1, Loss: 0.522549569606781\n",
      "Epoch: 1, Loss: 0.5320881009101868\n",
      "Epoch: 1, Loss: 0.5004901885986328\n",
      "Epoch: 1, Loss: 0.5522094368934631\n",
      "Epoch: 1, Loss: 0.40876829624176025\n",
      "Epoch: 1, Loss: 0.6731371283531189\n",
      "Epoch: 1, Loss: 0.47069883346557617\n",
      "Epoch: 1, Loss: 0.5267887711524963\n",
      "Epoch: 1, Loss: 0.5115290880203247\n",
      "Epoch: 1, Loss: 0.3936423659324646\n",
      "Epoch: 1, Loss: 0.5783129930496216\n",
      "Epoch: 1, Loss: 0.5731926560401917\n",
      "Epoch: 1, Loss: 0.4963696300983429\n",
      "Epoch: 1, Loss: 0.5778499245643616\n",
      "Epoch: 1, Loss: 0.6932609677314758\n",
      "Epoch: 1, Loss: 0.5804502367973328\n",
      "Epoch: 1, Loss: 0.47433578968048096\n",
      "Epoch: 1, Loss: 0.4696425199508667\n",
      "Epoch: 1, Loss: 0.49838268756866455\n",
      "Epoch: 1, Loss: 0.5080593228340149\n",
      "Epoch: 1, Loss: 0.5716844797134399\n",
      "Epoch: 1, Loss: 0.4996412396430969\n",
      "Epoch: 1, Loss: 0.38367390632629395\n",
      "Epoch: 1, Loss: 0.4228903353214264\n",
      "Epoch: 1, Loss: 0.5253915786743164\n",
      "Epoch: 1, Loss: 0.5468281507492065\n",
      "Epoch: 1, Loss: 0.5215879082679749\n",
      "Epoch: 1, Loss: 0.4444023668766022\n",
      "Epoch: 1, Loss: 0.4209737777709961\n",
      "Epoch: 1, Loss: 0.45404139161109924\n",
      "Epoch: 1, Loss: 0.4974687099456787\n",
      "Epoch: 1, Loss: 0.45960867404937744\n",
      "Epoch: 1, Loss: 0.5579525828361511\n",
      "Epoch: 1, Loss: 0.5052307844161987\n",
      "Epoch: 1, Loss: 0.4774325489997864\n",
      "Epoch: 1, Loss: 0.48586970567703247\n",
      "Epoch: 1, Loss: 0.4841548502445221\n",
      "Epoch: 1, Loss: 0.5753189325332642\n",
      "Epoch: 1, Loss: 0.47549718618392944\n",
      "Epoch: 1, Loss: 0.4560661017894745\n",
      "Epoch: 1, Loss: 0.4933750629425049\n",
      "Epoch: 1, Loss: 0.6375565528869629\n",
      "Epoch: 1, Loss: 0.40854281187057495\n",
      "Epoch: 1, Loss: 0.5360363721847534\n",
      "Epoch: 1, Loss: 0.5398572683334351\n",
      "Epoch: 1, Loss: 0.4727092981338501\n",
      "Epoch: 1, Loss: 0.47651171684265137\n",
      "Epoch: 1, Loss: 0.4301338791847229\n",
      "Epoch: 1, Loss: 0.4754701554775238\n",
      "Epoch: 1, Loss: 0.4431489408016205\n",
      "Epoch: 1, Loss: 0.45670244097709656\n",
      "Epoch: 1, Loss: 0.43298131227493286\n",
      "Epoch: 1, Loss: 0.3845940828323364\n",
      "Epoch: 1, Loss: 0.4803135097026825\n",
      "Epoch: 1, Loss: 0.5172696709632874\n",
      "Epoch: 1, Loss: 0.510053277015686\n",
      "Epoch: 1, Loss: 0.44739246368408203\n",
      "Epoch: 1, Loss: 0.5067374110221863\n",
      "Epoch: 1, Loss: 0.5197657942771912\n",
      "Epoch: 1, Loss: 0.4120558202266693\n",
      "Epoch: 1, Loss: 0.48047006130218506\n",
      "Epoch: 1, Loss: 0.4289948642253876\n",
      "Epoch: 1, Loss: 0.5216701626777649\n",
      "Epoch: 1, Loss: 0.451670378446579\n",
      "Epoch: 1, Loss: 0.4677638113498688\n",
      "Epoch: 1, Loss: 0.43404215574264526\n",
      "Epoch: 1, Loss: 0.47129544615745544\n",
      "Epoch: 1, Loss: 0.49533191323280334\n",
      "Epoch: 1, Loss: 0.5454722046852112\n",
      "Epoch: 1, Loss: 0.6060566902160645\n",
      "Epoch: 1, Loss: 0.48953771591186523\n",
      "Epoch: 1, Loss: 0.5014128088951111\n",
      "Epoch: 1, Loss: 0.46444785594940186\n",
      "Epoch: 1, Loss: 0.43786880373954773\n",
      "Epoch: 1, Loss: 0.4683409631252289\n",
      "Epoch: 1, Loss: 0.5076826214790344\n",
      "Epoch: 1, Loss: 0.5776069164276123\n",
      "Epoch: 1, Loss: 0.5288995504379272\n",
      "Epoch: 1, Loss: 0.4015142619609833\n",
      "Epoch: 1, Loss: 0.49913689494132996\n",
      "Epoch: 1, Loss: 0.5202332735061646\n",
      "Epoch: 1, Loss: 0.5255929231643677\n",
      "Epoch: 1, Loss: 0.4438464045524597\n",
      "Epoch: 1, Loss: 0.466528058052063\n",
      "Epoch: 1, Loss: 0.5302968621253967\n",
      "Epoch: 1, Loss: 0.5450728535652161\n",
      "Epoch: 1, Loss: 0.4300406873226166\n",
      "Epoch: 1, Loss: 0.40986397862434387\n",
      "Epoch: 1, Loss: 0.4490710496902466\n",
      "Epoch: 1, Loss: 0.42819857597351074\n",
      "Epoch: 1, Loss: 0.5422894358634949\n",
      "Epoch: 1, Loss: 0.5147808790206909\n",
      "Epoch: 1, Loss: 0.45820897817611694\n",
      "Epoch: 1, Loss: 0.4787634015083313\n",
      "Epoch: 1, Loss: 0.598111093044281\n",
      "Epoch: 1, Loss: 0.44649696350097656\n",
      "Epoch: 1, Loss: 0.5885100960731506\n",
      "Epoch: 1, Loss: 0.44066599011421204\n",
      "Epoch: 1, Loss: 0.5199428796768188\n",
      "Epoch: 1, Loss: 0.45976728200912476\n",
      "Epoch: 1, Loss: 0.41934460401535034\n",
      "Epoch: 1, Loss: 0.4650249481201172\n",
      "Epoch: 1, Loss: 0.573904812335968\n",
      "Epoch: 1, Loss: 0.6056745648384094\n",
      "Epoch: 1, Loss: 0.5309990644454956\n",
      "Epoch: 1, Loss: 0.4385519325733185\n",
      "Epoch: 1, Loss: 0.5295384526252747\n",
      "Epoch: 1, Loss: 0.49823319911956787\n",
      "Epoch: 1, Loss: 0.4432891607284546\n",
      "Epoch: 1, Loss: 0.4739367663860321\n",
      "Epoch: 1, Loss: 0.4385295808315277\n",
      "Epoch: 1, Loss: 0.3933507204055786\n",
      "Epoch: 1, Loss: 0.4551692605018616\n",
      "Epoch: 1, Loss: 0.5574989318847656\n",
      "Epoch: 1, Loss: 0.5315533876419067\n",
      "Epoch: 1, Loss: 0.581628680229187\n",
      "Epoch: 1, Loss: 0.47411614656448364\n",
      "Epoch: 1, Loss: 0.5155094861984253\n",
      "Epoch: 1, Loss: 0.4307502210140228\n",
      "Epoch: 1, Loss: 0.46130871772766113\n",
      "Epoch: 1, Loss: 0.549615740776062\n",
      "Epoch: 1, Loss: 0.5312985181808472\n",
      "Epoch: 1, Loss: 0.4923264682292938\n",
      "Epoch: 1, Loss: 0.4044521450996399\n",
      "Epoch: 1, Loss: 0.5585898756980896\n",
      "Epoch: 1, Loss: 0.4263763427734375\n",
      "Epoch: 2, Loss: 0.44509583711624146\n",
      "Epoch: 2, Loss: 0.4698927402496338\n",
      "Epoch: 2, Loss: 0.5373168587684631\n",
      "Epoch: 2, Loss: 0.49306783080101013\n",
      "Epoch: 2, Loss: 0.44101768732070923\n",
      "Epoch: 2, Loss: 0.4263373911380768\n",
      "Epoch: 2, Loss: 0.5824912190437317\n",
      "Epoch: 2, Loss: 0.592061460018158\n",
      "Epoch: 2, Loss: 0.6298336386680603\n",
      "Epoch: 2, Loss: 0.5398695468902588\n",
      "Epoch: 2, Loss: 0.5187697410583496\n",
      "Epoch: 2, Loss: 0.5583773851394653\n",
      "Epoch: 2, Loss: 0.38300615549087524\n",
      "Epoch: 2, Loss: 0.4341690242290497\n",
      "Epoch: 2, Loss: 0.4639866352081299\n",
      "Epoch: 2, Loss: 0.5393807888031006\n",
      "Epoch: 2, Loss: 0.4157557487487793\n",
      "Epoch: 2, Loss: 0.5221350193023682\n",
      "Epoch: 2, Loss: 0.44529643654823303\n",
      "Epoch: 2, Loss: 0.5104673504829407\n",
      "Epoch: 2, Loss: 0.4537493586540222\n",
      "Epoch: 2, Loss: 0.4410842955112457\n",
      "Epoch: 2, Loss: 0.5290665626525879\n",
      "Epoch: 2, Loss: 0.5274094939231873\n",
      "Epoch: 2, Loss: 0.5096149444580078\n",
      "Epoch: 2, Loss: 0.5007149577140808\n",
      "Epoch: 2, Loss: 0.4977743625640869\n",
      "Epoch: 2, Loss: 0.4403422772884369\n",
      "Epoch: 2, Loss: 0.48124414682388306\n",
      "Epoch: 2, Loss: 0.4350002706050873\n",
      "Epoch: 2, Loss: 0.41063252091407776\n",
      "Epoch: 2, Loss: 0.4291549324989319\n",
      "Epoch: 2, Loss: 0.4569883942604065\n",
      "Epoch: 2, Loss: 0.5027004480361938\n",
      "Epoch: 2, Loss: 0.41688287258148193\n",
      "Epoch: 2, Loss: 0.4648146331310272\n",
      "Epoch: 2, Loss: 0.5098128914833069\n",
      "Epoch: 2, Loss: 0.42972618341445923\n",
      "Epoch: 2, Loss: 0.4974063038825989\n",
      "Epoch: 2, Loss: 0.5246130228042603\n",
      "Epoch: 2, Loss: 0.4257696568965912\n",
      "Epoch: 2, Loss: 0.5539197325706482\n",
      "Epoch: 2, Loss: 0.5012618899345398\n",
      "Epoch: 2, Loss: 0.5199669003486633\n",
      "Epoch: 2, Loss: 0.4385124742984772\n",
      "Epoch: 2, Loss: 0.3751071095466614\n",
      "Epoch: 2, Loss: 0.5131963491439819\n",
      "Epoch: 2, Loss: 0.5168323516845703\n",
      "Epoch: 2, Loss: 0.49757668375968933\n",
      "Epoch: 2, Loss: 0.5995075702667236\n",
      "Epoch: 2, Loss: 0.47764837741851807\n",
      "Epoch: 2, Loss: 0.46935364603996277\n",
      "Epoch: 2, Loss: 0.5305917263031006\n",
      "Epoch: 2, Loss: 0.6198951005935669\n",
      "Epoch: 2, Loss: 0.49851077795028687\n",
      "Epoch: 2, Loss: 0.42921411991119385\n",
      "Epoch: 2, Loss: 0.46985653042793274\n",
      "Epoch: 2, Loss: 0.5203493237495422\n",
      "Epoch: 2, Loss: 0.6071321964263916\n",
      "Epoch: 2, Loss: 0.47058388590812683\n",
      "Epoch: 2, Loss: 0.45570871233940125\n",
      "Epoch: 2, Loss: 0.41857442259788513\n",
      "Epoch: 2, Loss: 0.45428454875946045\n",
      "Epoch: 2, Loss: 0.461697518825531\n",
      "Epoch: 2, Loss: 0.4555913209915161\n",
      "Epoch: 2, Loss: 0.3921528458595276\n",
      "Epoch: 2, Loss: 0.4896193742752075\n",
      "Epoch: 2, Loss: 0.4254070818424225\n",
      "Epoch: 2, Loss: 0.6560089588165283\n",
      "Epoch: 2, Loss: 0.6822561025619507\n",
      "Epoch: 2, Loss: 0.47817423939704895\n",
      "Epoch: 2, Loss: 0.4586522579193115\n",
      "Epoch: 2, Loss: 0.5731081962585449\n",
      "Epoch: 2, Loss: 0.46559858322143555\n",
      "Epoch: 2, Loss: 0.4784190356731415\n",
      "Epoch: 2, Loss: 0.5775732398033142\n",
      "Epoch: 2, Loss: 0.39607855677604675\n",
      "Epoch: 2, Loss: 0.5062506198883057\n",
      "Epoch: 2, Loss: 0.40376710891723633\n",
      "Epoch: 2, Loss: 0.47766029834747314\n",
      "Epoch: 2, Loss: 0.6808919906616211\n",
      "Epoch: 2, Loss: 0.5856549739837646\n",
      "Epoch: 2, Loss: 0.4371952414512634\n",
      "Epoch: 2, Loss: 0.6229084134101868\n",
      "Epoch: 2, Loss: 0.4888710677623749\n",
      "Epoch: 2, Loss: 0.44735127687454224\n",
      "Epoch: 2, Loss: 0.5430546402931213\n",
      "Epoch: 2, Loss: 0.40953966975212097\n",
      "Epoch: 2, Loss: 0.5195793509483337\n",
      "Epoch: 2, Loss: 0.5430573225021362\n",
      "Epoch: 2, Loss: 0.4103532135486603\n",
      "Epoch: 2, Loss: 0.47839856147766113\n",
      "Epoch: 2, Loss: 0.4447394013404846\n",
      "Epoch: 2, Loss: 0.4475281834602356\n",
      "Epoch: 2, Loss: 0.47654035687446594\n",
      "Epoch: 2, Loss: 0.41924482583999634\n",
      "Epoch: 2, Loss: 0.6524304151535034\n",
      "Epoch: 2, Loss: 0.5227160453796387\n",
      "Epoch: 2, Loss: 0.46222808957099915\n",
      "Epoch: 2, Loss: 0.5070411562919617\n",
      "Epoch: 2, Loss: 0.40538451075553894\n",
      "Epoch: 2, Loss: 0.4657924771308899\n",
      "Epoch: 2, Loss: 0.4613717198371887\n",
      "Epoch: 2, Loss: 0.570970892906189\n",
      "Epoch: 2, Loss: 0.4825460612773895\n",
      "Epoch: 2, Loss: 0.46096837520599365\n",
      "Epoch: 2, Loss: 0.5638445019721985\n",
      "Epoch: 2, Loss: 0.5820635557174683\n",
      "Epoch: 2, Loss: 0.4314187169075012\n",
      "Epoch: 2, Loss: 0.4055767357349396\n",
      "Epoch: 2, Loss: 0.4519113302230835\n",
      "Epoch: 2, Loss: 0.46271833777427673\n",
      "Epoch: 2, Loss: 0.5027969479560852\n",
      "Epoch: 2, Loss: 0.5260472297668457\n",
      "Epoch: 2, Loss: 0.5355843901634216\n",
      "Epoch: 2, Loss: 0.554455578327179\n",
      "Epoch: 2, Loss: 0.46965503692626953\n",
      "Epoch: 2, Loss: 0.525820255279541\n",
      "Epoch: 2, Loss: 0.43711450695991516\n",
      "Epoch: 2, Loss: 0.45664504170417786\n",
      "Epoch: 2, Loss: 0.38942623138427734\n",
      "Epoch: 2, Loss: 0.42105937004089355\n",
      "Epoch: 2, Loss: 0.39686816930770874\n",
      "Epoch: 2, Loss: 0.4543411433696747\n",
      "Epoch: 2, Loss: 0.40573519468307495\n",
      "Epoch: 2, Loss: 0.5504189133644104\n",
      "Epoch: 2, Loss: 0.4463335871696472\n",
      "Epoch: 2, Loss: 0.49750572443008423\n",
      "Epoch: 2, Loss: 0.5734647512435913\n",
      "Epoch: 2, Loss: 0.5415187478065491\n",
      "Epoch: 2, Loss: 0.4155789613723755\n",
      "Epoch: 2, Loss: 0.4541383683681488\n",
      "Epoch: 2, Loss: 0.4625409245491028\n",
      "Epoch: 2, Loss: 0.45331311225891113\n",
      "Epoch: 2, Loss: 0.4608708918094635\n",
      "Epoch: 2, Loss: 0.4757392406463623\n",
      "Epoch: 2, Loss: 0.5893604755401611\n",
      "Epoch: 2, Loss: 0.4400908946990967\n",
      "Epoch: 2, Loss: 0.5115018486976624\n",
      "Epoch: 2, Loss: 0.47026151418685913\n",
      "Epoch: 2, Loss: 0.3931852877140045\n",
      "Epoch: 2, Loss: 0.4689585864543915\n",
      "Epoch: 2, Loss: 0.50772625207901\n",
      "Epoch: 2, Loss: 0.4754531979560852\n",
      "Epoch: 2, Loss: 0.47041013836860657\n",
      "Epoch: 2, Loss: 0.6097501516342163\n",
      "Epoch: 2, Loss: 0.4118817150592804\n",
      "Epoch: 2, Loss: 0.4476417005062103\n",
      "Epoch: 2, Loss: 0.37993568181991577\n",
      "Epoch: 2, Loss: 0.4936622381210327\n",
      "Epoch: 2, Loss: 0.41896533966064453\n",
      "Epoch: 2, Loss: 0.47431138157844543\n",
      "Epoch: 2, Loss: 0.4254506826400757\n",
      "Epoch: 2, Loss: 0.5500572919845581\n",
      "Epoch: 2, Loss: 0.6480299234390259\n",
      "Epoch: 2, Loss: 0.42817187309265137\n",
      "Epoch: 2, Loss: 0.575394868850708\n",
      "Epoch: 2, Loss: 0.49735361337661743\n",
      "Epoch: 2, Loss: 0.5115482807159424\n",
      "Epoch: 2, Loss: 0.37608927488327026\n",
      "Epoch: 2, Loss: 0.5028328895568848\n",
      "Epoch: 2, Loss: 0.5123186707496643\n",
      "Epoch: 2, Loss: 0.476339191198349\n",
      "Epoch: 2, Loss: 0.4943077564239502\n",
      "Epoch: 2, Loss: 0.4271150827407837\n",
      "Epoch: 2, Loss: 0.45541656017303467\n",
      "Epoch: 2, Loss: 0.506445586681366\n",
      "Epoch: 2, Loss: 0.6039078235626221\n",
      "Epoch: 2, Loss: 0.6334988474845886\n",
      "Epoch: 2, Loss: 0.6324583888053894\n",
      "Epoch: 2, Loss: 0.6854255199432373\n",
      "Epoch: 2, Loss: 0.5658612251281738\n",
      "Epoch: 2, Loss: 0.4745798110961914\n",
      "Epoch: 2, Loss: 0.4907253086566925\n",
      "Epoch: 2, Loss: 0.415751188993454\n",
      "Epoch: 2, Loss: 0.4036256670951843\n",
      "Epoch: 2, Loss: 0.4493182301521301\n",
      "Epoch: 2, Loss: 0.44395482540130615\n",
      "Epoch: 2, Loss: 0.4211827516555786\n",
      "Epoch: 2, Loss: 0.49742621183395386\n",
      "Epoch: 2, Loss: 0.5174334645271301\n",
      "Epoch: 2, Loss: 0.48958274722099304\n",
      "Epoch: 2, Loss: 0.5388641357421875\n",
      "Epoch: 2, Loss: 0.5421044826507568\n",
      "Epoch: 2, Loss: 0.46095752716064453\n",
      "Epoch: 2, Loss: 0.5082868933677673\n",
      "Epoch: 2, Loss: 0.3738815188407898\n",
      "Epoch: 2, Loss: 0.43239644169807434\n",
      "Epoch: 2, Loss: 0.42928802967071533\n",
      "Epoch: 2, Loss: 0.44216832518577576\n",
      "Epoch: 2, Loss: 0.5012156963348389\n",
      "Epoch: 2, Loss: 0.5636229515075684\n",
      "Epoch: 2, Loss: 0.4836883544921875\n",
      "Epoch: 2, Loss: 0.43250465393066406\n",
      "Epoch: 2, Loss: 0.47934165596961975\n",
      "Epoch: 2, Loss: 0.4539625346660614\n",
      "Epoch: 2, Loss: 0.4015982151031494\n",
      "Epoch: 2, Loss: 0.46642157435417175\n",
      "Epoch: 2, Loss: 0.5113551616668701\n",
      "Epoch: 2, Loss: 0.44191521406173706\n",
      "Epoch: 2, Loss: 0.4615192711353302\n",
      "Epoch: 2, Loss: 0.4485929012298584\n",
      "Epoch: 2, Loss: 0.49891674518585205\n",
      "Epoch: 2, Loss: 0.39735281467437744\n",
      "Epoch: 2, Loss: 0.47642403841018677\n",
      "Epoch: 2, Loss: 0.4260103106498718\n",
      "Epoch: 2, Loss: 0.4838552474975586\n",
      "Epoch: 2, Loss: 0.4328294098377228\n",
      "Epoch: 2, Loss: 0.5229528546333313\n",
      "Epoch: 2, Loss: 0.4965355694293976\n",
      "Epoch: 2, Loss: 0.4400556683540344\n",
      "Epoch: 2, Loss: 0.4640966057777405\n",
      "Epoch: 2, Loss: 0.4429764747619629\n",
      "Epoch: 2, Loss: 0.42079034447669983\n",
      "Epoch: 2, Loss: 0.3912956416606903\n",
      "Epoch: 2, Loss: 0.5882647037506104\n",
      "Epoch: 2, Loss: 0.5213986039161682\n",
      "Epoch: 2, Loss: 0.5108174085617065\n",
      "Epoch: 2, Loss: 0.45794880390167236\n",
      "Epoch: 2, Loss: 0.4372161626815796\n",
      "Epoch: 2, Loss: 0.3798218369483948\n",
      "Epoch: 2, Loss: 0.4491293728351593\n",
      "Epoch: 2, Loss: 0.4873518943786621\n",
      "Epoch: 2, Loss: 0.5794560313224792\n",
      "Epoch: 2, Loss: 0.5256745219230652\n",
      "Epoch: 2, Loss: 0.5045034289360046\n",
      "Epoch: 2, Loss: 0.5005269050598145\n",
      "Epoch: 2, Loss: 0.5079514980316162\n",
      "Epoch: 2, Loss: 0.4871714413166046\n",
      "Epoch: 2, Loss: 0.4903109073638916\n",
      "Epoch: 2, Loss: 0.3967694044113159\n",
      "Epoch: 2, Loss: 0.48236608505249023\n",
      "Epoch: 2, Loss: 0.49873316287994385\n",
      "Epoch: 2, Loss: 0.4890030026435852\n",
      "Epoch: 2, Loss: 0.4411965608596802\n",
      "Epoch: 2, Loss: 0.4759860038757324\n",
      "Epoch: 2, Loss: 0.4350880980491638\n",
      "Epoch: 2, Loss: 0.41692185401916504\n",
      "Epoch: 2, Loss: 0.4528578519821167\n",
      "Epoch: 2, Loss: 0.4749242663383484\n",
      "Epoch: 2, Loss: 0.5191116333007812\n",
      "Epoch: 2, Loss: 0.5333021879196167\n",
      "Epoch: 2, Loss: 0.4900551438331604\n",
      "Epoch: 2, Loss: 0.4791473150253296\n",
      "Epoch: 2, Loss: 0.6295602917671204\n",
      "Epoch: 2, Loss: 0.5139322280883789\n",
      "Epoch: 2, Loss: 0.5984783172607422\n",
      "Epoch: 2, Loss: 0.4705205261707306\n",
      "Epoch: 2, Loss: 0.48462337255477905\n",
      "Epoch: 2, Loss: 0.38203349709510803\n",
      "Epoch: 2, Loss: 0.47575467824935913\n",
      "Epoch: 2, Loss: 0.46383947134017944\n",
      "Epoch: 2, Loss: 0.5104004740715027\n",
      "Epoch: 2, Loss: 0.5416731834411621\n",
      "Epoch: 2, Loss: 0.4302706718444824\n",
      "Epoch: 2, Loss: 0.4794657528400421\n",
      "Epoch: 2, Loss: 0.5097799897193909\n",
      "Epoch: 2, Loss: 0.5464627146720886\n",
      "Epoch: 2, Loss: 0.40313202142715454\n",
      "Epoch: 2, Loss: 0.5357624888420105\n",
      "Epoch: 2, Loss: 0.5359612703323364\n",
      "Epoch: 2, Loss: 0.5414258241653442\n",
      "Epoch: 2, Loss: 0.40059036016464233\n",
      "Epoch: 2, Loss: 0.521285355091095\n",
      "Epoch: 2, Loss: 0.49066606163978577\n",
      "Epoch: 2, Loss: 0.40627408027648926\n",
      "Epoch: 2, Loss: 0.4015581011772156\n",
      "Epoch: 2, Loss: 0.48973238468170166\n",
      "Epoch: 2, Loss: 0.5672942996025085\n",
      "Epoch: 2, Loss: 0.5179906487464905\n",
      "Epoch: 2, Loss: 0.41898754239082336\n",
      "Epoch: 2, Loss: 0.47342732548713684\n",
      "Epoch: 2, Loss: 0.48973503708839417\n",
      "Epoch: 2, Loss: 0.5824730396270752\n",
      "Epoch: 2, Loss: 0.5378683805465698\n",
      "Epoch: 2, Loss: 0.4764922559261322\n",
      "Epoch: 2, Loss: 0.4606796205043793\n",
      "Epoch: 2, Loss: 0.4480416774749756\n",
      "Epoch: 2, Loss: 0.40286511182785034\n",
      "Epoch: 2, Loss: 0.5634079575538635\n",
      "Epoch: 2, Loss: 0.4218440651893616\n",
      "Epoch: 2, Loss: 0.4691372811794281\n",
      "Epoch: 2, Loss: 0.39610201120376587\n",
      "Epoch: 2, Loss: 0.4380880296230316\n",
      "Epoch: 2, Loss: 0.4559977650642395\n",
      "Epoch: 2, Loss: 0.42248156666755676\n",
      "Epoch: 2, Loss: 0.6053710579872131\n",
      "Epoch: 2, Loss: 0.48346763849258423\n",
      "Epoch: 2, Loss: 0.35939186811447144\n",
      "Epoch: 2, Loss: 0.46993449330329895\n",
      "Epoch: 2, Loss: 0.4817466735839844\n",
      "Epoch: 2, Loss: 0.5504090785980225\n",
      "Epoch: 2, Loss: 0.5123368501663208\n",
      "Epoch: 2, Loss: 0.5089556574821472\n",
      "Epoch: 2, Loss: 0.44539982080459595\n",
      "Epoch: 2, Loss: 0.3883737027645111\n",
      "Epoch: 2, Loss: 0.4950854778289795\n",
      "Epoch: 2, Loss: 0.4293041229248047\n",
      "Epoch: 2, Loss: 0.47623753547668457\n",
      "Epoch: 2, Loss: 0.5270649194717407\n",
      "Epoch: 2, Loss: 0.5252419710159302\n",
      "Epoch: 2, Loss: 0.5488529205322266\n",
      "Epoch: 2, Loss: 0.4839174151420593\n",
      "Epoch: 2, Loss: 0.4964655935764313\n",
      "Epoch: 2, Loss: 0.43657398223876953\n",
      "Epoch: 2, Loss: 0.407108873128891\n",
      "Epoch: 2, Loss: 0.41632378101348877\n",
      "Epoch: 2, Loss: 0.5006105303764343\n",
      "Epoch: 2, Loss: 0.4615696668624878\n",
      "Epoch: 2, Loss: 0.4958195686340332\n",
      "Epoch: 2, Loss: 0.5474138259887695\n",
      "Epoch: 2, Loss: 0.4509601294994354\n",
      "Epoch: 2, Loss: 0.42800119519233704\n",
      "Epoch: 2, Loss: 0.6005284190177917\n",
      "Epoch: 2, Loss: 0.47435247898101807\n",
      "Epoch: 2, Loss: 0.5544841289520264\n",
      "Epoch: 2, Loss: 0.4041179120540619\n",
      "Epoch: 2, Loss: 0.4846404492855072\n",
      "Epoch: 2, Loss: 0.37946924567222595\n",
      "Epoch: 2, Loss: 0.6105403304100037\n",
      "Epoch: 2, Loss: 0.45601898431777954\n",
      "Epoch: 2, Loss: 0.5957613587379456\n",
      "Epoch: 2, Loss: 0.6270721554756165\n",
      "Epoch: 2, Loss: 0.4486166536808014\n",
      "Epoch: 2, Loss: 0.4575348496437073\n",
      "Epoch: 2, Loss: 0.4025426506996155\n",
      "Epoch: 2, Loss: 0.4653978943824768\n",
      "Epoch: 2, Loss: 0.4488239884376526\n",
      "Epoch: 2, Loss: 0.4264540672302246\n",
      "Epoch: 2, Loss: 0.5669844150543213\n",
      "Epoch: 2, Loss: 0.41027557849884033\n",
      "Epoch: 2, Loss: 0.4381296634674072\n",
      "Epoch: 2, Loss: 0.4428207278251648\n",
      "Epoch: 2, Loss: 0.5351197719573975\n",
      "Epoch: 2, Loss: 0.4442010521888733\n",
      "Epoch: 2, Loss: 0.43338069319725037\n",
      "Epoch: 2, Loss: 0.49450790882110596\n",
      "Epoch: 2, Loss: 0.4368478059768677\n",
      "Epoch: 2, Loss: 0.4343484044075012\n",
      "Epoch: 2, Loss: 0.5390697717666626\n",
      "Epoch: 2, Loss: 0.48628339171409607\n",
      "Epoch: 2, Loss: 0.4968179762363434\n",
      "Epoch: 2, Loss: 0.37570512294769287\n",
      "Epoch: 2, Loss: 0.4312838912010193\n",
      "Epoch: 2, Loss: 0.48692265152931213\n",
      "Epoch: 2, Loss: 0.4892813563346863\n",
      "Epoch: 2, Loss: 0.5138065218925476\n",
      "Epoch: 2, Loss: 0.5338611602783203\n",
      "Epoch: 2, Loss: 0.40187954902648926\n",
      "Epoch: 2, Loss: 0.5281485319137573\n",
      "Epoch: 2, Loss: 0.5624175071716309\n",
      "Epoch: 2, Loss: 0.5152892470359802\n",
      "Epoch: 2, Loss: 0.48232996463775635\n",
      "Epoch: 2, Loss: 0.6290799379348755\n",
      "Epoch: 2, Loss: 0.44044724106788635\n",
      "Epoch: 2, Loss: 0.504402220249176\n",
      "Epoch: 2, Loss: 0.6122128963470459\n",
      "Epoch: 2, Loss: 0.5193959474563599\n",
      "Epoch: 2, Loss: 0.4514903128147125\n",
      "Epoch: 2, Loss: 0.4079248905181885\n",
      "Epoch: 2, Loss: 0.5374083518981934\n",
      "Epoch: 2, Loss: 0.4505898952484131\n",
      "Epoch: 2, Loss: 0.5753094553947449\n",
      "Epoch: 2, Loss: 0.48958030343055725\n",
      "Epoch: 2, Loss: 0.5115624070167542\n",
      "Epoch: 2, Loss: 0.602879524230957\n",
      "Epoch: 2, Loss: 0.5200360417366028\n",
      "Epoch: 2, Loss: 0.49661028385162354\n",
      "Epoch: 2, Loss: 0.5048983097076416\n",
      "Epoch: 2, Loss: 0.4280220568180084\n",
      "Epoch: 2, Loss: 0.4358343482017517\n",
      "Epoch: 2, Loss: 0.5145111083984375\n",
      "Epoch: 2, Loss: 0.5037229657173157\n",
      "Epoch: 2, Loss: 0.39450785517692566\n",
      "Epoch: 2, Loss: 0.3990130126476288\n",
      "Epoch: 2, Loss: 0.53403240442276\n",
      "Epoch: 2, Loss: 0.6166850328445435\n",
      "Epoch: 2, Loss: 0.36419400572776794\n",
      "Epoch: 2, Loss: 0.5307499170303345\n",
      "Epoch: 2, Loss: 0.43581467866897583\n",
      "Epoch: 2, Loss: 0.47241395711898804\n",
      "Epoch: 2, Loss: 0.4203982949256897\n",
      "Epoch: 2, Loss: 0.43851354718208313\n",
      "Epoch: 2, Loss: 0.42466723918914795\n",
      "Epoch: 2, Loss: 0.487442284822464\n",
      "Epoch: 2, Loss: 0.48495063185691833\n",
      "Epoch: 2, Loss: 0.5795001983642578\n",
      "Epoch: 2, Loss: 0.5268620252609253\n",
      "Epoch: 2, Loss: 0.37634360790252686\n",
      "Epoch: 2, Loss: 0.41038957238197327\n",
      "Epoch: 2, Loss: 0.722141683101654\n",
      "Epoch: 2, Loss: 0.5090150237083435\n",
      "Epoch: 2, Loss: 0.45873188972473145\n",
      "Epoch: 2, Loss: 0.5339889526367188\n",
      "Epoch: 2, Loss: 0.42388224601745605\n",
      "Epoch: 2, Loss: 0.44012707471847534\n",
      "Epoch: 2, Loss: 0.4126240313053131\n",
      "Epoch: 2, Loss: 0.4847024381160736\n",
      "Epoch: 2, Loss: 0.426582932472229\n",
      "Epoch: 2, Loss: 0.47987210750579834\n",
      "Epoch: 2, Loss: 0.5149301290512085\n",
      "Epoch: 2, Loss: 0.509534478187561\n",
      "Epoch: 2, Loss: 0.4100481867790222\n",
      "Epoch: 2, Loss: 0.4657474756240845\n",
      "Epoch: 2, Loss: 0.4299769401550293\n",
      "Epoch: 2, Loss: 0.4263499677181244\n",
      "Epoch: 2, Loss: 0.39857685565948486\n",
      "Epoch: 2, Loss: 0.5336527824401855\n",
      "Epoch: 2, Loss: 0.6156754493713379\n",
      "Epoch: 2, Loss: 0.4839693009853363\n",
      "Epoch: 2, Loss: 0.5149403214454651\n",
      "Epoch: 2, Loss: 0.5341328978538513\n",
      "Epoch: 2, Loss: 0.5542020201683044\n",
      "Epoch: 2, Loss: 0.468936026096344\n",
      "Epoch: 2, Loss: 0.4947505593299866\n",
      "Epoch: 2, Loss: 0.45624709129333496\n",
      "Epoch: 2, Loss: 0.4015791416168213\n",
      "Epoch: 2, Loss: 0.5267752408981323\n",
      "Epoch: 2, Loss: 0.4656684994697571\n",
      "Epoch: 2, Loss: 0.5005992650985718\n",
      "Epoch: 2, Loss: 0.4742012619972229\n",
      "Epoch: 2, Loss: 0.40758827328681946\n",
      "Epoch: 2, Loss: 0.47650429606437683\n",
      "Epoch: 2, Loss: 0.4446813464164734\n",
      "Epoch: 2, Loss: 0.4242905080318451\n",
      "Epoch: 2, Loss: 0.48085296154022217\n",
      "Epoch: 2, Loss: 0.4954507350921631\n",
      "Epoch: 2, Loss: 0.5418177247047424\n",
      "Epoch: 2, Loss: 0.4777776002883911\n",
      "Epoch: 2, Loss: 0.39226970076560974\n",
      "Epoch: 2, Loss: 0.5074585676193237\n",
      "Epoch: 2, Loss: 0.5289549827575684\n",
      "Epoch: 2, Loss: 0.5070040822029114\n",
      "Epoch: 2, Loss: 0.4783526360988617\n",
      "Epoch: 2, Loss: 0.42120975255966187\n",
      "Epoch: 2, Loss: 0.5430097579956055\n",
      "Epoch: 2, Loss: 0.39706364274024963\n",
      "Epoch: 2, Loss: 0.44191208481788635\n",
      "Epoch: 2, Loss: 0.55230712890625\n",
      "Epoch: 2, Loss: 0.4882778525352478\n",
      "Epoch: 2, Loss: 0.545006275177002\n",
      "Epoch: 2, Loss: 0.4519316554069519\n",
      "Epoch: 2, Loss: 0.44568508863449097\n",
      "Epoch: 2, Loss: 0.47342026233673096\n",
      "Epoch: 2, Loss: 0.4537060856819153\n",
      "Epoch: 2, Loss: 0.5242906808853149\n",
      "Epoch: 2, Loss: 0.5416256189346313\n",
      "Epoch: 2, Loss: 0.39902201294898987\n",
      "Epoch: 2, Loss: 0.46796202659606934\n",
      "Epoch: 2, Loss: 0.5120893716812134\n",
      "Epoch: 2, Loss: 0.4541580080986023\n",
      "Epoch: 2, Loss: 0.49777358770370483\n",
      "Epoch: 2, Loss: 0.5664054155349731\n",
      "Epoch: 2, Loss: 0.4561493992805481\n",
      "Epoch: 2, Loss: 0.5374741554260254\n",
      "Epoch: 2, Loss: 0.3782213032245636\n",
      "Epoch: 2, Loss: 0.4883693754673004\n",
      "Epoch: 2, Loss: 0.42425045371055603\n",
      "Epoch: 2, Loss: 0.4288032650947571\n",
      "Epoch: 2, Loss: 0.4432889521121979\n",
      "Epoch: 2, Loss: 0.4159492254257202\n",
      "Epoch: 2, Loss: 0.43208953738212585\n",
      "Epoch: 2, Loss: 0.4134998619556427\n",
      "Epoch: 2, Loss: 0.43637388944625854\n",
      "Epoch: 2, Loss: 0.4970662295818329\n",
      "Epoch: 2, Loss: 0.38900670409202576\n",
      "Epoch: 2, Loss: 0.534633994102478\n",
      "Epoch: 2, Loss: 0.5094976425170898\n",
      "Epoch: 2, Loss: 0.4220624268054962\n",
      "Epoch: 2, Loss: 0.5997544527053833\n",
      "Epoch: 2, Loss: 0.46233370900154114\n",
      "Epoch: 2, Loss: 0.5511650443077087\n",
      "Epoch: 2, Loss: 0.4836811423301697\n",
      "Epoch: 2, Loss: 0.5180853009223938\n",
      "Epoch: 2, Loss: 0.4361669421195984\n",
      "Epoch: 2, Loss: 0.41297852993011475\n",
      "Epoch: 2, Loss: 0.42149531841278076\n",
      "Epoch: 2, Loss: 0.5577746629714966\n",
      "Epoch: 2, Loss: 0.5614351630210876\n",
      "Epoch: 2, Loss: 0.4207775592803955\n",
      "Epoch: 2, Loss: 0.4984813332557678\n",
      "Epoch: 2, Loss: 0.44370755553245544\n",
      "Epoch: 2, Loss: 0.43637895584106445\n",
      "Epoch: 2, Loss: 0.5170619487762451\n",
      "Epoch: 2, Loss: 0.5666840672492981\n",
      "Epoch: 2, Loss: 0.4684034287929535\n",
      "Epoch: 2, Loss: 0.5499932765960693\n",
      "Epoch: 2, Loss: 0.4020800292491913\n",
      "Epoch: 2, Loss: 0.5184841752052307\n",
      "Epoch: 2, Loss: 0.3922886550426483\n",
      "Epoch: 2, Loss: 0.5080243349075317\n",
      "Epoch: 2, Loss: 0.47749239206314087\n",
      "Epoch: 2, Loss: 0.4851536154747009\n",
      "Epoch: 2, Loss: 0.44949835538864136\n",
      "Epoch: 2, Loss: 0.45592036843299866\n",
      "Epoch: 2, Loss: 0.5424140095710754\n",
      "Epoch: 2, Loss: 0.6814694404602051\n",
      "Epoch: 2, Loss: 0.5346221923828125\n",
      "Epoch: 2, Loss: 0.4746975600719452\n",
      "Epoch: 2, Loss: 0.4821688234806061\n",
      "Epoch: 2, Loss: 0.5439788699150085\n",
      "Epoch: 2, Loss: 0.5802689790725708\n",
      "Epoch: 2, Loss: 0.44221776723861694\n",
      "Epoch: 2, Loss: 0.5148319602012634\n",
      "Epoch: 2, Loss: 0.4672622084617615\n",
      "Epoch: 2, Loss: 0.5252227783203125\n",
      "Epoch: 2, Loss: 0.4868931770324707\n",
      "Epoch: 2, Loss: 0.4739709794521332\n",
      "Epoch: 2, Loss: 0.5651578307151794\n",
      "Epoch: 2, Loss: 0.46249693632125854\n",
      "Epoch: 2, Loss: 0.4539739787578583\n",
      "Epoch: 2, Loss: 0.5766002535820007\n",
      "Epoch: 2, Loss: 0.43996596336364746\n",
      "Epoch: 2, Loss: 0.525455892086029\n",
      "Epoch: 2, Loss: 0.4861518442630768\n",
      "Epoch: 2, Loss: 0.46001726388931274\n",
      "Epoch: 2, Loss: 0.5169878005981445\n",
      "Epoch: 2, Loss: 0.562567412853241\n",
      "Epoch: 2, Loss: 0.6442782282829285\n",
      "Epoch: 2, Loss: 0.5319201350212097\n",
      "Epoch: 2, Loss: 0.5399373769760132\n",
      "Epoch: 2, Loss: 0.4277394413948059\n",
      "Epoch: 2, Loss: 0.47841253876686096\n",
      "Epoch: 2, Loss: 0.4774802327156067\n",
      "Epoch: 2, Loss: 0.5227150321006775\n",
      "Epoch: 2, Loss: 0.429726779460907\n",
      "Epoch: 2, Loss: 0.4628320038318634\n",
      "Epoch: 2, Loss: 0.5188443660736084\n",
      "Epoch: 2, Loss: 0.5079318881034851\n",
      "Epoch: 2, Loss: 0.47109174728393555\n",
      "Epoch: 2, Loss: 0.6657581329345703\n",
      "Epoch: 2, Loss: 0.47745296359062195\n",
      "Epoch: 2, Loss: 0.47849345207214355\n",
      "Epoch: 2, Loss: 0.5708812475204468\n",
      "Epoch: 2, Loss: 0.4488108158111572\n",
      "Epoch: 2, Loss: 0.4597737193107605\n",
      "Epoch: 2, Loss: 0.6380975842475891\n",
      "Epoch: 2, Loss: 0.5826545357704163\n",
      "Epoch: 2, Loss: 0.3908684551715851\n",
      "Epoch: 2, Loss: 0.3657447099685669\n",
      "Epoch: 2, Loss: 0.5335738062858582\n",
      "Epoch: 2, Loss: 0.40844181180000305\n",
      "Epoch: 2, Loss: 0.43668222427368164\n",
      "Epoch: 2, Loss: 0.4094011187553406\n",
      "Epoch: 2, Loss: 0.6177998185157776\n",
      "Epoch: 2, Loss: 0.4782930612564087\n",
      "Epoch: 2, Loss: 0.5481063723564148\n",
      "Epoch: 2, Loss: 0.45120465755462646\n",
      "Epoch: 2, Loss: 0.6117895841598511\n",
      "Epoch: 2, Loss: 0.46156853437423706\n",
      "Epoch: 2, Loss: 0.46612226963043213\n",
      "Epoch: 2, Loss: 0.588274359703064\n",
      "Epoch: 2, Loss: 0.7113366723060608\n",
      "Epoch: 2, Loss: 0.4456993043422699\n",
      "Epoch: 2, Loss: 0.4950360655784607\n",
      "Epoch: 2, Loss: 0.43325310945510864\n",
      "Epoch: 2, Loss: 0.5163646340370178\n",
      "Epoch: 2, Loss: 0.45830607414245605\n",
      "Epoch: 2, Loss: 0.3957265615463257\n",
      "Epoch: 2, Loss: 0.6289293169975281\n",
      "Epoch: 2, Loss: 0.5223824977874756\n",
      "Epoch: 2, Loss: 0.5258941650390625\n",
      "Epoch: 2, Loss: 0.6446576714515686\n",
      "Epoch: 2, Loss: 0.4307338297367096\n",
      "Epoch: 2, Loss: 0.4576810598373413\n",
      "Epoch: 2, Loss: 0.4848358631134033\n",
      "Epoch: 2, Loss: 0.5486783981323242\n",
      "Epoch: 2, Loss: 0.5405970215797424\n",
      "Epoch: 2, Loss: 0.44288769364356995\n",
      "Epoch: 2, Loss: 0.4701412320137024\n",
      "Epoch: 2, Loss: 0.3780457675457001\n",
      "Epoch: 2, Loss: 0.4623754024505615\n",
      "Epoch: 2, Loss: 0.5184186100959778\n",
      "Epoch: 2, Loss: 0.45931655168533325\n",
      "Epoch: 2, Loss: 0.47643449902534485\n",
      "Epoch: 2, Loss: 0.43713247776031494\n",
      "Epoch: 2, Loss: 0.5132680535316467\n",
      "Epoch: 2, Loss: 0.49090415239334106\n",
      "Epoch: 2, Loss: 0.4371646046638489\n",
      "Epoch: 2, Loss: 0.5637573599815369\n",
      "Epoch: 2, Loss: 0.5241163969039917\n",
      "Epoch: 2, Loss: 0.5232154726982117\n",
      "Epoch: 2, Loss: 0.44941261410713196\n",
      "Epoch: 2, Loss: 0.531649649143219\n",
      "Epoch: 2, Loss: 0.44021230936050415\n",
      "Epoch: 2, Loss: 0.5779674649238586\n",
      "Epoch: 2, Loss: 0.5359700918197632\n",
      "Epoch: 2, Loss: 0.5347220301628113\n",
      "Epoch: 2, Loss: 0.511061429977417\n",
      "Epoch: 2, Loss: 0.48275354504585266\n",
      "Epoch: 2, Loss: 0.5834910273551941\n",
      "Epoch: 2, Loss: 0.44678908586502075\n",
      "Epoch: 2, Loss: 0.5643842220306396\n",
      "Epoch: 2, Loss: 0.5442018508911133\n",
      "Epoch: 2, Loss: 0.4871704578399658\n",
      "Epoch: 2, Loss: 0.46745964884757996\n",
      "Epoch: 2, Loss: 0.47651657462120056\n",
      "Epoch: 2, Loss: 0.6165943741798401\n",
      "Epoch: 2, Loss: 0.5133789777755737\n",
      "Epoch: 2, Loss: 0.4681612551212311\n",
      "Epoch: 2, Loss: 0.5595782995223999\n",
      "Epoch: 2, Loss: 0.4373413026332855\n",
      "Epoch: 2, Loss: 0.45253273844718933\n",
      "Epoch: 2, Loss: 0.41276833415031433\n",
      "Epoch: 2, Loss: 0.43485939502716064\n",
      "Epoch: 2, Loss: 0.48637014627456665\n",
      "Epoch: 2, Loss: 0.565457820892334\n",
      "Epoch: 2, Loss: 0.6488386988639832\n",
      "Epoch: 2, Loss: 0.39546987414360046\n",
      "Epoch: 2, Loss: 0.5210404396057129\n",
      "Epoch: 2, Loss: 0.4399571418762207\n",
      "Epoch: 2, Loss: 0.5281564593315125\n",
      "Epoch: 2, Loss: 0.3964635133743286\n",
      "Epoch: 2, Loss: 0.46156829595565796\n",
      "Epoch: 2, Loss: 0.4698646366596222\n",
      "Epoch: 2, Loss: 0.49028918147087097\n",
      "Epoch: 2, Loss: 0.506142258644104\n",
      "Epoch: 2, Loss: 0.6751054525375366\n",
      "Epoch: 2, Loss: 0.49315717816352844\n",
      "Epoch: 2, Loss: 0.5307493209838867\n",
      "Epoch: 2, Loss: 0.4279956817626953\n",
      "Epoch: 2, Loss: 0.5721916556358337\n",
      "Epoch: 2, Loss: 0.4890875518321991\n",
      "Epoch: 2, Loss: 0.396919846534729\n",
      "Epoch: 2, Loss: 0.42674076557159424\n",
      "Epoch: 2, Loss: 0.5296910405158997\n",
      "Epoch: 2, Loss: 0.5750007629394531\n",
      "Epoch: 2, Loss: 0.5462660789489746\n",
      "Epoch: 2, Loss: 0.45359525084495544\n",
      "Epoch: 2, Loss: 0.5044899582862854\n",
      "Epoch: 2, Loss: 0.5826705694198608\n",
      "Epoch: 2, Loss: 0.4602030813694\n",
      "Epoch: 2, Loss: 0.5230482220649719\n",
      "Epoch: 2, Loss: 0.4875616133213043\n",
      "Epoch: 2, Loss: 0.432095468044281\n",
      "Epoch: 2, Loss: 0.4445687532424927\n",
      "Epoch: 2, Loss: 0.4492136538028717\n",
      "Epoch: 2, Loss: 0.47647804021835327\n",
      "Epoch: 2, Loss: 0.46453940868377686\n",
      "Epoch: 2, Loss: 0.4747779369354248\n",
      "Epoch: 2, Loss: 0.4984724521636963\n",
      "Epoch: 2, Loss: 0.5276458263397217\n",
      "Epoch: 2, Loss: 0.4565627872943878\n",
      "Epoch: 2, Loss: 0.4645940959453583\n",
      "Epoch: 2, Loss: 0.4690719246864319\n",
      "Epoch: 2, Loss: 0.4667609930038452\n",
      "Epoch: 2, Loss: 0.45059460401535034\n",
      "Epoch: 2, Loss: 0.44453945755958557\n",
      "Epoch: 2, Loss: 0.41635745763778687\n",
      "Epoch: 2, Loss: 0.4748597741127014\n",
      "Epoch: 2, Loss: 0.5062718391418457\n",
      "Epoch: 2, Loss: 0.4328889846801758\n",
      "Epoch: 2, Loss: 0.5594465732574463\n",
      "Epoch: 2, Loss: 0.4472932517528534\n",
      "Epoch: 2, Loss: 0.540227472782135\n",
      "Epoch: 2, Loss: 0.44215086102485657\n",
      "Epoch: 2, Loss: 0.4671418070793152\n",
      "Epoch: 2, Loss: 0.44041725993156433\n",
      "Epoch: 2, Loss: 0.432422399520874\n",
      "Epoch: 2, Loss: 0.4596700668334961\n",
      "Epoch: 2, Loss: 0.5129187703132629\n",
      "Epoch: 2, Loss: 0.5315661430358887\n",
      "Epoch: 2, Loss: 0.42659103870391846\n",
      "Epoch: 2, Loss: 0.5074796676635742\n",
      "Epoch: 2, Loss: 0.5832079648971558\n",
      "Epoch: 2, Loss: 0.5317392945289612\n",
      "Epoch: 2, Loss: 0.49337083101272583\n",
      "Epoch: 2, Loss: 0.5525423288345337\n",
      "Epoch: 2, Loss: 0.45374757051467896\n",
      "Epoch: 2, Loss: 0.4402937889099121\n",
      "Epoch: 2, Loss: 0.42462384700775146\n",
      "Epoch: 2, Loss: 0.36417537927627563\n",
      "Epoch: 2, Loss: 0.45415177941322327\n",
      "Epoch: 2, Loss: 0.4953162670135498\n",
      "Epoch: 2, Loss: 0.4555467367172241\n",
      "Epoch: 2, Loss: 0.47250789403915405\n",
      "Epoch: 2, Loss: 0.4580936133861542\n",
      "Epoch: 2, Loss: 0.6141850352287292\n",
      "Epoch: 2, Loss: 0.4592262804508209\n",
      "Epoch: 2, Loss: 0.4708394408226013\n",
      "Epoch: 2, Loss: 0.4611806869506836\n",
      "Epoch: 2, Loss: 0.5202018022537231\n",
      "Epoch: 2, Loss: 0.5366138219833374\n",
      "Epoch: 2, Loss: 0.4945719838142395\n",
      "Epoch: 2, Loss: 0.4720411002635956\n",
      "Epoch: 2, Loss: 0.5983344316482544\n",
      "Epoch: 2, Loss: 0.47741204500198364\n",
      "Epoch: 2, Loss: 0.5278825759887695\n",
      "Epoch: 2, Loss: 0.4867473840713501\n",
      "Epoch: 2, Loss: 0.5129421353340149\n",
      "Epoch: 2, Loss: 0.5698698163032532\n",
      "Epoch: 2, Loss: 0.6287348866462708\n",
      "Epoch: 2, Loss: 0.5944778919219971\n",
      "Epoch: 2, Loss: 0.45725706219673157\n",
      "Epoch: 2, Loss: 0.47166383266448975\n",
      "Epoch: 2, Loss: 0.40799689292907715\n",
      "Epoch: 2, Loss: 0.4812456965446472\n",
      "Epoch: 2, Loss: 0.5213747620582581\n",
      "Epoch: 2, Loss: 0.48740530014038086\n",
      "Epoch: 2, Loss: 0.4528784155845642\n",
      "Epoch: 2, Loss: 0.43627503514289856\n",
      "Epoch: 2, Loss: 0.5568361282348633\n",
      "Epoch: 2, Loss: 0.4269309341907501\n",
      "Epoch: 2, Loss: 0.40303635597229004\n",
      "Epoch: 2, Loss: 0.4597732722759247\n",
      "Epoch: 2, Loss: 0.43269968032836914\n",
      "Epoch: 2, Loss: 0.4289642870426178\n",
      "Epoch: 2, Loss: 0.47670942544937134\n",
      "Epoch: 2, Loss: 0.4961817264556885\n",
      "Epoch: 2, Loss: 0.504332959651947\n",
      "Epoch: 2, Loss: 0.6055228114128113\n",
      "Epoch: 2, Loss: 0.43919116258621216\n",
      "Epoch: 2, Loss: 0.5436453819274902\n",
      "Epoch: 2, Loss: 0.4273412525653839\n",
      "Epoch: 2, Loss: 0.5178964734077454\n",
      "Epoch: 2, Loss: 0.4241706430912018\n",
      "Epoch: 2, Loss: 0.5917085409164429\n",
      "Epoch: 2, Loss: 0.478287011384964\n",
      "Epoch: 2, Loss: 0.44485414028167725\n",
      "Epoch: 2, Loss: 0.45663177967071533\n",
      "Epoch: 2, Loss: 0.495852530002594\n",
      "Epoch: 2, Loss: 0.48241156339645386\n",
      "Epoch: 2, Loss: 0.42506206035614014\n",
      "Epoch: 2, Loss: 0.4140125811100006\n",
      "Epoch: 2, Loss: 0.5525413155555725\n",
      "Epoch: 2, Loss: 0.3943944275379181\n",
      "Epoch: 2, Loss: 0.470086932182312\n",
      "Epoch: 2, Loss: 0.4524661600589752\n",
      "Epoch: 2, Loss: 0.49505963921546936\n",
      "Epoch: 2, Loss: 0.5360546112060547\n",
      "Epoch: 2, Loss: 0.3996654152870178\n",
      "Epoch: 2, Loss: 0.6376501321792603\n",
      "Epoch: 2, Loss: 0.4828440546989441\n",
      "Epoch: 2, Loss: 0.5817870497703552\n",
      "Epoch: 2, Loss: 0.44273295998573303\n",
      "Epoch: 2, Loss: 0.4525453448295593\n",
      "Epoch: 2, Loss: 0.5427883863449097\n",
      "Epoch: 2, Loss: 0.49954503774642944\n",
      "Epoch: 2, Loss: 0.550724983215332\n",
      "Epoch: 2, Loss: 0.5714082717895508\n",
      "Epoch: 2, Loss: 0.40868622064590454\n",
      "Epoch: 2, Loss: 0.39480137825012207\n",
      "Epoch: 2, Loss: 0.5049328207969666\n",
      "Epoch: 2, Loss: 0.4987119734287262\n",
      "Epoch: 2, Loss: 0.4110015630722046\n",
      "Epoch: 2, Loss: 0.5086053609848022\n",
      "Epoch: 2, Loss: 0.4900922179222107\n",
      "Epoch: 2, Loss: 0.5491873025894165\n",
      "Epoch: 2, Loss: 0.40219810605049133\n",
      "Epoch: 2, Loss: 0.3824017345905304\n",
      "Epoch: 2, Loss: 0.46142080426216125\n",
      "Epoch: 2, Loss: 0.42537522315979004\n",
      "Epoch: 2, Loss: 0.46743059158325195\n",
      "Epoch: 2, Loss: 0.38747966289520264\n",
      "Epoch: 2, Loss: 0.4345812499523163\n",
      "Epoch: 2, Loss: 0.47038671374320984\n",
      "Epoch: 2, Loss: 0.44106408953666687\n",
      "Epoch: 2, Loss: 0.4228593111038208\n",
      "Epoch: 2, Loss: 0.5377669334411621\n",
      "Epoch: 2, Loss: 0.49845778942108154\n",
      "Epoch: 2, Loss: 0.4788917005062103\n",
      "Epoch: 2, Loss: 0.4753771424293518\n",
      "Epoch: 2, Loss: 0.44921499490737915\n",
      "Epoch: 2, Loss: 0.506365180015564\n",
      "Epoch: 2, Loss: 0.5540127158164978\n",
      "Epoch: 2, Loss: 0.5155686140060425\n",
      "Epoch: 2, Loss: 0.4866889417171478\n",
      "Epoch: 2, Loss: 0.46903401613235474\n",
      "Epoch: 2, Loss: 0.4862324595451355\n",
      "Epoch: 2, Loss: 0.46476462483406067\n",
      "Epoch: 2, Loss: 0.5722635984420776\n",
      "Epoch: 2, Loss: 0.4623267352581024\n",
      "Epoch: 2, Loss: 0.4808310270309448\n",
      "Epoch: 2, Loss: 0.5187163352966309\n",
      "Epoch: 2, Loss: 0.6228867173194885\n",
      "Epoch: 2, Loss: 0.501721978187561\n",
      "Epoch: 2, Loss: 0.5409599542617798\n",
      "Epoch: 2, Loss: 0.668613612651825\n",
      "Epoch: 2, Loss: 0.39475446939468384\n",
      "Epoch: 2, Loss: 0.4751163125038147\n",
      "Epoch: 2, Loss: 0.42663323879241943\n",
      "Epoch: 2, Loss: 0.45644423365592957\n",
      "Epoch: 2, Loss: 0.531223475933075\n",
      "Epoch: 2, Loss: 0.421805739402771\n",
      "Epoch: 2, Loss: 0.532615602016449\n",
      "Epoch: 2, Loss: 0.507727324962616\n",
      "Epoch: 2, Loss: 0.5642232894897461\n",
      "Epoch: 2, Loss: 0.4185430407524109\n",
      "Epoch: 2, Loss: 0.41205286979675293\n",
      "Epoch: 2, Loss: 0.4810628890991211\n",
      "Epoch: 2, Loss: 0.4935849905014038\n",
      "Epoch: 2, Loss: 0.47193700075149536\n",
      "Epoch: 2, Loss: 0.4456244111061096\n",
      "Epoch: 2, Loss: 0.5098242163658142\n",
      "Epoch: 2, Loss: 0.4696742296218872\n",
      "Epoch: 2, Loss: 0.3729415535926819\n",
      "Epoch: 2, Loss: 0.4486643075942993\n",
      "Epoch: 2, Loss: 0.4106753468513489\n",
      "Epoch: 2, Loss: 0.4701688587665558\n",
      "Epoch: 2, Loss: 0.5735546946525574\n",
      "Epoch: 2, Loss: 0.4987085461616516\n",
      "Epoch: 2, Loss: 0.42308735847473145\n",
      "Epoch: 2, Loss: 0.512380838394165\n",
      "Epoch: 2, Loss: 0.4635361135005951\n",
      "Epoch: 2, Loss: 0.46866533160209656\n",
      "Epoch: 2, Loss: 0.5110094547271729\n",
      "Epoch: 2, Loss: 0.45157840847969055\n",
      "Epoch: 2, Loss: 0.6207746267318726\n",
      "Epoch: 2, Loss: 0.4397140443325043\n",
      "Epoch: 2, Loss: 0.39457637071609497\n",
      "Epoch: 2, Loss: 0.5051161646842957\n",
      "Epoch: 2, Loss: 0.5326622724533081\n",
      "Epoch: 2, Loss: 0.490233451128006\n",
      "Epoch: 2, Loss: 0.559932291507721\n",
      "Epoch: 2, Loss: 0.4934650659561157\n",
      "Epoch: 2, Loss: 0.37557747960090637\n",
      "Epoch: 2, Loss: 0.4896959364414215\n",
      "Epoch: 2, Loss: 0.4856627285480499\n",
      "Epoch: 2, Loss: 0.4250422716140747\n",
      "Epoch: 2, Loss: 0.4889708161354065\n",
      "Epoch: 2, Loss: 0.40264788269996643\n",
      "Epoch: 2, Loss: 0.4597679376602173\n",
      "Epoch: 2, Loss: 0.4564497470855713\n",
      "Epoch: 2, Loss: 0.40515419840812683\n",
      "Epoch: 2, Loss: 0.3723469376564026\n",
      "Epoch: 2, Loss: 0.4282691776752472\n",
      "Epoch: 2, Loss: 0.49179279804229736\n",
      "Epoch: 2, Loss: 0.5335005521774292\n",
      "Epoch: 2, Loss: 0.4630155563354492\n",
      "Epoch: 2, Loss: 0.4414353370666504\n",
      "Epoch: 2, Loss: 0.4026949405670166\n",
      "Epoch: 2, Loss: 0.45109739899635315\n",
      "Epoch: 2, Loss: 0.4165470600128174\n",
      "Epoch: 2, Loss: 0.40580815076828003\n",
      "Epoch: 2, Loss: 0.555059552192688\n",
      "Epoch: 2, Loss: 0.4684256315231323\n",
      "Epoch: 2, Loss: 0.42167773842811584\n",
      "Epoch: 2, Loss: 0.5016816258430481\n",
      "Epoch: 2, Loss: 0.4361133277416229\n",
      "Epoch: 2, Loss: 0.48180556297302246\n",
      "Epoch: 2, Loss: 0.40026652812957764\n",
      "Epoch: 2, Loss: 0.49876701831817627\n",
      "Epoch: 2, Loss: 0.4608162045478821\n",
      "Epoch: 2, Loss: 0.6066362261772156\n",
      "Epoch: 2, Loss: 0.5369601249694824\n",
      "Epoch: 2, Loss: 0.5672304630279541\n",
      "Epoch: 2, Loss: 0.4631243348121643\n",
      "Epoch: 2, Loss: 0.5136293172836304\n",
      "Epoch: 2, Loss: 0.4826403260231018\n",
      "Epoch: 2, Loss: 0.5228719711303711\n",
      "Epoch: 2, Loss: 0.5259878635406494\n",
      "Epoch: 2, Loss: 0.5419535040855408\n",
      "Epoch: 2, Loss: 0.40187549591064453\n",
      "Epoch: 2, Loss: 0.40370139479637146\n",
      "Epoch: 2, Loss: 0.5220882892608643\n",
      "Epoch: 2, Loss: 0.5088047385215759\n",
      "Epoch: 2, Loss: 0.5787829756736755\n",
      "Epoch: 2, Loss: 0.44528648257255554\n",
      "Epoch: 2, Loss: 0.41992199420928955\n",
      "Epoch: 2, Loss: 0.5379798412322998\n",
      "Epoch: 2, Loss: 0.4194950461387634\n",
      "Epoch: 2, Loss: 0.5006973743438721\n",
      "Epoch: 2, Loss: 0.39597317576408386\n",
      "Epoch: 2, Loss: 0.47178420424461365\n",
      "Epoch: 2, Loss: 0.457589328289032\n",
      "Epoch: 2, Loss: 0.48205357789993286\n",
      "Epoch: 2, Loss: 0.46303683519363403\n",
      "Epoch: 2, Loss: 0.41566893458366394\n",
      "Epoch: 2, Loss: 0.4562654495239258\n",
      "Epoch: 2, Loss: 0.4382236897945404\n",
      "Epoch: 2, Loss: 0.5558941960334778\n",
      "Epoch: 2, Loss: 0.5755856037139893\n",
      "Epoch: 2, Loss: 0.6012151837348938\n",
      "Epoch: 2, Loss: 0.42750313878059387\n",
      "Epoch: 2, Loss: 0.4690611660480499\n",
      "Epoch: 2, Loss: 0.47031331062316895\n",
      "Epoch: 2, Loss: 0.42688387632369995\n",
      "Epoch: 2, Loss: 0.558049201965332\n",
      "Epoch: 2, Loss: 0.5045703649520874\n",
      "Epoch: 2, Loss: 0.6330084800720215\n",
      "Epoch: 2, Loss: 0.5136895775794983\n",
      "Epoch: 2, Loss: 0.4681156277656555\n",
      "Epoch: 2, Loss: 0.4983859956264496\n",
      "Epoch: 2, Loss: 0.5369296669960022\n",
      "Epoch: 2, Loss: 0.4675092399120331\n",
      "Epoch: 2, Loss: 0.517177939414978\n",
      "Epoch: 2, Loss: 0.4990886449813843\n",
      "Epoch: 2, Loss: 0.4621601402759552\n",
      "Epoch: 2, Loss: 0.5146074891090393\n",
      "Epoch: 2, Loss: 0.6791755557060242\n",
      "Epoch: 2, Loss: 0.4758005738258362\n",
      "Epoch: 2, Loss: 0.503896176815033\n",
      "Epoch: 2, Loss: 0.43661338090896606\n",
      "Epoch: 2, Loss: 0.4636942148208618\n",
      "Epoch: 2, Loss: 0.46711915731430054\n",
      "Epoch: 2, Loss: 0.5217493772506714\n",
      "Epoch: 2, Loss: 0.5244865417480469\n",
      "Epoch: 2, Loss: 0.39604079723358154\n",
      "Epoch: 2, Loss: 0.5789859294891357\n",
      "Epoch: 2, Loss: 0.5040895342826843\n",
      "Epoch: 2, Loss: 0.451834112405777\n",
      "Epoch: 2, Loss: 0.46528178453445435\n",
      "Epoch: 2, Loss: 0.4202106297016144\n",
      "Epoch: 2, Loss: 0.5201424956321716\n",
      "Epoch: 2, Loss: 0.5219528079032898\n",
      "Epoch: 2, Loss: 0.37311026453971863\n",
      "Epoch: 2, Loss: 0.3776428997516632\n",
      "Epoch: 2, Loss: 0.5575905442237854\n",
      "Epoch: 2, Loss: 0.41349509358406067\n",
      "Epoch: 2, Loss: 0.42449504137039185\n",
      "Epoch: 2, Loss: 0.6235656142234802\n",
      "Epoch: 2, Loss: 0.5045366287231445\n",
      "Epoch: 2, Loss: 0.5270159840583801\n",
      "Epoch: 2, Loss: 0.44408467411994934\n",
      "Epoch: 2, Loss: 0.44115638732910156\n",
      "Epoch: 2, Loss: 0.48514795303344727\n",
      "Epoch: 2, Loss: 0.5569940805435181\n",
      "Epoch: 2, Loss: 0.4716477692127228\n",
      "Epoch: 2, Loss: 0.48298242688179016\n",
      "Epoch: 2, Loss: 0.5293058753013611\n",
      "Epoch: 2, Loss: 0.45131146907806396\n",
      "Epoch: 2, Loss: 0.5831684470176697\n",
      "Epoch: 2, Loss: 0.6094093918800354\n",
      "Epoch: 2, Loss: 0.4380512535572052\n",
      "Epoch: 2, Loss: 0.47483474016189575\n",
      "Epoch: 2, Loss: 0.4949953258037567\n",
      "Epoch: 2, Loss: 0.47198939323425293\n",
      "Epoch: 2, Loss: 0.46806126832962036\n",
      "Epoch: 2, Loss: 0.526611864566803\n",
      "Epoch: 2, Loss: 0.38129961490631104\n",
      "Epoch: 2, Loss: 0.6313732266426086\n",
      "Epoch: 2, Loss: 0.5269776582717896\n",
      "Epoch: 2, Loss: 0.46233832836151123\n",
      "Epoch: 2, Loss: 0.6425786018371582\n",
      "Epoch: 2, Loss: 0.5499978065490723\n",
      "Epoch: 2, Loss: 0.6701745986938477\n",
      "Epoch: 2, Loss: 0.4883861541748047\n",
      "Epoch: 2, Loss: 0.574257493019104\n",
      "Epoch: 2, Loss: 0.5841469168663025\n",
      "Epoch: 2, Loss: 0.4225008189678192\n",
      "Epoch: 2, Loss: 0.47721678018569946\n",
      "Epoch: 2, Loss: 0.40902549028396606\n",
      "Epoch: 2, Loss: 0.5439286828041077\n",
      "Epoch: 2, Loss: 0.4458053410053253\n",
      "Epoch: 2, Loss: 0.6777022480964661\n",
      "Epoch: 2, Loss: 0.47213298082351685\n",
      "Epoch: 2, Loss: 0.5436893701553345\n",
      "Epoch: 2, Loss: 0.4739585518836975\n",
      "Epoch: 2, Loss: 0.5053713321685791\n",
      "Epoch: 2, Loss: 0.501545786857605\n",
      "Epoch: 2, Loss: 0.43872666358947754\n",
      "Epoch: 2, Loss: 0.582570493221283\n",
      "Epoch: 2, Loss: 0.5271639823913574\n",
      "Epoch: 2, Loss: 0.47371599078178406\n",
      "Epoch: 2, Loss: 0.45504292845726013\n",
      "Epoch: 2, Loss: 0.496113121509552\n",
      "Epoch: 2, Loss: 0.48228195309638977\n",
      "Epoch: 2, Loss: 0.5486001372337341\n",
      "Epoch: 2, Loss: 0.49223825335502625\n",
      "Epoch: 2, Loss: 0.47745418548583984\n",
      "Epoch: 2, Loss: 0.5563206672668457\n",
      "Epoch: 2, Loss: 0.45501554012298584\n",
      "Epoch: 2, Loss: 0.4229962229728699\n",
      "Epoch: 2, Loss: 0.4618940055370331\n",
      "Epoch: 2, Loss: 0.5510053634643555\n",
      "Epoch: 2, Loss: 0.40375933051109314\n",
      "Epoch: 2, Loss: 0.45859822630882263\n",
      "Epoch: 2, Loss: 0.5730243921279907\n",
      "Epoch: 2, Loss: 0.46946948766708374\n",
      "Epoch: 2, Loss: 0.4814712703227997\n",
      "Epoch: 2, Loss: 0.47224152088165283\n",
      "Epoch: 2, Loss: 0.47920501232147217\n",
      "Epoch: 2, Loss: 0.38688477873802185\n",
      "Epoch: 2, Loss: 0.5576074719429016\n",
      "Epoch: 2, Loss: 0.4161795973777771\n",
      "Epoch: 2, Loss: 0.6108090281486511\n",
      "Epoch: 2, Loss: 0.5141474008560181\n",
      "Epoch: 2, Loss: 0.5203037261962891\n",
      "Epoch: 2, Loss: 0.4372716546058655\n",
      "Epoch: 2, Loss: 0.5666041374206543\n",
      "Epoch: 2, Loss: 0.45024174451828003\n",
      "Epoch: 2, Loss: 0.4481104016304016\n",
      "Epoch: 2, Loss: 0.470491886138916\n",
      "Epoch: 2, Loss: 0.529971718788147\n",
      "Epoch: 2, Loss: 0.38949498534202576\n",
      "Epoch: 2, Loss: 0.516309380531311\n",
      "Epoch: 2, Loss: 0.4277074635028839\n",
      "Epoch: 2, Loss: 0.38687413930892944\n",
      "Epoch: 2, Loss: 0.4858768880367279\n",
      "Epoch: 2, Loss: 0.46360325813293457\n",
      "Epoch: 2, Loss: 0.6417824029922485\n",
      "Epoch: 2, Loss: 0.4844103455543518\n",
      "Epoch: 2, Loss: 0.6077724099159241\n",
      "Epoch: 2, Loss: 0.45656073093414307\n",
      "Epoch: 2, Loss: 0.5110010504722595\n",
      "Epoch: 2, Loss: 0.4050503373146057\n",
      "Epoch: 2, Loss: 0.5314334630966187\n",
      "Epoch: 2, Loss: 0.40981999039649963\n",
      "Epoch: 2, Loss: 0.6007943749427795\n",
      "Epoch: 2, Loss: 0.44687026739120483\n",
      "Epoch: 2, Loss: 0.5616962909698486\n",
      "Epoch: 2, Loss: 0.4596216678619385\n",
      "Epoch: 2, Loss: 0.4420957565307617\n",
      "Epoch: 2, Loss: 0.5517539381980896\n",
      "Epoch: 2, Loss: 0.46606966853141785\n",
      "Epoch: 2, Loss: 0.4767284095287323\n",
      "Epoch: 2, Loss: 0.4845390021800995\n",
      "Epoch: 2, Loss: 0.6232190132141113\n",
      "Epoch: 2, Loss: 0.4490703344345093\n",
      "Epoch: 2, Loss: 0.42965224385261536\n",
      "Epoch: 2, Loss: 0.46483370661735535\n",
      "Epoch: 2, Loss: 0.5037816762924194\n",
      "Epoch: 2, Loss: 0.5207857489585876\n",
      "Epoch: 2, Loss: 0.5099563598632812\n",
      "Epoch: 2, Loss: 0.40510183572769165\n",
      "Epoch: 2, Loss: 0.4511663317680359\n",
      "Epoch: 2, Loss: 0.5558292865753174\n",
      "Epoch: 2, Loss: 0.48580560088157654\n",
      "Epoch: 2, Loss: 0.5872913002967834\n",
      "Epoch: 2, Loss: 0.4542359709739685\n",
      "Epoch: 2, Loss: 0.4624600112438202\n",
      "Epoch: 2, Loss: 0.46963509917259216\n",
      "Epoch: 2, Loss: 0.47202038764953613\n",
      "Epoch: 2, Loss: 0.48382753133773804\n",
      "Epoch: 2, Loss: 0.4068283140659332\n",
      "Epoch: 2, Loss: 0.49156808853149414\n",
      "Epoch: 2, Loss: 0.4663374423980713\n",
      "Epoch: 2, Loss: 0.5340477228164673\n",
      "Epoch: 2, Loss: 0.48351621627807617\n",
      "Epoch: 2, Loss: 0.530463457107544\n",
      "Epoch: 2, Loss: 0.4167769253253937\n",
      "Epoch: 2, Loss: 0.45460325479507446\n",
      "Epoch: 2, Loss: 0.44723641872406006\n",
      "Epoch: 2, Loss: 0.45658281445503235\n",
      "Epoch: 2, Loss: 0.4001157283782959\n",
      "Epoch: 2, Loss: 0.4758763313293457\n",
      "Epoch: 2, Loss: 0.4869258999824524\n",
      "Epoch: 2, Loss: 0.4056764245033264\n",
      "Epoch: 2, Loss: 0.48863673210144043\n",
      "Epoch: 2, Loss: 0.560722291469574\n",
      "Epoch: 2, Loss: 0.5452513694763184\n",
      "Epoch: 2, Loss: 0.5327765345573425\n",
      "Epoch: 2, Loss: 0.4490026831626892\n",
      "Epoch: 2, Loss: 0.45601797103881836\n",
      "Epoch: 2, Loss: 0.7156165838241577\n",
      "Epoch: 2, Loss: 0.3963514566421509\n",
      "Epoch: 2, Loss: 0.46133944392204285\n",
      "Epoch: 2, Loss: 0.5426688194274902\n",
      "Epoch: 2, Loss: 0.5817610025405884\n",
      "Epoch: 2, Loss: 0.4393763244152069\n",
      "Epoch: 2, Loss: 0.4104061722755432\n",
      "Epoch: 2, Loss: 0.46073225140571594\n",
      "Epoch: 2, Loss: 0.4661775529384613\n",
      "Epoch: 2, Loss: 0.443636417388916\n",
      "Epoch: 2, Loss: 0.46675431728363037\n",
      "Epoch: 2, Loss: 0.5279819369316101\n",
      "Epoch: 2, Loss: 0.5125089287757874\n",
      "Epoch: 2, Loss: 0.459930419921875\n",
      "Epoch: 2, Loss: 0.5110944509506226\n",
      "Epoch: 2, Loss: 0.4248252511024475\n",
      "Epoch: 2, Loss: 0.529130220413208\n",
      "Epoch: 2, Loss: 0.5786129236221313\n",
      "Epoch: 2, Loss: 0.4164007902145386\n",
      "Epoch: 2, Loss: 0.3936265707015991\n",
      "Epoch: 2, Loss: 0.4976210594177246\n",
      "Epoch: 2, Loss: 0.5518892407417297\n",
      "Epoch: 2, Loss: 0.5006619095802307\n",
      "Epoch: 2, Loss: 0.5923159122467041\n",
      "Epoch: 2, Loss: 0.45219507813453674\n",
      "Epoch: 2, Loss: 0.5667280554771423\n",
      "Epoch: 2, Loss: 0.4785354733467102\n",
      "Epoch: 2, Loss: 0.6684108972549438\n",
      "Epoch: 2, Loss: 0.45813512802124023\n",
      "Epoch: 2, Loss: 0.5703644752502441\n",
      "Epoch: 2, Loss: 0.5025317668914795\n",
      "Epoch: 2, Loss: 0.4117857813835144\n",
      "Epoch: 2, Loss: 0.46443143486976624\n",
      "Epoch: 2, Loss: 0.4058486819267273\n",
      "Epoch: 2, Loss: 0.5119675397872925\n",
      "Epoch: 2, Loss: 0.5098035335540771\n",
      "Epoch: 2, Loss: 0.45788636803627014\n",
      "Epoch: 2, Loss: 0.5160034894943237\n",
      "Epoch: 2, Loss: 0.5089335441589355\n",
      "Epoch: 2, Loss: 0.4681386351585388\n",
      "Epoch: 2, Loss: 0.4200786054134369\n",
      "Epoch: 2, Loss: 0.5061188340187073\n",
      "Epoch: 2, Loss: 0.5768962502479553\n",
      "Epoch: 2, Loss: 0.4526515305042267\n",
      "Epoch: 2, Loss: 0.4591056704521179\n",
      "Epoch: 2, Loss: 0.5388875007629395\n",
      "Epoch: 2, Loss: 0.532925009727478\n",
      "Epoch: 2, Loss: 0.6359217166900635\n",
      "Epoch: 2, Loss: 0.45990508794784546\n",
      "Epoch: 2, Loss: 0.4174072742462158\n",
      "Epoch: 2, Loss: 0.4772326350212097\n",
      "Epoch: 2, Loss: 0.5420958399772644\n",
      "Epoch: 2, Loss: 0.4270797073841095\n",
      "Epoch: 2, Loss: 0.48351526260375977\n",
      "Epoch: 2, Loss: 0.41969558596611023\n",
      "Epoch: 2, Loss: 0.4799392521381378\n",
      "Epoch: 2, Loss: 0.6456257700920105\n",
      "Epoch: 2, Loss: 0.3865703046321869\n",
      "Epoch: 2, Loss: 0.4491318166255951\n",
      "Epoch: 2, Loss: 0.5562353730201721\n",
      "Epoch: 2, Loss: 0.5926544666290283\n",
      "Epoch: 2, Loss: 0.42197009921073914\n",
      "Epoch: 2, Loss: 0.4373469352722168\n",
      "Epoch: 2, Loss: 0.4010659158229828\n",
      "Epoch: 2, Loss: 0.3731876611709595\n",
      "Epoch: 2, Loss: 0.4921072721481323\n",
      "Epoch: 2, Loss: 0.40057575702667236\n",
      "Epoch: 2, Loss: 0.40774068236351013\n",
      "Epoch: 2, Loss: 0.43558165431022644\n",
      "Epoch: 2, Loss: 0.48207715153694153\n",
      "Epoch: 2, Loss: 0.5126572251319885\n",
      "Epoch: 2, Loss: 0.3965754508972168\n",
      "Epoch: 2, Loss: 0.40480655431747437\n",
      "Epoch: 2, Loss: 0.4138125777244568\n",
      "Epoch: 2, Loss: 0.4180392920970917\n",
      "Epoch: 2, Loss: 0.5338108539581299\n",
      "Epoch: 2, Loss: 0.5601130723953247\n",
      "Epoch: 2, Loss: 0.47609034180641174\n",
      "Epoch: 2, Loss: 0.46292242407798767\n",
      "Epoch: 2, Loss: 0.44853726029396057\n",
      "Epoch: 2, Loss: 0.4333976209163666\n",
      "Epoch: 2, Loss: 0.4357096552848816\n",
      "Epoch: 2, Loss: 0.4173588454723358\n",
      "Epoch: 2, Loss: 0.4332129955291748\n",
      "Epoch: 2, Loss: 0.5461883544921875\n",
      "Epoch: 2, Loss: 0.5163559317588806\n",
      "Epoch: 2, Loss: 0.4760853052139282\n",
      "Epoch: 2, Loss: 0.5430092811584473\n",
      "Epoch: 2, Loss: 0.4443287253379822\n",
      "Epoch: 2, Loss: 0.5324842929840088\n",
      "Epoch: 2, Loss: 0.5744186639785767\n",
      "Epoch: 2, Loss: 0.46245893836021423\n",
      "Epoch: 2, Loss: 0.4146438539028168\n",
      "Epoch: 2, Loss: 0.4305085837841034\n",
      "Epoch: 2, Loss: 0.44874170422554016\n",
      "Epoch: 2, Loss: 0.5589402318000793\n",
      "Epoch: 2, Loss: 0.5359691381454468\n",
      "Epoch: 2, Loss: 0.47095876932144165\n",
      "Epoch: 2, Loss: 0.5521949529647827\n",
      "Epoch: 2, Loss: 0.4482567012310028\n",
      "Epoch: 2, Loss: 0.4471021294593811\n",
      "Epoch: 2, Loss: 0.6893670558929443\n",
      "Epoch: 2, Loss: 0.5512701272964478\n",
      "Epoch: 2, Loss: 0.4401775896549225\n",
      "Epoch: 2, Loss: 0.47540175914764404\n",
      "Epoch: 2, Loss: 0.5851419568061829\n",
      "Epoch: 2, Loss: 0.5546988248825073\n",
      "Epoch: 2, Loss: 0.42211541533470154\n",
      "Epoch: 2, Loss: 0.4381980895996094\n",
      "Epoch: 2, Loss: 0.4535643756389618\n",
      "Epoch: 2, Loss: 0.41810300946235657\n",
      "Epoch: 2, Loss: 0.4124957025051117\n",
      "Epoch: 2, Loss: 0.4884813725948334\n",
      "Epoch: 2, Loss: 0.45200738310813904\n",
      "Epoch: 2, Loss: 0.5280898809432983\n",
      "Epoch: 2, Loss: 0.4669918417930603\n",
      "Epoch: 2, Loss: 0.45794549584388733\n",
      "Epoch: 2, Loss: 0.5931548476219177\n",
      "Epoch: 2, Loss: 0.44323644042015076\n",
      "Epoch: 2, Loss: 0.503341555595398\n",
      "Epoch: 2, Loss: 0.48372018337249756\n",
      "Epoch: 2, Loss: 0.5809980630874634\n",
      "Epoch: 2, Loss: 0.5454199314117432\n",
      "Epoch: 2, Loss: 0.5132439136505127\n",
      "Epoch: 2, Loss: 0.5609893798828125\n",
      "Epoch: 2, Loss: 0.5241578221321106\n",
      "Epoch: 2, Loss: 0.3792760968208313\n",
      "Epoch: 2, Loss: 0.4269147515296936\n",
      "Epoch: 2, Loss: 0.41267114877700806\n",
      "Epoch: 2, Loss: 0.3799426555633545\n",
      "Epoch: 2, Loss: 0.5494161248207092\n",
      "Epoch: 2, Loss: 0.4112861752510071\n",
      "Epoch: 2, Loss: 0.5782703161239624\n",
      "Epoch: 2, Loss: 0.46678245067596436\n",
      "Epoch: 2, Loss: 0.5213114619255066\n",
      "Epoch: 2, Loss: 0.47795647382736206\n",
      "Epoch: 2, Loss: 0.47199195623397827\n",
      "Epoch: 2, Loss: 0.4734393358230591\n",
      "Epoch: 2, Loss: 0.38697853684425354\n",
      "Epoch: 2, Loss: 0.46839019656181335\n",
      "Epoch: 2, Loss: 0.6507033109664917\n",
      "Epoch: 2, Loss: 0.45487380027770996\n",
      "Epoch: 2, Loss: 0.5405334234237671\n",
      "Epoch: 2, Loss: 0.5370145440101624\n",
      "Epoch: 2, Loss: 0.45584210753440857\n",
      "Epoch: 2, Loss: 0.5550154447555542\n",
      "Epoch: 2, Loss: 0.39402538537979126\n",
      "Epoch: 2, Loss: 0.49535030126571655\n",
      "Epoch: 2, Loss: 0.4630453288555145\n",
      "Epoch: 2, Loss: 0.4395521879196167\n",
      "Epoch: 2, Loss: 0.510116457939148\n",
      "Epoch: 2, Loss: 0.41910266876220703\n",
      "Epoch: 2, Loss: 0.4458538889884949\n",
      "Epoch: 2, Loss: 0.5152308344841003\n",
      "Epoch: 2, Loss: 0.43003329634666443\n",
      "Epoch: 2, Loss: 0.4187431037425995\n",
      "Epoch: 2, Loss: 0.4615662097930908\n",
      "Epoch: 2, Loss: 0.6533973217010498\n",
      "Epoch: 2, Loss: 0.5229631066322327\n",
      "Epoch: 2, Loss: 0.49198806285858154\n",
      "Epoch: 2, Loss: 0.5186479091644287\n",
      "Epoch: 2, Loss: 0.4848361611366272\n",
      "Epoch: 2, Loss: 0.46656882762908936\n",
      "Epoch: 2, Loss: 0.5184658765792847\n",
      "Epoch: 2, Loss: 0.4731222093105316\n",
      "Epoch: 2, Loss: 0.42120683193206787\n",
      "Epoch: 2, Loss: 0.5293715000152588\n",
      "Epoch: 2, Loss: 0.43159714341163635\n",
      "Epoch: 2, Loss: 0.5289269089698792\n",
      "Epoch: 2, Loss: 0.427090585231781\n",
      "Epoch: 2, Loss: 0.5041858553886414\n",
      "Epoch: 2, Loss: 0.5002977848052979\n",
      "Epoch: 2, Loss: 0.47479090094566345\n",
      "Epoch: 2, Loss: 0.5760759115219116\n",
      "Epoch: 2, Loss: 0.435447633266449\n",
      "Epoch: 2, Loss: 0.5163349509239197\n",
      "Epoch: 2, Loss: 0.45750439167022705\n",
      "Epoch: 2, Loss: 0.43792521953582764\n",
      "Epoch: 2, Loss: 0.50040203332901\n",
      "Epoch: 2, Loss: 0.5662626624107361\n",
      "Epoch: 2, Loss: 0.5863528251647949\n",
      "Epoch: 2, Loss: 0.4701683521270752\n",
      "Epoch: 2, Loss: 0.3899952471256256\n",
      "Epoch: 2, Loss: 0.531514048576355\n",
      "Epoch: 2, Loss: 0.38997092843055725\n",
      "Epoch: 2, Loss: 0.5564703941345215\n",
      "Epoch: 2, Loss: 0.4744446277618408\n",
      "Epoch: 2, Loss: 0.46366387605667114\n",
      "Epoch: 2, Loss: 0.3807487487792969\n",
      "Epoch: 2, Loss: 0.5570335388183594\n",
      "Epoch: 2, Loss: 0.46474507451057434\n",
      "Epoch: 2, Loss: 0.5107299089431763\n",
      "Epoch: 2, Loss: 0.47969499230384827\n",
      "Epoch: 2, Loss: 0.5581793785095215\n",
      "Epoch: 2, Loss: 0.46066540479660034\n",
      "Epoch: 2, Loss: 0.5294643640518188\n",
      "Epoch: 2, Loss: 0.501644492149353\n",
      "Epoch: 2, Loss: 0.43307292461395264\n",
      "Epoch: 2, Loss: 0.5567706823348999\n",
      "Epoch: 2, Loss: 0.45851296186447144\n",
      "Epoch: 2, Loss: 0.510347843170166\n",
      "Epoch: 2, Loss: 0.5300099849700928\n",
      "Epoch: 2, Loss: 0.44474971294403076\n",
      "Epoch: 2, Loss: 0.4882095456123352\n",
      "Epoch: 2, Loss: 0.525894820690155\n",
      "Epoch: 2, Loss: 0.40808019042015076\n",
      "Epoch: 2, Loss: 0.5905061364173889\n",
      "Epoch: 2, Loss: 0.4735296666622162\n",
      "Epoch: 2, Loss: 0.43002840876579285\n",
      "Epoch: 2, Loss: 0.5156934261322021\n",
      "Epoch: 2, Loss: 0.5131682753562927\n",
      "Epoch: 2, Loss: 0.4825647473335266\n",
      "Epoch: 2, Loss: 0.49910271167755127\n",
      "Epoch: 2, Loss: 0.44373056292533875\n",
      "Epoch: 2, Loss: 0.5267690420150757\n",
      "Epoch: 2, Loss: 0.4599750339984894\n",
      "Epoch: 2, Loss: 0.3762219548225403\n",
      "Epoch: 2, Loss: 0.49167269468307495\n",
      "Epoch: 2, Loss: 0.4500412940979004\n",
      "Epoch: 2, Loss: 0.4091215133666992\n",
      "Epoch: 2, Loss: 0.5800434947013855\n",
      "Epoch: 2, Loss: 0.39219436049461365\n",
      "Epoch: 2, Loss: 0.5620198249816895\n",
      "Epoch: 2, Loss: 0.45161280035972595\n",
      "Epoch: 2, Loss: 0.5035519003868103\n",
      "Epoch: 2, Loss: 0.43642234802246094\n",
      "Epoch: 2, Loss: 0.5316222906112671\n",
      "Epoch: 2, Loss: 0.44462674856185913\n",
      "Epoch: 2, Loss: 0.6062619686126709\n",
      "Epoch: 2, Loss: 0.39409491419792175\n",
      "Epoch: 2, Loss: 0.42413657903671265\n",
      "Epoch: 2, Loss: 0.3764071464538574\n",
      "Epoch: 2, Loss: 0.3767901659011841\n",
      "Epoch: 2, Loss: 0.5084623098373413\n",
      "Epoch: 2, Loss: 0.48436641693115234\n",
      "Epoch: 2, Loss: 0.5029381513595581\n",
      "Epoch: 2, Loss: 0.45784950256347656\n",
      "Epoch: 2, Loss: 0.5175949335098267\n",
      "Epoch: 2, Loss: 0.6299795508384705\n",
      "Epoch: 2, Loss: 0.4138672947883606\n",
      "Epoch: 2, Loss: 0.4749857783317566\n",
      "Epoch: 2, Loss: 0.46504339575767517\n",
      "Epoch: 2, Loss: 0.4259627163410187\n",
      "Epoch: 2, Loss: 0.38312217593193054\n",
      "Epoch: 2, Loss: 0.5210182070732117\n",
      "Epoch: 2, Loss: 0.5320168733596802\n",
      "Epoch: 2, Loss: 0.5688773393630981\n",
      "Epoch: 2, Loss: 0.6028643250465393\n",
      "Epoch: 2, Loss: 0.4947267174720764\n",
      "Epoch: 2, Loss: 0.39577555656433105\n",
      "Epoch: 2, Loss: 0.5475431084632874\n",
      "Epoch: 2, Loss: 0.5575466156005859\n",
      "Epoch: 2, Loss: 0.45995667576789856\n",
      "Epoch: 2, Loss: 0.42987439036369324\n",
      "Epoch: 2, Loss: 0.4026898145675659\n",
      "Epoch: 2, Loss: 0.5610479116439819\n",
      "Epoch: 2, Loss: 0.4655578136444092\n",
      "Epoch: 2, Loss: 0.4620695114135742\n",
      "Epoch: 2, Loss: 0.5252104997634888\n",
      "Epoch: 2, Loss: 0.5135497450828552\n",
      "Epoch: 2, Loss: 0.5623592138290405\n",
      "Epoch: 2, Loss: 0.3897923231124878\n",
      "Epoch: 2, Loss: 0.5990356802940369\n",
      "Epoch: 2, Loss: 0.48399484157562256\n",
      "Epoch: 2, Loss: 0.531787097454071\n",
      "Epoch: 2, Loss: 0.5171619057655334\n",
      "Epoch: 2, Loss: 0.4752505123615265\n",
      "Epoch: 2, Loss: 0.5723589658737183\n",
      "Epoch: 2, Loss: 0.5715164542198181\n",
      "Epoch: 2, Loss: 0.38019347190856934\n",
      "Epoch: 2, Loss: 0.48183202743530273\n",
      "Epoch: 2, Loss: 0.39361056685447693\n",
      "Epoch: 2, Loss: 0.41620755195617676\n",
      "Epoch: 2, Loss: 0.4883624315261841\n",
      "Epoch: 2, Loss: 0.40160876512527466\n",
      "Epoch: 2, Loss: 0.5352141857147217\n",
      "Epoch: 2, Loss: 0.4755040109157562\n",
      "Epoch: 2, Loss: 0.4289645552635193\n",
      "Epoch: 2, Loss: 0.4641999900341034\n",
      "Epoch: 2, Loss: 0.5753955841064453\n",
      "Epoch: 2, Loss: 0.535463273525238\n",
      "Epoch: 2, Loss: 0.3870526850223541\n",
      "Epoch: 2, Loss: 0.4584603011608124\n",
      "Epoch: 2, Loss: 0.5190989971160889\n",
      "Epoch: 2, Loss: 0.5551584959030151\n",
      "Epoch: 2, Loss: 0.5477559566497803\n",
      "Epoch: 2, Loss: 0.48480692505836487\n",
      "Epoch: 2, Loss: 0.4300605058670044\n",
      "Epoch: 2, Loss: 0.47449183464050293\n",
      "Epoch: 2, Loss: 0.5134019255638123\n",
      "Epoch: 2, Loss: 0.4646652936935425\n",
      "Epoch: 2, Loss: 0.4631153345108032\n",
      "Epoch: 2, Loss: 0.4777987599372864\n",
      "Epoch: 2, Loss: 0.49063438177108765\n",
      "Epoch: 2, Loss: 0.514672577381134\n",
      "Epoch: 2, Loss: 0.5112432837486267\n",
      "Epoch: 2, Loss: 0.5113050937652588\n",
      "Epoch: 2, Loss: 0.39582693576812744\n",
      "Epoch: 2, Loss: 0.5882017612457275\n",
      "Epoch: 2, Loss: 0.49731218814849854\n",
      "Epoch: 2, Loss: 0.5531442165374756\n",
      "Epoch: 2, Loss: 0.4063451886177063\n",
      "Epoch: 2, Loss: 0.5510358810424805\n",
      "Epoch: 2, Loss: 0.532289445400238\n",
      "Epoch: 2, Loss: 0.42806461453437805\n",
      "Epoch: 2, Loss: 0.4256437122821808\n",
      "Epoch: 2, Loss: 0.3963908851146698\n",
      "Epoch: 2, Loss: 0.45965784788131714\n",
      "Epoch: 2, Loss: 0.42263901233673096\n",
      "Epoch: 2, Loss: 0.5254936814308167\n",
      "Epoch: 2, Loss: 0.546210527420044\n",
      "Epoch: 2, Loss: 0.4624795615673065\n",
      "Epoch: 2, Loss: 0.3916303217411041\n",
      "Epoch: 2, Loss: 0.45452767610549927\n",
      "Epoch: 2, Loss: 0.4878225028514862\n",
      "Epoch: 2, Loss: 0.5722686648368835\n",
      "Epoch: 2, Loss: 0.5261049270629883\n",
      "Epoch: 2, Loss: 0.5056619644165039\n",
      "Epoch: 2, Loss: 0.4794110059738159\n",
      "Epoch: 2, Loss: 0.4642147421836853\n",
      "Epoch: 2, Loss: 0.47081905603408813\n",
      "Epoch: 2, Loss: 0.4119746685028076\n",
      "Epoch: 2, Loss: 0.5064435005187988\n",
      "Epoch: 2, Loss: 0.5440012812614441\n",
      "Epoch: 2, Loss: 0.5942890048027039\n",
      "Epoch: 2, Loss: 0.3704826533794403\n",
      "Epoch: 2, Loss: 0.4873014986515045\n",
      "Epoch: 2, Loss: 0.49031519889831543\n",
      "Epoch: 2, Loss: 0.3864648938179016\n",
      "Epoch: 2, Loss: 0.460814505815506\n",
      "Epoch: 2, Loss: 0.6690893769264221\n",
      "Epoch: 2, Loss: 0.39311227202415466\n",
      "Epoch: 2, Loss: 0.44789958000183105\n",
      "Epoch: 2, Loss: 0.5836447477340698\n",
      "Epoch: 2, Loss: 0.4482584595680237\n",
      "Epoch: 2, Loss: 0.48289954662323\n",
      "Epoch: 2, Loss: 0.46979501843452454\n",
      "Epoch: 2, Loss: 0.5053073167800903\n",
      "Epoch: 2, Loss: 0.39394283294677734\n",
      "Epoch: 2, Loss: 0.5951905846595764\n",
      "Epoch: 2, Loss: 0.5673760771751404\n",
      "Epoch: 2, Loss: 0.4564495384693146\n",
      "Epoch: 2, Loss: 0.4404407739639282\n",
      "Epoch: 2, Loss: 0.4342879354953766\n",
      "Epoch: 2, Loss: 0.532002866268158\n",
      "Epoch: 2, Loss: 0.53419429063797\n",
      "Epoch: 2, Loss: 0.4649757444858551\n",
      "Epoch: 2, Loss: 0.5003328919410706\n",
      "Epoch: 2, Loss: 0.5049477815628052\n",
      "Epoch: 2, Loss: 0.4701535701751709\n",
      "Epoch: 2, Loss: 0.5271146893501282\n",
      "Epoch: 2, Loss: 0.4538845419883728\n",
      "Epoch: 2, Loss: 0.5182332992553711\n",
      "Epoch: 2, Loss: 0.4968310594558716\n",
      "Epoch: 2, Loss: 0.3894308805465698\n",
      "Epoch: 2, Loss: 0.4984937310218811\n",
      "Epoch: 2, Loss: 0.4594724774360657\n",
      "Epoch: 2, Loss: 0.49788719415664673\n",
      "Epoch: 2, Loss: 0.5827628374099731\n",
      "Epoch: 2, Loss: 0.5426996946334839\n",
      "Epoch: 2, Loss: 0.4820631146430969\n",
      "Epoch: 2, Loss: 0.4156843423843384\n",
      "Epoch: 2, Loss: 0.46461161971092224\n",
      "Epoch: 2, Loss: 0.5820858478546143\n",
      "Epoch: 2, Loss: 0.37107300758361816\n",
      "Epoch: 2, Loss: 0.4337620139122009\n",
      "Epoch: 2, Loss: 0.393840491771698\n",
      "Epoch: 2, Loss: 0.6258424520492554\n",
      "Epoch: 2, Loss: 0.6263872385025024\n",
      "Epoch: 2, Loss: 0.5408852696418762\n",
      "Epoch: 2, Loss: 0.5151817798614502\n",
      "Epoch: 2, Loss: 0.4523855745792389\n",
      "Epoch: 2, Loss: 0.49631959199905396\n",
      "Epoch: 2, Loss: 0.4528079032897949\n",
      "Epoch: 2, Loss: 0.43210601806640625\n",
      "Epoch: 2, Loss: 0.45469358563423157\n",
      "Epoch: 2, Loss: 0.5127612352371216\n",
      "Epoch: 2, Loss: 0.35293546319007874\n",
      "Epoch: 2, Loss: 0.45376384258270264\n",
      "Epoch: 2, Loss: 0.48212549090385437\n",
      "Epoch: 2, Loss: 0.44358494877815247\n",
      "Epoch: 2, Loss: 0.5138483047485352\n",
      "Epoch: 2, Loss: 0.5243178606033325\n",
      "Epoch: 2, Loss: 0.5763759016990662\n",
      "Epoch: 2, Loss: 0.4460197985172272\n",
      "Epoch: 2, Loss: 0.48993760347366333\n",
      "Epoch: 2, Loss: 0.5054082870483398\n",
      "Epoch: 2, Loss: 0.5612568855285645\n",
      "Epoch: 2, Loss: 0.5218064785003662\n",
      "Epoch: 2, Loss: 0.5833597183227539\n",
      "Epoch: 2, Loss: 0.5032415986061096\n",
      "Epoch: 2, Loss: 0.4387940764427185\n",
      "Epoch: 2, Loss: 0.5727185010910034\n",
      "Epoch: 2, Loss: 0.4201772212982178\n",
      "Epoch: 2, Loss: 0.3836338520050049\n",
      "Epoch: 2, Loss: 0.44167235493659973\n",
      "Epoch: 2, Loss: 0.5635787844657898\n",
      "Epoch: 2, Loss: 0.5077551007270813\n",
      "Epoch: 2, Loss: 0.4988974928855896\n",
      "Epoch: 2, Loss: 0.535408616065979\n",
      "Epoch: 2, Loss: 0.3942457437515259\n",
      "Epoch: 2, Loss: 0.4491245746612549\n",
      "Epoch: 2, Loss: 0.532148003578186\n",
      "Epoch: 2, Loss: 0.5340696573257446\n",
      "Epoch: 2, Loss: 0.40442410111427307\n",
      "Epoch: 2, Loss: 0.4901396632194519\n",
      "Epoch: 2, Loss: 0.43686509132385254\n",
      "Epoch: 2, Loss: 0.5367735028266907\n",
      "Epoch: 2, Loss: 0.5929325819015503\n",
      "Epoch: 2, Loss: 0.4194408655166626\n",
      "Epoch: 2, Loss: 0.5078220367431641\n",
      "Epoch: 2, Loss: 0.4594089090824127\n",
      "Epoch: 2, Loss: 0.36894550919532776\n",
      "Epoch: 2, Loss: 0.5359569787979126\n",
      "Epoch: 2, Loss: 0.5081101655960083\n",
      "Epoch: 2, Loss: 0.6048381328582764\n",
      "Epoch: 2, Loss: 0.4010566174983978\n",
      "Epoch: 2, Loss: 0.4259675145149231\n",
      "Epoch: 2, Loss: 0.5707709789276123\n",
      "Epoch: 2, Loss: 0.4928092360496521\n",
      "Epoch: 2, Loss: 0.5255314707756042\n",
      "Epoch: 2, Loss: 0.4110442101955414\n",
      "Epoch: 2, Loss: 0.455318808555603\n",
      "Epoch: 2, Loss: 0.46374863386154175\n",
      "Epoch: 2, Loss: 0.43749380111694336\n",
      "Epoch: 2, Loss: 0.46384596824645996\n",
      "Epoch: 2, Loss: 0.4453241229057312\n",
      "Epoch: 2, Loss: 0.5572980046272278\n",
      "Epoch: 2, Loss: 0.48781049251556396\n",
      "Epoch: 2, Loss: 0.48708346486091614\n",
      "Epoch: 2, Loss: 0.5267767906188965\n",
      "Epoch: 2, Loss: 0.48078617453575134\n",
      "Epoch: 2, Loss: 0.39626872539520264\n",
      "Epoch: 2, Loss: 0.4229438900947571\n",
      "Epoch: 2, Loss: 0.5186077952384949\n",
      "Epoch: 2, Loss: 0.5906577110290527\n",
      "Epoch: 2, Loss: 0.46890074014663696\n",
      "Epoch: 2, Loss: 0.4305620491504669\n",
      "Epoch: 2, Loss: 0.4418432116508484\n",
      "Epoch: 2, Loss: 0.4766550660133362\n",
      "Epoch: 2, Loss: 0.5194544792175293\n",
      "Epoch: 2, Loss: 0.472859650850296\n",
      "Epoch: 2, Loss: 0.6156706809997559\n",
      "Epoch: 2, Loss: 0.5175411701202393\n",
      "Epoch: 2, Loss: 0.42170706391334534\n",
      "Epoch: 2, Loss: 0.49375927448272705\n",
      "Epoch: 2, Loss: 0.6258693337440491\n",
      "Epoch: 2, Loss: 0.4317619800567627\n",
      "Epoch: 2, Loss: 0.5014952421188354\n",
      "Epoch: 2, Loss: 0.4078228771686554\n",
      "Epoch: 2, Loss: 0.3883940577507019\n",
      "Epoch: 2, Loss: 0.4492759108543396\n",
      "Epoch: 2, Loss: 0.5006148815155029\n",
      "Epoch: 2, Loss: 0.44911903142929077\n",
      "Epoch: 2, Loss: 0.44924286007881165\n",
      "Epoch: 2, Loss: 0.519633412361145\n",
      "Epoch: 2, Loss: 0.447770893573761\n",
      "Epoch: 2, Loss: 0.4546758234500885\n",
      "Epoch: 2, Loss: 0.46901702880859375\n",
      "Epoch: 2, Loss: 0.5455906391143799\n",
      "Epoch: 2, Loss: 0.41153037548065186\n",
      "Epoch: 2, Loss: 0.44128355383872986\n",
      "Epoch: 2, Loss: 0.5149474143981934\n",
      "Epoch: 2, Loss: 0.4723563492298126\n",
      "Epoch: 2, Loss: 0.5334768295288086\n",
      "Epoch: 2, Loss: 0.4162064790725708\n",
      "Epoch: 2, Loss: 0.3725152909755707\n",
      "Epoch: 2, Loss: 0.36981064081192017\n",
      "Epoch: 2, Loss: 0.55390465259552\n",
      "Epoch: 2, Loss: 0.38300710916519165\n",
      "Epoch: 2, Loss: 0.5313635468482971\n",
      "Epoch: 2, Loss: 0.4968419671058655\n",
      "Epoch: 2, Loss: 0.425298810005188\n",
      "Epoch: 2, Loss: 0.4320375621318817\n",
      "Epoch: 2, Loss: 0.4773940443992615\n",
      "Epoch: 2, Loss: 0.5409812927246094\n",
      "Epoch: 2, Loss: 0.4787297248840332\n",
      "Epoch: 2, Loss: 0.6462473273277283\n",
      "Epoch: 2, Loss: 0.4744036793708801\n",
      "Epoch: 2, Loss: 0.46886447072029114\n",
      "Epoch: 2, Loss: 0.44317740201950073\n",
      "Epoch: 2, Loss: 0.4026164412498474\n",
      "Epoch: 2, Loss: 0.5026664733886719\n",
      "Epoch: 2, Loss: 0.47827503085136414\n",
      "Epoch: 2, Loss: 0.5490671992301941\n",
      "Epoch: 2, Loss: 0.4288940131664276\n",
      "Epoch: 2, Loss: 0.5372362732887268\n",
      "Epoch: 2, Loss: 0.4203477203845978\n",
      "Epoch: 2, Loss: 0.4936447739601135\n",
      "Epoch: 2, Loss: 0.47267231345176697\n",
      "Epoch: 2, Loss: 0.40162813663482666\n",
      "Epoch: 2, Loss: 0.5479452013969421\n",
      "Epoch: 2, Loss: 0.472307026386261\n",
      "Epoch: 2, Loss: 0.4575376510620117\n",
      "Epoch: 2, Loss: 0.6195071935653687\n",
      "Epoch: 2, Loss: 0.5789852738380432\n",
      "Epoch: 2, Loss: 0.5592736005783081\n",
      "Epoch: 2, Loss: 0.40482962131500244\n",
      "Epoch: 2, Loss: 0.5004910826683044\n",
      "Epoch: 2, Loss: 0.4110882878303528\n",
      "Epoch: 2, Loss: 0.38994407653808594\n",
      "Epoch: 2, Loss: 0.46435320377349854\n",
      "Epoch: 2, Loss: 0.5846493244171143\n",
      "Epoch: 2, Loss: 0.5220283269882202\n",
      "Epoch: 2, Loss: 0.4823746085166931\n",
      "Epoch: 2, Loss: 0.491603821516037\n",
      "Epoch: 2, Loss: 0.5684559345245361\n",
      "Epoch: 2, Loss: 0.48022785782814026\n",
      "Epoch: 2, Loss: 0.5893535017967224\n",
      "Epoch: 2, Loss: 0.5531423687934875\n",
      "Epoch: 2, Loss: 0.5057519674301147\n",
      "Epoch: 2, Loss: 0.6160682439804077\n",
      "Epoch: 2, Loss: 0.4702570140361786\n",
      "Epoch: 2, Loss: 0.5147280097007751\n",
      "Epoch: 2, Loss: 0.4239843487739563\n",
      "Epoch: 2, Loss: 0.4616682529449463\n",
      "Epoch: 2, Loss: 0.571374773979187\n",
      "Epoch: 2, Loss: 0.4494095742702484\n",
      "Epoch: 2, Loss: 0.6325502991676331\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 249\u001b[0m\n\u001b[1;32m    244\u001b[0m adaptive_train(\n\u001b[1;32m    245\u001b[0m     student_model, teacher_model, train_dataloader, hardware_info, epochs\u001b[38;5;241m=\u001b[39mepochs\n\u001b[1;32m    246\u001b[0m )\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[43madaptive_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 219\u001b[0m, in \u001b[0;36madaptive_evaluate\u001b[0;34m(student_model, eval_dataloader)\u001b[0m\n\u001b[1;32m    216\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m student_model(input_ids, attention_mask)\n\u001b[1;32m    217\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m     correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     total_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(predictions)\n\u001b[1;32m    222\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_predictions\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import psutil\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load teacher model and tokenizer\n",
    "teacher_model_name = \"distilbert-base-uncased\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_model_name, num_labels=2\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "\n",
    "# Updated tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Ensure the input is a list of strings\n",
    "    texts = [str(text) for text in examples[\"text\"]]\n",
    "\n",
    "    # Tokenize each example and ensure padding, truncation, and correct tensor formatting\n",
    "    tokenized = teacher_tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",  # Use max_length padding\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Remove the batch dimension added by the tokenizer\n",
    "    tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "    # Add labels to the tokenized output\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Update the dataset processing\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets.set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Hardware info function\n",
    "def get_hardware_info():\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    mps_available = torch.backends.mps.is_available()\n",
    "    total_ram = psutil.virtual_memory().total / (1024**3)  # Convert to GB\n",
    "    available_ram = psutil.virtual_memory().available / (1024**3)  # Convert to GB\n",
    "    cpu_cores = psutil.cpu_count(logical=False)  # Physical cores\n",
    "    logical_cores = psutil.cpu_count(logical=True)  # Logical cores\n",
    "\n",
    "    if gpu_available:\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (\n",
    "            1024**3\n",
    "        )  # Convert to GB\n",
    "        gpu_type = \"CUDA\"\n",
    "    elif mps_available:\n",
    "        gpu_count = 1\n",
    "        gpu_mem = None\n",
    "        gpu_type = \"MPS\"\n",
    "    else:\n",
    "        gpu_count = 0\n",
    "        gpu_mem = 0\n",
    "        gpu_type = \"None\"\n",
    "\n",
    "    return {\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"mps_available\": mps_available,\n",
    "        \"gpu_type\": gpu_type,\n",
    "        \"gpu_count\": gpu_count,\n",
    "        \"gpu_memory\": gpu_mem,\n",
    "        \"total_ram\": total_ram,\n",
    "        \"available_ram\": available_ram,\n",
    "        \"cpu_cores\": cpu_cores,\n",
    "        \"logical_cores\": logical_cores,\n",
    "    }\n",
    "\n",
    "\n",
    "# Check hardware info\n",
    "hardware_info = get_hardware_info()\n",
    "print(hardware_info)\n",
    "\n",
    "\n",
    "class AdaptiveStudentModel(nn.Module):\n",
    "    def __init__(self, teacher_model_name, hardware_info):\n",
    "        super(AdaptiveStudentModel, self).__init__()\n",
    "        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            teacher_model_name, num_labels=2\n",
    "        )\n",
    "        hidden_size = self.teacher_model.config.hidden_size\n",
    "\n",
    "        # Determine the size of the intermediate layer\n",
    "        if hardware_info[\"gpu_available\"]:\n",
    "            if hardware_info[\"gpu_memory\"] < 4:\n",
    "                intermediate_size = 128\n",
    "            else:\n",
    "                intermediate_size = 256\n",
    "        elif hardware_info[\"mps_available\"]:\n",
    "            intermediate_size = 128\n",
    "        else:\n",
    "            intermediate_size = 64\n",
    "\n",
    "        # Create a sequence of layers\n",
    "        self.student_model = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_size, intermediate_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(intermediate_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Use the teacher model to get hidden states\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(\n",
    "                input_ids, attention_mask=attention_mask, output_hidden_states=True\n",
    "            )\n",
    "            teacher_hidden_states = teacher_outputs.hidden_states[\n",
    "                -1\n",
    "            ]  # Last hidden layer\n",
    "\n",
    "        # Pool the hidden states (mean pooling)\n",
    "        pooled_output = teacher_hidden_states.mean(\n",
    "            dim=1\n",
    "        )  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Process the pooled output through the student model\n",
    "        student_hidden = self.student_model(pooled_output)\n",
    "\n",
    "        # Final classification layer\n",
    "        student_logits = self.output_layer(student_hidden)\n",
    "\n",
    "        return student_logits\n",
    "\n",
    "\n",
    "# Distillation loss function\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_probs = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "    return nn.KLDivLoss()(student_probs, teacher_probs) * (temperature**2)\n",
    "\n",
    "\n",
    "# Function to get adaptive batch size based on hardware\n",
    "def get_adaptive_batch_size(hardware_info):\n",
    "    if hardware_info[\"gpu_available\"]:\n",
    "        return 32 if hardware_info[\"gpu_memory\"] >= 4 else 16\n",
    "    elif hardware_info[\"mps_available\"]:\n",
    "        return 16\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "\n",
    "# Training function\n",
    "# Update the training function\n",
    "def adaptive_train(student_model, teacher_model, train_dataloader, hardware_info, epochs=3):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()  # Ensure teacher model is in evaluation mode\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            student_logits = student_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            distill_loss = distillation_loss(student_logits, teacher_outputs.logits)\n",
    "            \n",
    "            # Add a classification loss component\n",
    "            clf_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "            \n",
    "            # Combine losses\n",
    "            loss = distill_loss + clf_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation function\n",
    "def adaptive_evaluate(student_model, eval_dataloader):\n",
    "    student_model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = student_model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += len(predictions)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Initialize models and run training\n",
    "hardware_info = get_hardware_info()\n",
    "student_model = AdaptiveStudentModel(teacher_model_name, hardware_info).to(device)\n",
    "teacher_model.to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=get_adaptive_batch_size(hardware_info),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=get_adaptive_batch_size(hardware_info)\n",
    ")\n",
    "\n",
    "# Run training\n",
    "epochs = 3\n",
    "adaptive_train(\n",
    "    student_model, teacher_model, train_dataloader, hardware_info, epochs=epochs\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "adaptive_evaluate(student_model, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
